{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuemEzco8Oq/BQe7VAs/BK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dCqM2CvoZCRp","executionInfo":{"status":"ok","timestamp":1653552455081,"user_tz":-120,"elapsed":282,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"8b9319e4-3bf6-4a64-f458-195a2f26bfee"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu May 26 08:07:34 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   45C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/facebookresearch/fairseq.git\n","!cd fairseq && pip install .\n","!pip install subword_nmt sacremoses\n","!git clone https://github.com/Szuumii/en-de-backtranslation.git\n","!pip install -q wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwxCSZKWZEoq","executionInfo":{"status":"ok","timestamp":1653552556269,"user_tz":-120,"elapsed":100981,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"c408fec7-6c8d-4d0a-ebd1-b8a4789e02db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'fairseq' already exists and is not an empty directory.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/fairseq\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (0.29.30)\n","Collecting hydra-core<1.1,>=1.0.7\n","  Using cached hydra_core-1.0.7-py3-none-any.whl (123 kB)\n","Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (4.64.0)\n","Collecting sacrebleu>=1.4.12\n","  Using cached sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n","Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (0.11.0+cu113)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (1.21.6)\n","Collecting omegaconf<2.1\n","  Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\n","Collecting bitarray\n","  Using cached bitarray-2.5.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (2019.12.20)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+b5e7b25) (1.11.0+cu113)\n","Collecting antlr4-python3-runtime==4.8\n","  Using cached antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+b5e7b25) (5.7.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+b5e7b25) (4.2.0)\n","Collecting PyYAML>=5.1.*\n","  Using cached PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+b5e7b25) (0.8.9)\n","Collecting portalocker\n","  Using cached portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting colorama\n","  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+b5e7b25) (2.21)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==1.0.0a0+b5e7b25) (3.8.0)\n","Building wheels for collected packages: fairseq, antlr4-python3-runtime\n","  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairseq: filename=fairseq-1.0.0a0+b5e7b25-cp37-cp37m-linux_x86_64.whl size=15365017 sha256=4d3ac216add893503966baf645243c27b8a51b853cab92ccbf471a1043130326\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-y0cmo_bf/wheels/7c/35/80/edbd520a1a7e615df007002aeea9f6bf5f3c8f9243e072f6ce\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=7a8d7a1d0e08daaa04f9218036eae142b60c10c55a546d53647776f2f933f446\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","Successfully built fairseq antlr4-python3-runtime\n","Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.5.1 colorama-0.4.4 fairseq-1.0.0a0+b5e7b25 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.4.0 sacrebleu-2.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: subword_nmt in /usr/local/lib/python3.7/dist-packages (0.3.8)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.53)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from subword_nmt) (4.64.0)\n","Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword_nmt) (4.0.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n","fatal: destination path 'en-de-backtranslation' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["#3863de247ad4f4556935cb0c04f181dba98fc7cf\n","!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tr2GssKvZpO7","executionInfo":{"status":"ok","timestamp":1653552572966,"user_tz":-120,"elapsed":1209,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"96f6f450-d8e1-4afa-bc22-dd1f3620a3cd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mszumi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}]},{"cell_type":"code","source":["# generate dataset for training model for backtranslation\n","!fairseq-preprocess --source-lang de --target-lang en \\\n","  --trainpref ./en-de-backtranslation/data/datasets/unaugmented/original.train \\\n","  --validpref ./en-de-backtranslation/data/datasets/unaugmented/original.val  \\\n","  --testpref  ./en-de-backtranslation/data/datasets/unaugmented/original.test  \\\n","  --destdir ./data/binarized/ua.tokenized.de-en \\\n","  --thresholdsrc 2 \\\n","  --thresholdtgt 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHTOCfGNZbGM","executionInfo":{"status":"ok","timestamp":1653552678967,"user_tz":-120,"elapsed":103740,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"0d9e13de-8fc3-4757-93ac-27803d1910a3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 08:09:37 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 08:09:37 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='./data/binarized/ua.tokenized.de-en', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='de', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='./en-de-backtranslation/data/datasets/unaugmented/original.test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer=None, tpu=False, trainpref='./en-de-backtranslation/data/datasets/unaugmented/original.train', use_plasma_view=False, user_dir=None, validpref='./en-de-backtranslation/data/datasets/unaugmented/original.val', wandb_project=None, workers=1)\n","2022-05-26 08:10:00 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 08:10:28 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.train.de: 200000 sents, 4673897 tokens, 2.92% replaced (by <unk>)\n","2022-05-26 08:10:28 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 08:10:34 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.val.de: 42989 sents, 999226 tokens, 4.67% replaced (by <unk>)\n","2022-05-26 08:10:34 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 08:10:40 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.test.de: 43291 sents, 1006306 tokens, 4.87% replaced (by <unk>)\n","2022-05-26 08:10:40 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 08:11:06 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.train.en: 200000 sents, 4562780 tokens, 1.62% replaced (by <unk>)\n","2022-05-26 08:11:06 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 08:11:12 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.val.en: 42989 sents, 974477 tokens, 2.91% replaced (by <unk>)\n","2022-05-26 08:11:12 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 08:11:18 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.test.en: 43291 sents, 982864 tokens, 2.96% replaced (by <unk>)\n","2022-05-26 08:11:18 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ./data/binarized/ua.tokenized.de-en\n"]}]},{"cell_type":"code","source":["# train de-en model for backtranslation\n","!CHECKPOINT_DIR=checkpoints_de_en && fairseq-train ./data/binarized/ua.tokenized.de-en --fp16\\\n","  --arch transformer \\\n","  --source-lang de --target-lang en \\\n","  --dropout 0.1 \\\n","  --attention-dropout 0.1 \\\n","  --activation-dropout 0.1 \\\n","  --encoder-embed-dim 256 \\\n","  --encoder-ffn-embed-dim 512 \\\n","  --encoder-layers 3 \\\n","  --encoder-attention-heads 8 \\\n","  --encoder-learned-pos \\\n","  --decoder-embed-dim 256 \\\n","  --decoder-ffn-embed-dim 512 \\\n","  --decoder-layers 3 \\\n","  --decoder-attention-heads 8 \\\n","  --no-epoch-checkpoints \\\n","  --decoder-learned-pos \\\n","  --max-epoch 10\\\n","  --optimizer adam \\\n","  --lr 5e-4 \\\n","  --batch-size 128 \\\n","  --seed 1 \\\n","  --wandb-project \"en-de-backtranslation\" \\\n","  --save-dir $CHECKPOINT_DIR \\\n","  #--finetune-from-model ./checkpoints_de_en/checkpoint_last.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0XIpDwHZuLK","executionInfo":{"status":"ok","timestamp":1653554470799,"user_tz":-120,"elapsed":1787646,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"5d7a9049-533c-4efc-baff-b7721b8eadc9"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 08:11:24 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 08:11:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'en-de-backtranslation', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 10, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints_de_en', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.1, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='./data/binarized/ua.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=512, decoder_input_dim=256, decoder_layerdrop=0, decoder_layers=3, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=512, encoder_layerdrop=0, encoder_layers=3, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=10, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints_de_en', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='en-de-backtranslation', warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': './data/binarized/ua.tokenized.de-en', 'source_lang': 'de', 'target_lang': 'en', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2022-05-26 08:11:27 | INFO | fairseq.tasks.translation | [de] dictionary: 102504 types\n","2022-05-26 08:11:27 | INFO | fairseq.tasks.translation | [en] dictionary: 73392 types\n","2022-05-26 08:11:28 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(102504, 256, padding_idx=1)\n","    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(73392, 256, padding_idx=1)\n","    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=73392, bias=False)\n","  )\n",")\n","2022-05-26 08:11:28 | INFO | fairseq_cli.train | task: TranslationTask\n","2022-05-26 08:11:28 | INFO | fairseq_cli.train | model: TransformerModel\n","2022-05-26 08:11:28 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n","2022-05-26 08:11:28 | INFO | fairseq_cli.train | num. shared model params: 68,296,704 (num. trained: 68,296,704)\n","2022-05-26 08:11:28 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2022-05-26 08:11:28 | INFO | fairseq.data.data_utils | loaded 42,989 examples from: ./data/binarized/ua.tokenized.de-en/valid.de-en.de\n","2022-05-26 08:11:28 | INFO | fairseq.data.data_utils | loaded 42,989 examples from: ./data/binarized/ua.tokenized.de-en/valid.de-en.en\n","2022-05-26 08:11:28 | INFO | fairseq.tasks.translation | ./data/binarized/ua.tokenized.de-en valid de-en 42989 examples\n","2022-05-26 08:11:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2022-05-26 08:11:37 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n","2022-05-26 08:11:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2022-05-26 08:11:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2022-05-26 08:11:37 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 128\n","2022-05-26 08:11:37 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:11:37 | INFO | fairseq.trainer | No existing checkpoint found checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:11:37 | INFO | fairseq.trainer | loading train data for epoch 1\n","2022-05-26 08:11:37 | INFO | fairseq.data.data_utils | loaded 200,000 examples from: ./data/binarized/ua.tokenized.de-en/train.de-en.de\n","2022-05-26 08:11:37 | INFO | fairseq.data.data_utils | loaded 200,000 examples from: ./data/binarized/ua.tokenized.de-en/train.de-en.en\n","2022-05-26 08:11:37 | INFO | fairseq.tasks.translation | ./data/binarized/ua.tokenized.de-en train de-en 200000 examples\n","2022-05-26 08:11:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 001:   0% 0/1563 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mszumi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220526_081138-m7phxnxz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints_de_en\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation/runs/m7phxnxz\u001b[0m\n","2022-05-26 08:11:42 | INFO | fairseq.trainer | begin training epoch 1\n","2022-05-26 08:11:42 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 001:   0% 1/1563 [00:05<2:14:08,  5.15s/it]2022-05-26 08:11:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n","epoch 001:   4% 58/1563 [00:11<02:17, 10.96it/s]2022-05-26 08:11:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n","epoch 001:   5% 73/1563 [00:13<02:08, 11.63it/s]2022-05-26 08:11:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n","epoch 001: 100% 1562/1563 [02:43<00:00,  9.92it/s, loss=7.664, ppl=202.82, wps=29689.7, ups=9.59, wpb=3096.9, bsz=128, num_updates=1500, lr=0.0005, gnorm=0.867, loss_scale=16, train_wall=10, gb_free=12.2, wall=157]2022-05-26 08:14:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 001 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   1% 2/337 [00:00<00:21, 15.65it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   2% 6/337 [00:00<00:12, 25.82it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   3% 10/337 [00:00<00:11, 28.74it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   4% 14/337 [00:00<00:11, 28.48it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   6% 19/337 [00:00<00:09, 34.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   7% 24/337 [00:00<00:08, 38.74it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   8% 28/337 [00:00<00:08, 37.47it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:   9% 32/337 [00:00<00:08, 37.12it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  11% 37/337 [00:01<00:07, 39.55it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  12% 41/337 [00:01<00:07, 37.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  14% 46/337 [00:01<00:07, 38.67it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  15% 50/337 [00:01<00:07, 36.62it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  16% 55/337 [00:01<00:07, 39.10it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  18% 59/337 [00:01<00:07, 36.80it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  19% 64/337 [00:01<00:07, 38.87it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  20% 68/337 [00:01<00:07, 35.97it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  22% 73/337 [00:02<00:06, 37.90it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  23% 77/337 [00:02<00:06, 38.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  24% 81/337 [00:02<00:06, 37.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  25% 85/337 [00:02<00:06, 37.98it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  26% 89/337 [00:02<00:06, 35.64it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  28% 93/337 [00:02<00:06, 36.46it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  29% 98/337 [00:02<00:06, 37.67it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  30% 102/337 [00:02<00:06, 35.97it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  32% 107/337 [00:02<00:06, 37.51it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  33% 111/337 [00:03<00:05, 37.86it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  34% 115/337 [00:03<00:06, 35.57it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  36% 120/337 [00:03<00:05, 37.00it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  37% 124/337 [00:03<00:05, 36.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  38% 128/337 [00:03<00:06, 34.79it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  39% 132/337 [00:03<00:05, 35.63it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  40% 136/337 [00:03<00:05, 35.78it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  42% 140/337 [00:03<00:05, 33.67it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  43% 144/337 [00:04<00:05, 34.77it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  44% 148/337 [00:04<00:05, 34.83it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  45% 152/337 [00:04<00:05, 33.32it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  46% 156/337 [00:04<00:05, 34.73it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  47% 160/337 [00:04<00:05, 34.91it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  49% 164/337 [00:04<00:05, 32.42it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  50% 168/337 [00:04<00:05, 33.67it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  51% 172/337 [00:04<00:04, 34.07it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  52% 176/337 [00:04<00:05, 30.81it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  53% 180/337 [00:05<00:04, 32.65it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  55% 184/337 [00:05<00:04, 32.46it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  56% 188/337 [00:05<00:04, 30.28it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  57% 192/337 [00:05<00:04, 31.64it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  58% 196/337 [00:05<00:04, 32.09it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  59% 200/337 [00:05<00:04, 29.67it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  61% 204/337 [00:05<00:04, 30.97it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  62% 208/337 [00:06<00:04, 31.20it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  63% 212/337 [00:06<00:04, 29.93it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  64% 216/337 [00:06<00:03, 31.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  65% 220/337 [00:06<00:03, 29.26it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  66% 224/337 [00:06<00:03, 30.70it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  68% 228/337 [00:06<00:03, 31.23it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  69% 232/337 [00:06<00:03, 30.20it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  70% 236/337 [00:06<00:03, 30.98it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  71% 240/337 [00:07<00:03, 29.47it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  72% 243/337 [00:07<00:03, 29.45it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  73% 247/337 [00:07<00:02, 30.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  74% 251/337 [00:07<00:03, 28.62it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  76% 255/337 [00:07<00:02, 29.38it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  77% 258/337 [00:07<00:02, 27.21it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  78% 262/337 [00:07<00:02, 28.33it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  79% 265/337 [00:07<00:02, 26.34it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  80% 269/337 [00:08<00:02, 27.88it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  81% 272/337 [00:08<00:02, 26.32it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  82% 276/337 [00:08<00:02, 27.25it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  83% 279/337 [00:08<00:02, 26.47it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  84% 282/337 [00:08<00:02, 27.20it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 285/337 [00:08<00:02, 25.18it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  85% 288/337 [00:08<00:01, 25.96it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  86% 291/337 [00:09<00:01, 24.42it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  87% 294/337 [00:09<00:01, 25.21it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  88% 297/337 [00:09<00:01, 23.42it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  89% 300/337 [00:09<00:01, 23.43it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  90% 303/337 [00:09<00:01, 24.14it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  91% 306/337 [00:09<00:02, 14.57it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  91% 308/337 [00:10<00:01, 15.40it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  92% 311/337 [00:10<00:01, 17.04it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  93% 314/337 [00:10<00:01, 18.39it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  94% 317/337 [00:10<00:01, 19.08it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  95% 320/337 [00:10<00:00, 19.02it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  96% 323/337 [00:10<00:00, 19.15it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  96% 325/337 [00:10<00:00, 18.88it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  97% 327/337 [00:10<00:00, 18.76it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 329/337 [00:11<00:00, 18.45it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  98% 331/337 [00:11<00:00, 18.25it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 333/337 [00:11<00:00, 17.90it/s]\u001b[A\n","epoch 001 | valid on 'valid' subset:  99% 335/337 [00:11<00:00, 16.11it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:14:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 7.265 | ppl 153.81 | wps 85010.3 | wpb 2891.6 | bsz 127.6 | num_updates 1560\n","2022-05-26 08:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1560 updates\n","2022-05-26 08:14:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","epoch 001: 100% 1563/1563 [03:01<00:00,  3.27s/it, loss=7.664, ppl=202.82, wps=29689.7, ups=9.59, wpb=3096.9, bsz=128, num_updates=1500, lr=0.0005, gnorm=0.867, loss_scale=16, train_wall=10, gb_free=12.2, wall=157]2022-05-26 08:14:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_best.pt (epoch 1 @ 1560 updates, score 7.265) (writing took 6.37277966399995 seconds)\n","2022-05-26 08:14:39 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n","2022-05-26 08:14:39 | INFO | train | epoch 001 | loss 8.751 | ppl 430.84 | wps 25819.9 | ups 8.86 | wpb 2914.8 | bsz 128 | num_updates 1560 | lr 0.0005 | gnorm 0.866 | loss_scale 16 | train_wall 154 | gb_free 11.1 | wall 181\n","2022-05-26 08:14:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 002:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:14:39 | INFO | fairseq.trainer | begin training epoch 2\n","2022-05-26 08:14:39 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 002: 100% 1561/1563 [02:39<00:00, 11.31it/s, loss=6.352, ppl=81.68, wps=28698, ups=9.96, wpb=2880.4, bsz=128, num_updates=3100, lr=0.0005, gnorm=1.116, loss_scale=16, train_wall=10, gb_free=12, wall=339]2022-05-26 08:17:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 002 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   0% 1/337 [00:00<00:39,  8.52it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   1% 4/337 [00:00<00:16, 20.29it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   2% 8/337 [00:00<00:12, 26.41it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   4% 13/337 [00:00<00:09, 32.43it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   5% 17/337 [00:00<00:09, 33.98it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   6% 21/337 [00:00<00:09, 34.77it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   8% 26/337 [00:00<00:08, 35.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:   9% 31/337 [00:00<00:07, 38.93it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  10% 35/337 [00:01<00:07, 39.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  12% 39/337 [00:01<00:07, 39.17it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  13% 43/337 [00:01<00:07, 37.37it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  14% 47/337 [00:01<00:07, 37.94it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  15% 51/337 [00:01<00:07, 36.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  17% 56/337 [00:01<00:07, 38.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  18% 60/337 [00:01<00:07, 36.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  19% 65/337 [00:01<00:07, 37.93it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  20% 69/337 [00:01<00:07, 35.81it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  22% 74/337 [00:02<00:06, 37.90it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  23% 78/337 [00:02<00:07, 36.13it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  25% 83/337 [00:02<00:06, 37.29it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  26% 87/337 [00:02<00:06, 37.67it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  27% 91/337 [00:02<00:06, 36.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  28% 95/337 [00:02<00:06, 36.69it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  29% 99/337 [00:02<00:06, 36.91it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  31% 103/337 [00:02<00:06, 36.15it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  32% 107/337 [00:02<00:06, 37.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  33% 111/337 [00:03<00:05, 37.88it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  34% 115/337 [00:03<00:06, 34.28it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  35% 119/337 [00:03<00:06, 35.67it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  36% 123/337 [00:03<00:05, 36.49it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  38% 127/337 [00:03<00:06, 34.84it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  39% 131/337 [00:03<00:05, 35.66it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  40% 135/337 [00:03<00:05, 35.85it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  41% 139/337 [00:03<00:05, 33.80it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  42% 143/337 [00:04<00:05, 35.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  44% 147/337 [00:04<00:05, 35.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  45% 151/337 [00:04<00:05, 32.27it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  46% 155/337 [00:04<00:05, 33.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  47% 159/337 [00:04<00:05, 33.15it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  48% 163/337 [00:04<00:05, 30.85it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  50% 167/337 [00:04<00:05, 32.62it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  51% 171/337 [00:04<00:05, 33.12it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  52% 175/337 [00:05<00:05, 31.19it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  53% 179/337 [00:05<00:04, 32.23it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  54% 183/337 [00:05<00:04, 32.99it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  55% 187/337 [00:05<00:05, 29.55it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  57% 191/337 [00:05<00:04, 31.30it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  58% 195/337 [00:05<00:04, 32.35it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  59% 199/337 [00:05<00:04, 30.29it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  60% 203/337 [00:05<00:04, 31.40it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  61% 207/337 [00:06<00:04, 31.78it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  63% 211/337 [00:06<00:04, 30.65it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  64% 215/337 [00:06<00:03, 31.43it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  65% 219/337 [00:06<00:03, 30.75it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  66% 223/337 [00:06<00:03, 29.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  67% 227/337 [00:06<00:03, 30.31it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  69% 231/337 [00:06<00:03, 29.34it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  70% 235/337 [00:06<00:03, 30.44it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  71% 239/337 [00:07<00:03, 30.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  72% 243/337 [00:07<00:03, 29.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  73% 247/337 [00:07<00:03, 29.21it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  74% 250/337 [00:07<00:03, 27.60it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  75% 253/337 [00:07<00:02, 28.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  76% 256/337 [00:07<00:02, 28.14it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  77% 259/337 [00:07<00:02, 26.43it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  78% 262/337 [00:07<00:02, 27.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  79% 265/337 [00:08<00:02, 25.29it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  80% 269/337 [00:08<00:02, 27.04it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  81% 272/337 [00:08<00:02, 25.18it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 275/337 [00:08<00:02, 26.10it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  82% 278/337 [00:08<00:02, 26.66it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  83% 281/337 [00:08<00:02, 25.94it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  84% 284/337 [00:08<00:02, 26.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  85% 287/337 [00:08<00:02, 24.01it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  86% 290/337 [00:09<00:01, 24.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  87% 293/337 [00:09<00:01, 23.88it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  88% 296/337 [00:09<00:01, 22.94it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  89% 299/337 [00:09<00:01, 23.59it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  90% 302/337 [00:09<00:01, 23.59it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  91% 305/337 [00:09<00:01, 21.33it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  91% 308/337 [00:09<00:01, 20.97it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  92% 311/337 [00:10<00:01, 21.28it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  93% 314/337 [00:10<00:01, 21.45it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  94% 317/337 [00:10<00:00, 21.05it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  95% 320/337 [00:10<00:00, 20.00it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  96% 323/337 [00:10<00:00, 20.03it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.64it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 19.02it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.42it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 17.91it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.68it/s]\u001b[A\n","epoch 002 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.67it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:17:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.215 | ppl 74.28 | wps 85733.4 | wpb 2891.6 | bsz 127.6 | num_updates 3123 | best_loss 6.215\n","2022-05-26 08:17:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3123 updates\n","2022-05-26 08:17:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:17:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_best.pt (epoch 2 @ 3123 updates, score 6.215) (writing took 6.404750450999927 seconds)\n","2022-05-26 08:17:36 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n","2022-05-26 08:17:36 | INFO | train | epoch 002 | loss 6.702 | ppl 104.11 | wps 25701.7 | ups 8.8 | wpb 2919.2 | bsz 128 | num_updates 3123 | lr 0.0005 | gnorm 1.01 | loss_scale 16 | train_wall 155 | gb_free 10.5 | wall 359\n","epoch 003:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:17:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","2022-05-26 08:17:36 | INFO | fairseq.trainer | begin training epoch 3\n","2022-05-26 08:17:36 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 003: 100% 1561/1563 [02:39<00:00, 12.02it/s, loss=5.552, ppl=46.92, wps=28697.9, ups=9.37, wpb=3064, bsz=128, num_updates=4600, lr=0.0005, gnorm=1.14, loss_scale=16, train_wall=10, gb_free=10.3, wall=509]2022-05-26 08:20:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 003 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   0% 1/337 [00:00<00:35,  9.40it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   1% 5/337 [00:00<00:13, 24.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   2% 8/337 [00:00<00:13, 24.86it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   4% 13/337 [00:00<00:10, 31.91it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   5% 17/337 [00:00<00:09, 33.53it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   6% 21/337 [00:00<00:08, 35.26it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   8% 26/337 [00:00<00:08, 36.07it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:   9% 31/337 [00:00<00:07, 39.86it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  11% 36/337 [00:01<00:07, 40.15it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  12% 41/337 [00:01<00:07, 38.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  14% 46/337 [00:01<00:07, 39.03it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  15% 50/337 [00:01<00:07, 36.60it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  16% 55/337 [00:01<00:07, 37.84it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  18% 59/337 [00:01<00:07, 35.71it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  19% 64/337 [00:01<00:07, 38.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  20% 68/337 [00:01<00:07, 35.51it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  22% 73/337 [00:02<00:07, 37.44it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  23% 77/337 [00:02<00:07, 37.14it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  24% 81/337 [00:02<00:06, 36.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  25% 85/337 [00:02<00:06, 37.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  26% 89/337 [00:02<00:07, 34.96it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  28% 94/337 [00:02<00:06, 37.67it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  29% 99/337 [00:02<00:06, 38.41it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  31% 103/337 [00:02<00:06, 36.63it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  32% 108/337 [00:02<00:06, 37.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  33% 112/337 [00:03<00:06, 36.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  34% 116/337 [00:03<00:06, 35.38it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  36% 120/337 [00:03<00:06, 35.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  37% 124/337 [00:03<00:05, 36.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  38% 128/337 [00:03<00:06, 34.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  39% 132/337 [00:03<00:05, 35.56it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  40% 136/337 [00:03<00:05, 35.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  42% 140/337 [00:03<00:05, 33.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  43% 144/337 [00:04<00:05, 34.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  44% 148/337 [00:04<00:05, 34.76it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  45% 152/337 [00:04<00:05, 33.01it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  46% 156/337 [00:04<00:05, 34.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  47% 160/337 [00:04<00:05, 34.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  49% 164/337 [00:04<00:05, 31.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  50% 168/337 [00:04<00:05, 33.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  51% 172/337 [00:04<00:04, 33.32it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  52% 176/337 [00:05<00:05, 30.84it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  53% 180/337 [00:05<00:04, 32.58it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  55% 184/337 [00:05<00:04, 32.95it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  56% 188/337 [00:05<00:04, 30.55it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  57% 192/337 [00:05<00:04, 31.63it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  58% 196/337 [00:05<00:04, 32.46it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  59% 200/337 [00:05<00:04, 30.55it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  61% 204/337 [00:05<00:04, 31.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  62% 208/337 [00:06<00:04, 31.16it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  63% 212/337 [00:06<00:04, 30.58it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  64% 216/337 [00:06<00:03, 31.18it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  65% 220/337 [00:06<00:03, 29.36it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  66% 224/337 [00:06<00:03, 30.65it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  68% 228/337 [00:06<00:03, 30.56it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  69% 232/337 [00:06<00:03, 29.82it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  70% 236/337 [00:06<00:03, 30.43it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  71% 240/337 [00:07<00:03, 29.02it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  72% 244/337 [00:07<00:03, 30.22it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  74% 248/337 [00:07<00:02, 30.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  75% 252/337 [00:07<00:02, 28.64it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  76% 256/337 [00:07<00:02, 29.10it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  77% 259/337 [00:07<00:02, 27.26it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  78% 263/337 [00:07<00:02, 28.05it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  79% 266/337 [00:08<00:02, 26.28it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  80% 269/337 [00:08<00:02, 27.11it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  81% 272/337 [00:08<00:02, 25.81it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 275/337 [00:08<00:02, 26.68it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  82% 278/337 [00:08<00:02, 26.53it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  83% 281/337 [00:08<00:02, 26.08it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  84% 284/337 [00:08<00:02, 26.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  85% 287/337 [00:08<00:02, 24.80it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  86% 290/337 [00:08<00:01, 24.94it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  87% 293/337 [00:09<00:01, 24.31it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  88% 296/337 [00:09<00:01, 23.24it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  89% 299/337 [00:09<00:01, 23.69it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  90% 302/337 [00:09<00:01, 23.59it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 305/337 [00:09<00:01, 17.34it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  91% 308/337 [00:09<00:01, 18.25it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  92% 311/337 [00:10<00:01, 19.12it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  93% 314/337 [00:10<00:01, 19.84it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  94% 317/337 [00:10<00:01, 19.72it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  95% 320/337 [00:10<00:00, 19.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  96% 322/337 [00:10<00:00, 19.47it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  96% 324/337 [00:10<00:00, 19.26it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.10it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 18.61it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.17it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 17.86it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.53it/s]\u001b[A\n","epoch 003 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.68it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:20:27 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.796 | ppl 55.55 | wps 85442.9 | wpb 2891.6 | bsz 127.6 | num_updates 4686 | best_loss 5.796\n","2022-05-26 08:20:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4686 updates\n","2022-05-26 08:20:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","epoch 003: 100% 1561/1563 [02:51<00:00, 12.02it/s, loss=5.552, ppl=46.92, wps=28697.9, ups=9.37, wpb=3064, bsz=128, num_updates=4600, lr=0.0005, gnorm=1.14, loss_scale=16, train_wall=10, gb_free=10.3, wall=509]2022-05-26 08:20:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:20:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_best.pt (epoch 3 @ 4686 updates, score 5.796) (writing took 6.376317407999977 seconds)\n","2022-05-26 08:20:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n","2022-05-26 08:20:33 | INFO | train | epoch 003 | loss 5.609 | ppl 48.82 | wps 25738.2 | ups 8.82 | wpb 2919.2 | bsz 128 | num_updates 4686 | lr 0.0005 | gnorm 1.133 | loss_scale 16 | train_wall 155 | gb_free 7 | wall 536\n","2022-05-26 08:20:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 004:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:20:33 | INFO | fairseq.trainer | begin training epoch 4\n","2022-05-26 08:20:33 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 004: 100% 1561/1563 [02:39<00:00,  9.99it/s, loss=4.772, ppl=27.32, wps=29131.8, ups=10.23, wpb=2847.7, bsz=128, num_updates=6200, lr=0.0005, gnorm=1.212, loss_scale=16, train_wall=10, gb_free=12.4, wall=691]2022-05-26 08:23:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 004 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   0% 1/337 [00:00<00:41,  8.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   1% 4/337 [00:00<00:17, 19.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   2% 8/337 [00:00<00:13, 24.86it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   4% 13/337 [00:00<00:10, 30.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   5% 17/337 [00:00<00:09, 32.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   6% 21/337 [00:00<00:09, 34.69it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   8% 26/337 [00:00<00:08, 34.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:   9% 31/337 [00:00<00:08, 37.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  10% 35/337 [00:01<00:07, 37.99it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  12% 40/337 [00:01<00:08, 36.34it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  13% 45/337 [00:01<00:07, 38.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  15% 49/337 [00:01<00:08, 35.80it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  16% 53/337 [00:01<00:07, 36.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  17% 57/337 [00:01<00:07, 36.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  18% 61/337 [00:01<00:07, 35.70it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  19% 65/337 [00:01<00:07, 36.69it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  20% 69/337 [00:02<00:07, 33.61it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  22% 74/337 [00:02<00:07, 35.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  23% 78/337 [00:02<00:07, 33.50it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  25% 83/337 [00:02<00:07, 36.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  26% 87/337 [00:02<00:06, 36.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  27% 91/337 [00:02<00:07, 34.10it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  28% 95/337 [00:02<00:06, 34.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  29% 99/337 [00:02<00:06, 35.82it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  31% 103/337 [00:02<00:06, 34.49it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  32% 107/337 [00:03<00:06, 35.76it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  33% 111/337 [00:03<00:06, 36.34it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  34% 115/337 [00:03<00:06, 35.00it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  36% 120/337 [00:03<00:05, 36.64it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  37% 124/337 [00:03<00:05, 36.46it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  38% 128/337 [00:03<00:05, 34.91it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  39% 132/337 [00:03<00:05, 36.22it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  40% 136/337 [00:03<00:05, 35.81it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  42% 140/337 [00:04<00:05, 34.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  43% 144/337 [00:04<00:05, 35.48it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  44% 148/337 [00:04<00:05, 34.66it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  45% 152/337 [00:04<00:05, 32.73it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  46% 156/337 [00:04<00:05, 34.08it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  47% 160/337 [00:04<00:05, 34.72it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  49% 164/337 [00:04<00:05, 31.47it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  50% 168/337 [00:04<00:05, 33.17it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  51% 172/337 [00:04<00:04, 33.59it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  52% 176/337 [00:05<00:05, 31.19it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  53% 180/337 [00:05<00:04, 32.33it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  55% 184/337 [00:05<00:04, 32.63it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  56% 188/337 [00:05<00:04, 29.99it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  57% 192/337 [00:05<00:04, 31.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  58% 196/337 [00:05<00:04, 32.39it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  59% 200/337 [00:05<00:04, 29.67it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  61% 204/337 [00:06<00:04, 30.58it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  62% 208/337 [00:06<00:04, 31.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  63% 212/337 [00:06<00:04, 30.39it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  64% 216/337 [00:06<00:03, 31.52it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  65% 220/337 [00:06<00:03, 29.78it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  66% 224/337 [00:06<00:03, 30.14it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  68% 228/337 [00:06<00:03, 30.77it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  69% 232/337 [00:06<00:03, 30.05it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  70% 236/337 [00:07<00:03, 30.44it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  71% 240/337 [00:07<00:03, 29.09it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  72% 244/337 [00:07<00:03, 29.98it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  74% 248/337 [00:07<00:02, 30.26it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  75% 252/337 [00:07<00:02, 28.58it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  76% 256/337 [00:07<00:02, 29.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  77% 259/337 [00:07<00:02, 27.20it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  78% 263/337 [00:08<00:02, 27.76it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  79% 266/337 [00:08<00:02, 26.31it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  80% 270/337 [00:08<00:02, 27.76it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  81% 273/337 [00:08<00:02, 26.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  82% 276/337 [00:08<00:02, 27.03it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  83% 279/337 [00:08<00:02, 25.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  84% 282/337 [00:08<00:02, 26.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 285/337 [00:08<00:02, 24.55it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  85% 288/337 [00:09<00:01, 25.15it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  86% 291/337 [00:09<00:01, 23.74it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  87% 294/337 [00:09<00:01, 24.24it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  88% 297/337 [00:09<00:01, 23.31it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  89% 300/337 [00:09<00:01, 23.40it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  90% 303/337 [00:09<00:01, 23.90it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  91% 306/337 [00:09<00:01, 21.27it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  92% 309/337 [00:10<00:01, 21.23it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 312/337 [00:10<00:01, 21.35it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  93% 315/337 [00:10<00:01, 21.58it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  94% 318/337 [00:10<00:00, 21.25it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  95% 321/337 [00:10<00:00, 20.32it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  96% 324/337 [00:10<00:00, 19.72it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.46it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 18.86it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.38it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 18.02it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.62it/s]\u001b[A\n","epoch 004 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.74it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:23:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.505 | ppl 45.4 | wps 85526.5 | wpb 2891.6 | bsz 127.6 | num_updates 6249 | best_loss 5.505\n","2022-05-26 08:23:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6249 updates\n","2022-05-26 08:23:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","epoch 004: 100% 1561/1563 [02:54<00:00,  9.99it/s, loss=4.772, ppl=27.32, wps=29131.8, ups=10.23, wpb=2847.7, bsz=128, num_updates=6200, lr=0.0005, gnorm=1.212, loss_scale=16, train_wall=10, gb_free=12.4, wall=691]2022-05-26 08:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_best.pt (epoch 4 @ 6249 updates, score 5.505) (writing took 6.455540431000145 seconds)\n","2022-05-26 08:23:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n","2022-05-26 08:23:31 | INFO | train | epoch 004 | loss 4.831 | ppl 28.46 | wps 25736.3 | ups 8.82 | wpb 2919.2 | bsz 128 | num_updates 6249 | lr 0.0005 | gnorm 1.188 | loss_scale 16 | train_wall 154 | gb_free 11.7 | wall 713\n","2022-05-26 08:23:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 005:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:23:31 | INFO | fairseq.trainer | begin training epoch 5\n","2022-05-26 08:23:31 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 005: 100% 1562/1563 [02:38<00:00, 11.23it/s, loss=4.354, ppl=20.45, wps=27691.6, ups=9.85, wpb=2810.8, bsz=128, num_updates=7800, lr=0.0005, gnorm=1.264, loss_scale=16, train_wall=10, gb_free=10.2, wall=871]2022-05-26 08:26:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 005 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   0% 1/337 [00:00<00:38,  8.74it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   1% 5/337 [00:00<00:14, 23.36it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   3% 9/337 [00:00<00:11, 29.13it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   4% 14/337 [00:00<00:10, 31.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   6% 19/337 [00:00<00:09, 34.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   7% 24/337 [00:00<00:08, 38.55it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   8% 28/337 [00:00<00:08, 36.87it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:   9% 32/337 [00:00<00:08, 36.43it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  11% 37/337 [00:01<00:07, 39.58it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  12% 41/337 [00:01<00:07, 37.26it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  14% 46/337 [00:01<00:07, 39.67it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  15% 51/337 [00:01<00:07, 37.93it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  17% 56/337 [00:01<00:07, 39.68it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  18% 61/337 [00:01<00:07, 36.86it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  19% 65/337 [00:01<00:07, 37.05it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  20% 69/337 [00:01<00:07, 34.97it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  22% 74/337 [00:02<00:07, 36.97it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  23% 78/337 [00:02<00:07, 35.78it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  25% 83/337 [00:02<00:06, 37.77it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  26% 87/337 [00:02<00:06, 38.18it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  27% 91/337 [00:02<00:06, 36.63it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  28% 96/337 [00:02<00:06, 37.62it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  30% 100/337 [00:02<00:06, 35.61it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  31% 105/337 [00:02<00:06, 37.48it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  32% 109/337 [00:03<00:06, 36.81it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  34% 113/337 [00:03<00:06, 34.35it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  35% 117/337 [00:03<00:06, 35.66it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  36% 121/337 [00:03<00:05, 36.06it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  37% 125/337 [00:03<00:06, 34.51it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  38% 129/337 [00:03<00:05, 35.10it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  39% 133/337 [00:03<00:05, 36.14it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  41% 137/337 [00:03<00:05, 34.33it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  42% 141/337 [00:03<00:05, 34.03it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  43% 145/337 [00:04<00:05, 34.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  44% 149/337 [00:04<00:05, 33.94it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  45% 153/337 [00:04<00:05, 31.46it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  47% 157/337 [00:04<00:05, 32.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  48% 161/337 [00:04<00:05, 33.60it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  49% 165/337 [00:04<00:05, 31.21it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  50% 169/337 [00:04<00:05, 31.93it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  51% 173/337 [00:04<00:05, 32.44it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  53% 177/337 [00:05<00:05, 30.86it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  54% 181/337 [00:05<00:04, 32.05it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  55% 185/337 [00:05<00:04, 32.00it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  56% 189/337 [00:05<00:04, 30.07it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  57% 193/337 [00:05<00:04, 31.30it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  58% 197/337 [00:05<00:04, 31.47it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  60% 201/337 [00:05<00:04, 30.29it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  61% 205/337 [00:05<00:04, 31.44it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  62% 209/337 [00:06<00:04, 29.85it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  63% 213/337 [00:06<00:03, 31.59it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  64% 217/337 [00:06<00:03, 32.03it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  66% 221/337 [00:06<00:03, 29.97it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  67% 225/337 [00:06<00:03, 30.63it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  68% 229/337 [00:06<00:03, 30.70it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  69% 233/337 [00:06<00:03, 30.06it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  70% 237/337 [00:07<00:03, 30.17it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 241/337 [00:07<00:03, 29.08it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  72% 244/337 [00:07<00:03, 29.18it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  74% 248/337 [00:07<00:03, 29.52it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  74% 251/337 [00:07<00:03, 27.79it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  76% 255/337 [00:07<00:02, 28.53it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 258/337 [00:07<00:02, 26.43it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  77% 261/337 [00:07<00:02, 27.29it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  78% 264/337 [00:08<00:02, 27.60it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  79% 267/337 [00:08<00:02, 26.04it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  80% 271/337 [00:08<00:02, 27.27it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  81% 274/337 [00:08<00:02, 26.07it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  82% 277/337 [00:08<00:02, 26.26it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  83% 280/337 [00:08<00:02, 25.80it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  84% 283/337 [00:08<00:02, 26.64it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  85% 286/337 [00:08<00:02, 24.76it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  86% 289/337 [00:09<00:01, 25.25it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  87% 292/337 [00:09<00:01, 23.95it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 295/337 [00:09<00:01, 24.53it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  88% 298/337 [00:09<00:01, 23.51it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  89% 301/337 [00:09<00:01, 23.75it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  90% 304/337 [00:09<00:01, 21.50it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  91% 307/337 [00:09<00:01, 21.84it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  92% 310/337 [00:09<00:01, 21.49it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  93% 313/337 [00:10<00:01, 21.63it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  94% 316/337 [00:10<00:00, 21.80it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  95% 319/337 [00:10<00:00, 19.83it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  96% 322/337 [00:10<00:00, 20.27it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  96% 325/337 [00:10<00:00, 19.62it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  97% 327/337 [00:10<00:00, 19.30it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 329/337 [00:10<00:00, 18.91it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  98% 331/337 [00:11<00:00, 18.40it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  99% 333/337 [00:11<00:00, 17.83it/s]\u001b[A\n","epoch 005 | valid on 'valid' subset:  99% 335/337 [00:11<00:00, 16.06it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:26:21 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.41 | ppl 42.51 | wps 86030.6 | wpb 2891.6 | bsz 127.6 | num_updates 7812 | best_loss 5.41\n","2022-05-26 08:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7812 updates\n","2022-05-26 08:26:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_best.pt (epoch 5 @ 7812 updates, score 5.41) (writing took 6.976471069000127 seconds)\n","2022-05-26 08:26:28 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n","2022-05-26 08:26:28 | INFO | train | epoch 005 | loss 4.243 | ppl 18.94 | wps 25712.1 | ups 8.81 | wpb 2919.2 | bsz 128 | num_updates 7812 | lr 0.0005 | gnorm 1.222 | loss_scale 16 | train_wall 155 | gb_free 10.9 | wall 891\n","2022-05-26 08:26:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 006:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:26:28 | INFO | fairseq.trainer | begin training epoch 6\n","2022-05-26 08:26:28 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 006: 100% 1561/1563 [02:38<00:00, 11.39it/s, loss=3.6, ppl=12.12, wps=28179.6, ups=10.6, wpb=2657.8, bsz=128, num_updates=9300, lr=0.0005, gnorm=1.256, loss_scale=16, train_wall=9, gb_free=11.2, wall=1042]2022-05-26 08:29:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 006 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   0% 1/337 [00:00<00:41,  8.10it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   1% 5/337 [00:00<00:14, 22.92it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   3% 9/337 [00:00<00:11, 28.75it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   4% 13/337 [00:00<00:10, 31.86it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   5% 18/337 [00:00<00:08, 36.08it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   7% 23/337 [00:00<00:08, 38.53it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   8% 27/337 [00:00<00:08, 37.47it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:   9% 32/337 [00:00<00:07, 39.00it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  11% 37/337 [00:01<00:07, 40.98it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  12% 42/337 [00:01<00:07, 39.34it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  14% 47/337 [00:01<00:06, 41.51it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  15% 52/337 [00:01<00:07, 38.97it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  17% 57/337 [00:01<00:07, 39.01it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  18% 61/337 [00:01<00:07, 37.51it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  20% 66/337 [00:01<00:06, 38.75it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  21% 70/337 [00:01<00:07, 36.22it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  22% 75/337 [00:02<00:07, 36.78it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  23% 79/337 [00:02<00:07, 35.68it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  25% 84/337 [00:02<00:06, 37.42it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  26% 88/337 [00:02<00:06, 37.19it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  27% 92/337 [00:02<00:06, 36.64it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  29% 97/337 [00:02<00:06, 38.04it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  30% 101/337 [00:02<00:06, 36.77it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  31% 106/337 [00:02<00:05, 38.67it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  33% 110/337 [00:02<00:05, 38.08it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  34% 114/337 [00:03<00:06, 34.93it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  35% 118/337 [00:03<00:06, 35.93it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  36% 122/337 [00:03<00:05, 36.08it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  37% 126/337 [00:03<00:06, 34.36it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  39% 130/337 [00:03<00:05, 35.32it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  40% 134/337 [00:03<00:05, 35.74it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  41% 138/337 [00:03<00:05, 33.29it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  42% 142/337 [00:03<00:05, 34.90it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  43% 146/337 [00:04<00:05, 35.15it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  45% 150/337 [00:04<00:05, 33.62it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  46% 154/337 [00:04<00:05, 32.98it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  47% 158/337 [00:04<00:05, 34.41it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  48% 162/337 [00:04<00:05, 33.23it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  49% 166/337 [00:04<00:05, 32.33it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  50% 170/337 [00:04<00:05, 33.07it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  52% 174/337 [00:04<00:04, 32.61it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  53% 178/337 [00:05<00:05, 31.05it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  54% 182/337 [00:05<00:04, 31.79it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  55% 186/337 [00:05<00:04, 32.08it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  56% 190/337 [00:05<00:04, 30.45it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  58% 194/337 [00:05<00:04, 31.19it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  59% 198/337 [00:05<00:04, 29.63it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  60% 202/337 [00:05<00:04, 30.32it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  61% 206/337 [00:05<00:04, 30.72it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  62% 210/337 [00:06<00:04, 29.52it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  64% 214/337 [00:06<00:04, 30.25it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  65% 218/337 [00:06<00:03, 30.47it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  66% 222/337 [00:06<00:03, 28.78it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  67% 226/337 [00:06<00:03, 29.01it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  68% 229/337 [00:06<00:03, 28.89it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  69% 232/337 [00:06<00:03, 28.79it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  70% 235/337 [00:06<00:03, 28.91it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  71% 239/337 [00:07<00:03, 29.09it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  72% 242/337 [00:07<00:03, 27.59it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  73% 246/337 [00:07<00:03, 28.70it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  74% 249/337 [00:07<00:03, 26.72it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  75% 253/337 [00:07<00:03, 27.83it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  76% 256/337 [00:07<00:02, 27.90it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  77% 259/337 [00:07<00:03, 25.98it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  78% 263/337 [00:07<00:02, 27.34it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  79% 266/337 [00:08<00:02, 25.62it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  80% 269/337 [00:08<00:02, 25.97it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  81% 272/337 [00:08<00:02, 25.03it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  82% 275/337 [00:08<00:02, 25.86it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  82% 278/337 [00:08<00:02, 25.95it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  83% 281/337 [00:08<00:02, 25.93it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  84% 284/337 [00:08<00:02, 26.27it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  85% 287/337 [00:08<00:02, 24.68it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  86% 290/337 [00:09<00:01, 24.81it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  87% 293/337 [00:09<00:01, 23.78it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  88% 296/337 [00:09<00:01, 22.42it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  89% 299/337 [00:09<00:01, 23.36it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  90% 302/337 [00:09<00:01, 23.31it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  91% 305/337 [00:09<00:01, 21.00it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  91% 308/337 [00:09<00:01, 20.92it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  92% 311/337 [00:10<00:01, 20.90it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  93% 314/337 [00:10<00:01, 20.92it/s]\u001b[A\n","epoch 006: 100% 1561/1563 [02:49<00:00, 11.39it/s, loss=3.6, ppl=12.12, wps=28179.6, ups=10.6, wpb=2657.8, bsz=128, num_updates=9300, lr=0.0005, gnorm=1.256, loss_scale=16, train_wall=9, gb_free=11.2, wall=1042]\n","epoch 006 | valid on 'valid' subset:  95% 320/337 [00:10<00:00, 20.25it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  96% 323/337 [00:10<00:00, 20.19it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.77it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 19.07it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.54it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 18.08it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.76it/s]\u001b[A\n","epoch 006 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.84it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:29:18 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.405 | ppl 42.38 | wps 85771.4 | wpb 2891.6 | bsz 127.6 | num_updates 9375 | best_loss 5.405\n","2022-05-26 08:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9375 updates\n","2022-05-26 08:29:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:29:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:29:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_best.pt (epoch 6 @ 9375 updates, score 5.405) (writing took 6.433926878999955 seconds)\n","2022-05-26 08:29:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n","2022-05-26 08:29:25 | INFO | train | epoch 006 | loss 3.771 | ppl 13.65 | wps 25811.9 | ups 8.84 | wpb 2919.2 | bsz 128 | num_updates 9375 | lr 0.0005 | gnorm 1.236 | loss_scale 16 | train_wall 155 | gb_free 12.4 | wall 1068\n","2022-05-26 08:29:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 007:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:29:25 | INFO | fairseq.trainer | begin training epoch 7\n","2022-05-26 08:29:25 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 007: 100% 1561/1563 [02:39<00:00, 12.34it/s, loss=3.476, ppl=11.13, wps=29606.2, ups=9.82, wpb=3014, bsz=128, num_updates=10900, lr=0.0005, gnorm=1.309, loss_scale=16, train_wall=10, gb_free=12, wall=1224]2022-05-26 08:32:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 007 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   0% 1/337 [00:00<00:38,  8.72it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   1% 4/337 [00:00<00:17, 18.61it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   2% 8/337 [00:00<00:13, 24.73it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   4% 13/337 [00:00<00:10, 32.01it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   5% 18/337 [00:00<00:08, 35.97it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   7% 23/337 [00:00<00:08, 38.74it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   8% 27/337 [00:00<00:08, 36.21it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:   9% 32/337 [00:00<00:08, 37.51it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  11% 37/337 [00:01<00:07, 39.65it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  12% 41/337 [00:01<00:07, 37.91it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  14% 46/337 [00:01<00:07, 40.32it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  15% 51/337 [00:01<00:07, 38.82it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  17% 56/337 [00:01<00:06, 40.37it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  18% 61/337 [00:01<00:07, 38.08it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  20% 66/337 [00:01<00:07, 38.61it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  21% 70/337 [00:01<00:07, 36.51it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  22% 75/337 [00:02<00:06, 37.83it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  23% 79/337 [00:02<00:07, 35.69it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  25% 84/337 [00:02<00:06, 37.82it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  26% 88/337 [00:02<00:06, 37.42it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  27% 92/337 [00:02<00:06, 35.91it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  29% 97/337 [00:02<00:06, 37.40it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  30% 101/337 [00:02<00:06, 35.76it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  31% 105/337 [00:02<00:06, 36.77it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  32% 109/337 [00:03<00:06, 36.77it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  34% 113/337 [00:03<00:06, 33.76it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  35% 118/337 [00:03<00:06, 35.95it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  36% 122/337 [00:03<00:05, 36.45it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  37% 126/337 [00:03<00:06, 34.64it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  39% 131/337 [00:03<00:05, 36.05it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  40% 135/337 [00:03<00:05, 36.43it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  41% 139/337 [00:03<00:05, 33.67it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  42% 143/337 [00:03<00:05, 34.70it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  44% 147/337 [00:04<00:05, 34.82it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  45% 151/337 [00:04<00:05, 32.72it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  46% 155/337 [00:04<00:05, 33.57it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  47% 159/337 [00:04<00:05, 33.28it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  48% 163/337 [00:04<00:05, 31.39it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  50% 167/337 [00:04<00:05, 33.08it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  51% 171/337 [00:04<00:04, 33.50it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  52% 175/337 [00:04<00:05, 31.36it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  53% 179/337 [00:05<00:04, 31.84it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  54% 183/337 [00:05<00:04, 32.82it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  55% 187/337 [00:05<00:04, 30.29it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  57% 191/337 [00:05<00:04, 31.68it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  58% 195/337 [00:05<00:04, 32.60it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  59% 199/337 [00:05<00:04, 30.44it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  60% 203/337 [00:05<00:04, 31.52it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  61% 207/337 [00:05<00:04, 31.74it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  63% 211/337 [00:06<00:04, 30.71it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  64% 215/337 [00:06<00:03, 31.40it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  65% 219/337 [00:06<00:03, 31.22it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  66% 223/337 [00:06<00:03, 29.58it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  67% 227/337 [00:06<00:03, 30.33it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  69% 231/337 [00:06<00:03, 29.25it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  70% 235/337 [00:06<00:03, 30.04it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  71% 239/337 [00:07<00:03, 30.06it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  72% 243/337 [00:07<00:03, 28.94it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  73% 247/337 [00:07<00:03, 29.38it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  74% 250/337 [00:07<00:03, 27.43it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  75% 254/337 [00:07<00:02, 28.53it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  76% 257/337 [00:07<00:03, 26.20it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  77% 261/337 [00:07<00:02, 27.59it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  78% 264/337 [00:07<00:02, 27.65it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  79% 267/337 [00:08<00:02, 26.38it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  80% 270/337 [00:08<00:02, 26.69it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  81% 273/337 [00:08<00:02, 25.17it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  82% 276/337 [00:08<00:02, 26.09it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  83% 279/337 [00:08<00:02, 25.50it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  84% 282/337 [00:08<00:02, 26.33it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  85% 285/337 [00:08<00:02, 24.09it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  85% 288/337 [00:08<00:01, 24.92it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  86% 291/337 [00:09<00:01, 23.44it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  87% 294/337 [00:09<00:01, 23.81it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  88% 297/337 [00:09<00:01, 22.61it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  89% 300/337 [00:09<00:01, 22.75it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  90% 303/337 [00:09<00:01, 23.04it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  91% 306/337 [00:09<00:01, 21.09it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  92% 309/337 [00:09<00:01, 20.85it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  93% 312/337 [00:10<00:01, 21.14it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  93% 315/337 [00:10<00:01, 21.39it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  94% 318/337 [00:10<00:00, 20.98it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  95% 321/337 [00:10<00:00, 20.11it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  96% 324/337 [00:10<00:00, 19.67it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.38it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 18.69it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.32it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 17.79it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.52it/s]\u001b[A\n","epoch 007 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.65it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:32:16 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.487 | ppl 44.84 | wps 85862.7 | wpb 2891.6 | bsz 127.6 | num_updates 10938 | best_loss 5.405\n","2022-05-26 08:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10938 updates\n","2022-05-26 08:32:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","epoch 007: 100% 1561/1563 [02:52<00:00, 12.34it/s, loss=3.476, ppl=11.13, wps=29606.2, ups=9.82, wpb=3014, bsz=128, num_updates=10900, lr=0.0005, gnorm=1.309, loss_scale=16, train_wall=10, gb_free=12, wall=1224]2022-05-26 08:32:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_last.pt (epoch 7 @ 10938 updates, score 5.487) (writing took 3.133619453999927 seconds)\n","2022-05-26 08:32:19 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n","2022-05-26 08:32:19 | INFO | train | epoch 007 | loss 3.398 | ppl 10.54 | wps 26187.8 | ups 8.97 | wpb 2919.2 | bsz 128 | num_updates 10938 | lr 0.0005 | gnorm 1.255 | loss_scale 16 | train_wall 155 | gb_free 11.9 | wall 1242\n","epoch 008:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:32:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","2022-05-26 08:32:19 | INFO | fairseq.trainer | begin training epoch 8\n","2022-05-26 08:32:19 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 008: 100% 1561/1563 [02:38<00:00, 11.21it/s, loss=3.239, ppl=9.44, wps=29731.5, ups=9.6, wpb=3098.3, bsz=128, num_updates=12400, lr=0.0005, gnorm=1.28, loss_scale=16, train_wall=10, gb_free=11.2, wall=1390]2022-05-26 08:34:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 008 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   0% 1/337 [00:00<00:42,  7.99it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   1% 4/337 [00:00<00:18, 18.15it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   2% 8/337 [00:00<00:12, 25.90it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   4% 13/337 [00:00<00:09, 32.58it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   5% 18/337 [00:00<00:08, 35.58it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   7% 23/337 [00:00<00:08, 38.16it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   8% 27/337 [00:00<00:08, 36.82it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:   9% 32/337 [00:00<00:08, 36.91it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  11% 38/337 [00:01<00:07, 41.56it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  13% 43/337 [00:01<00:07, 39.96it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  14% 48/337 [00:01<00:07, 40.18it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  16% 53/337 [00:01<00:07, 39.03it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  17% 57/337 [00:01<00:07, 38.91it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  18% 61/337 [00:01<00:07, 37.04it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  20% 66/337 [00:01<00:07, 38.54it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  21% 70/337 [00:01<00:07, 34.62it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  22% 75/337 [00:02<00:07, 36.89it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  23% 79/337 [00:02<00:07, 36.04it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  25% 83/337 [00:02<00:06, 36.90it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  26% 87/337 [00:02<00:06, 37.63it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  27% 91/337 [00:02<00:06, 35.84it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  28% 95/337 [00:02<00:06, 36.61it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  29% 99/337 [00:02<00:06, 37.54it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  31% 103/337 [00:02<00:06, 35.06it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  32% 108/337 [00:02<00:06, 36.18it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  33% 112/337 [00:03<00:06, 36.36it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  34% 116/337 [00:03<00:06, 34.68it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  36% 120/337 [00:03<00:06, 36.04it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  37% 124/337 [00:03<00:05, 36.36it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  38% 128/337 [00:03<00:05, 35.41it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  39% 132/337 [00:03<00:05, 35.51it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  40% 136/337 [00:03<00:05, 35.71it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  42% 140/337 [00:03<00:05, 34.22it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  43% 144/337 [00:04<00:05, 34.78it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  44% 148/337 [00:04<00:05, 35.50it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  45% 152/337 [00:04<00:05, 33.14it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  46% 156/337 [00:04<00:05, 34.23it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  47% 160/337 [00:04<00:05, 33.89it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  49% 164/337 [00:04<00:05, 31.50it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  50% 168/337 [00:04<00:05, 32.93it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  51% 172/337 [00:04<00:04, 33.48it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  52% 176/337 [00:05<00:05, 30.14it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  53% 180/337 [00:05<00:05, 31.33it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  55% 184/337 [00:05<00:04, 31.53it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  56% 188/337 [00:05<00:05, 29.44it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  57% 192/337 [00:05<00:04, 31.36it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  58% 196/337 [00:05<00:04, 31.46it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  59% 200/337 [00:05<00:04, 29.76it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  61% 204/337 [00:05<00:04, 30.65it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  62% 208/337 [00:06<00:04, 30.43it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  63% 212/337 [00:06<00:04, 30.00it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  64% 216/337 [00:06<00:03, 30.77it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  65% 220/337 [00:06<00:04, 27.11it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  66% 224/337 [00:06<00:03, 28.35it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  68% 228/337 [00:06<00:03, 28.96it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  69% 231/337 [00:06<00:03, 27.96it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  70% 235/337 [00:07<00:03, 28.81it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  71% 238/337 [00:07<00:03, 28.24it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  72% 241/337 [00:07<00:03, 27.54it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  73% 245/337 [00:07<00:03, 28.83it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  74% 248/337 [00:07<00:03, 28.14it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  74% 251/337 [00:07<00:03, 26.83it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  76% 255/337 [00:07<00:02, 28.33it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  77% 258/337 [00:07<00:02, 26.50it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  78% 262/337 [00:08<00:02, 27.17it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  79% 265/337 [00:08<00:02, 25.36it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  80% 269/337 [00:08<00:02, 26.84it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  81% 272/337 [00:08<00:02, 25.57it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  82% 275/337 [00:08<00:02, 26.43it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  82% 278/337 [00:08<00:02, 26.48it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  83% 281/337 [00:08<00:02, 26.27it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  84% 284/337 [00:08<00:01, 26.79it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  85% 287/337 [00:08<00:02, 24.97it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  86% 290/337 [00:09<00:01, 24.98it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  87% 293/337 [00:09<00:01, 24.18it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  88% 296/337 [00:09<00:01, 23.08it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  89% 299/337 [00:09<00:01, 23.64it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  90% 302/337 [00:09<00:01, 23.69it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  91% 305/337 [00:09<00:01, 21.08it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  91% 308/337 [00:09<00:01, 21.02it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  92% 311/337 [00:10<00:01, 21.16it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  93% 314/337 [00:10<00:01, 21.37it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  94% 317/337 [00:10<00:00, 21.24it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  95% 320/337 [00:10<00:00, 20.55it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  96% 323/337 [00:10<00:00, 20.32it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.84it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 19.00it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.62it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 17.96it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.58it/s]\u001b[A\n","epoch 008 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.71it/s]\u001b[A\n","                                                                          2022-05-26 08:35:10 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 5.588 | ppl 48.09 | wps 85511.9 | wpb 2891.6 | bsz 127.6 | num_updates 12501 | best_loss 5.405\n","2022-05-26 08:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12501 updates\n","2022-05-26 08:35:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","\u001b[A2022-05-26 08:35:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:35:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_last.pt (epoch 8 @ 12501 updates, score 5.588) (writing took 3.2823266680002234 seconds)\n","2022-05-26 08:35:13 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n","2022-05-26 08:35:13 | INFO | train | epoch 008 | loss 3.097 | ppl 8.55 | wps 26265.3 | ups 9 | wpb 2919.2 | bsz 128 | num_updates 12501 | lr 0.0005 | gnorm 1.254 | loss_scale 16 | train_wall 155 | gb_free 12.2 | wall 1416\n","2022-05-26 08:35:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 009:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:35:13 | INFO | fairseq.trainer | begin training epoch 9\n","2022-05-26 08:35:13 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 009: 100% 1562/1563 [02:39<00:00, 10.53it/s, loss=3.023, ppl=8.13, wps=28735, ups=9.87, wpb=2910.6, bsz=128, num_updates=14000, lr=0.0005, gnorm=1.293, loss_scale=16, train_wall=10, gb_free=10.2, wall=1568]2022-05-26 08:37:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 009 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   0% 1/337 [00:00<00:38,  8.79it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   1% 4/337 [00:00<00:16, 20.35it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   2% 8/337 [00:00<00:12, 25.75it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   4% 13/337 [00:00<00:09, 32.73it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   5% 17/337 [00:00<00:09, 34.86it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   6% 21/337 [00:00<00:08, 36.25it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   8% 26/337 [00:00<00:08, 37.48it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:   9% 31/337 [00:00<00:07, 38.87it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  10% 35/337 [00:01<00:07, 38.11it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  12% 40/337 [00:01<00:08, 36.41it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  13% 45/337 [00:01<00:07, 38.92it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  15% 49/337 [00:01<00:07, 36.77it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  16% 54/337 [00:01<00:07, 38.63it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  17% 58/337 [00:01<00:07, 36.46it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  19% 63/337 [00:01<00:07, 38.90it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  20% 67/337 [00:01<00:07, 35.73it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  21% 72/337 [00:02<00:06, 38.56it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  23% 76/337 [00:02<00:07, 36.96it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  24% 80/337 [00:02<00:07, 35.27it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  25% 85/337 [00:02<00:06, 37.07it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  26% 89/337 [00:02<00:07, 34.50it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  28% 94/337 [00:02<00:06, 37.69it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  29% 99/337 [00:02<00:06, 38.36it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  31% 103/337 [00:02<00:06, 36.96it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  32% 107/337 [00:02<00:06, 37.61it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  33% 111/337 [00:03<00:06, 36.99it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  34% 115/337 [00:03<00:06, 34.57it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  35% 119/337 [00:03<00:06, 35.92it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  36% 123/337 [00:03<00:05, 36.65it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  38% 127/337 [00:03<00:06, 34.46it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  39% 131/337 [00:03<00:05, 35.45it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  40% 135/337 [00:03<00:05, 35.70it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  41% 139/337 [00:03<00:05, 33.61it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  42% 143/337 [00:03<00:05, 34.99it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  44% 147/337 [00:04<00:05, 35.84it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  45% 151/337 [00:04<00:05, 32.94it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  46% 155/337 [00:04<00:05, 34.29it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  47% 159/337 [00:04<00:05, 34.70it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  48% 163/337 [00:04<00:05, 31.60it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  50% 167/337 [00:04<00:05, 33.01it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  51% 171/337 [00:04<00:05, 33.05it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  52% 175/337 [00:04<00:05, 30.92it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  53% 179/337 [00:05<00:04, 32.18it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  54% 183/337 [00:05<00:04, 32.43it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  55% 187/337 [00:05<00:05, 29.84it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  57% 191/337 [00:05<00:04, 31.47it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  58% 195/337 [00:05<00:04, 32.29it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  59% 199/337 [00:05<00:04, 29.64it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  60% 203/337 [00:05<00:04, 30.84it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  61% 207/337 [00:06<00:04, 31.44it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  63% 211/337 [00:06<00:04, 30.24it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  64% 215/337 [00:06<00:03, 31.17it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  65% 219/337 [00:06<00:03, 30.73it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  66% 223/337 [00:06<00:03, 29.31it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  67% 227/337 [00:06<00:03, 30.09it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  69% 231/337 [00:06<00:03, 29.24it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  70% 235/337 [00:06<00:03, 30.28it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  71% 239/337 [00:07<00:03, 29.63it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  72% 242/337 [00:07<00:03, 28.54it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  73% 246/337 [00:07<00:03, 29.29it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  74% 249/337 [00:07<00:03, 27.32it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  75% 253/337 [00:07<00:02, 28.56it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  76% 256/337 [00:07<00:02, 28.53it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  77% 259/337 [00:07<00:02, 26.77it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  78% 263/337 [00:07<00:02, 27.85it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  79% 266/337 [00:08<00:02, 26.15it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  80% 269/337 [00:08<00:02, 27.00it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  81% 272/337 [00:08<00:02, 25.93it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  82% 275/337 [00:08<00:02, 26.58it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  82% 278/337 [00:08<00:02, 25.93it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  83% 281/337 [00:08<00:02, 25.86it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  84% 284/337 [00:08<00:02, 26.04it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  85% 287/337 [00:08<00:02, 24.15it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  86% 290/337 [00:09<00:01, 24.67it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  87% 293/337 [00:09<00:01, 23.75it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  88% 296/337 [00:09<00:01, 22.73it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  89% 299/337 [00:09<00:01, 23.31it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  90% 302/337 [00:09<00:01, 23.41it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  91% 305/337 [00:09<00:01, 21.20it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  91% 308/337 [00:09<00:01, 20.95it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  92% 311/337 [00:10<00:01, 21.16it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  93% 314/337 [00:10<00:01, 21.25it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  94% 317/337 [00:10<00:00, 21.20it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  95% 320/337 [00:10<00:00, 20.42it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  96% 323/337 [00:10<00:00, 20.31it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  97% 326/337 [00:10<00:00, 19.79it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  97% 328/337 [00:10<00:00, 19.16it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  98% 330/337 [00:11<00:00, 18.60it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  99% 332/337 [00:11<00:00, 18.03it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset:  99% 334/337 [00:11<00:00, 16.75it/s]\u001b[A\n","epoch 009 | valid on 'valid' subset: 100% 336/337 [00:11<00:00, 15.74it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 08:38:04 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 5.665 | ppl 50.75 | wps 85865.9 | wpb 2891.6 | bsz 127.6 | num_updates 14064 | best_loss 5.405\n","2022-05-26 08:38:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14064 updates\n","2022-05-26 08:38:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:38:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:38:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_last.pt (epoch 9 @ 14064 updates, score 5.665) (writing took 3.244331041999885 seconds)\n","2022-05-26 08:38:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n","2022-05-26 08:38:07 | INFO | train | epoch 009 | loss 2.844 | ppl 7.18 | wps 26222.5 | ups 8.98 | wpb 2919.2 | bsz 128 | num_updates 14064 | lr 0.0005 | gnorm 1.264 | loss_scale 16 | train_wall 155 | gb_free 11.1 | wall 1590\n","epoch 010:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 08:38:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","2022-05-26 08:38:07 | INFO | fairseq.trainer | begin training epoch 10\n","2022-05-26 08:38:07 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 010: 100% 1562/1563 [02:38<00:00, 10.39it/s, loss=2.752, ppl=6.74, wps=27685, ups=10.1, wpb=2741.2, bsz=128, num_updates=15600, lr=0.0005, gnorm=1.372, loss_scale=16, train_wall=10, gb_free=12.5, wall=1746]2022-05-26 08:40:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 010 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   0% 1/337 [00:00<00:38,  8.67it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   1% 4/337 [00:00<00:17, 19.27it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   2% 8/337 [00:00<00:12, 26.27it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   4% 12/337 [00:00<00:10, 30.40it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   5% 16/337 [00:00<00:09, 32.53it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   6% 20/337 [00:00<00:09, 34.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   7% 25/337 [00:00<00:08, 38.38it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:   9% 29/337 [00:00<00:08, 37.67it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  10% 33/337 [00:01<00:08, 35.88it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  12% 39/337 [00:01<00:07, 40.50it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  13% 44/337 [00:01<00:07, 38.98it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  14% 48/337 [00:01<00:07, 37.91it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  15% 52/337 [00:01<00:07, 37.36it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  17% 57/337 [00:01<00:07, 38.16it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  18% 61/337 [00:01<00:07, 36.39it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  20% 66/337 [00:01<00:07, 37.28it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  21% 70/337 [00:02<00:07, 34.67it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  22% 75/337 [00:02<00:07, 37.01it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  23% 79/337 [00:02<00:07, 35.89it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  25% 84/337 [00:02<00:06, 37.31it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  26% 88/337 [00:02<00:06, 37.73it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  27% 92/337 [00:02<00:06, 36.15it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  29% 97/337 [00:02<00:06, 38.24it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  30% 101/337 [00:02<00:06, 36.20it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  31% 105/337 [00:02<00:06, 37.16it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  32% 109/337 [00:03<00:06, 37.75it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  34% 113/337 [00:03<00:06, 35.14it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  35% 117/337 [00:03<00:06, 36.33it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  36% 122/337 [00:03<00:05, 37.34it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  37% 126/337 [00:03<00:06, 34.54it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  39% 130/337 [00:03<00:05, 35.52it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  40% 134/337 [00:03<00:05, 35.81it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  41% 138/337 [00:03<00:06, 33.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  42% 142/337 [00:03<00:05, 33.92it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  43% 146/337 [00:04<00:05, 34.60it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  45% 150/337 [00:04<00:05, 34.41it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  46% 154/337 [00:04<00:05, 33.23it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  47% 158/337 [00:04<00:05, 33.92it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  48% 162/337 [00:04<00:05, 33.22it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  49% 166/337 [00:04<00:05, 31.85it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  50% 170/337 [00:04<00:05, 32.87it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  52% 174/337 [00:04<00:05, 32.51it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  53% 178/337 [00:05<00:05, 31.27it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  54% 182/337 [00:05<00:04, 32.72it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  55% 186/337 [00:05<00:04, 31.95it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  56% 190/337 [00:05<00:04, 30.50it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  58% 194/337 [00:05<00:04, 30.96it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  59% 198/337 [00:05<00:04, 28.66it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  60% 202/337 [00:05<00:04, 30.12it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  61% 206/337 [00:06<00:04, 30.60it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  62% 210/337 [00:06<00:04, 28.73it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  64% 214/337 [00:06<00:04, 29.90it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  65% 218/337 [00:06<00:03, 30.46it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  66% 222/337 [00:06<00:04, 28.66it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  67% 226/337 [00:06<00:03, 29.42it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  68% 229/337 [00:06<00:03, 29.53it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  69% 232/337 [00:06<00:03, 29.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  70% 236/337 [00:07<00:03, 29.71it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  71% 240/337 [00:07<00:03, 28.47it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  72% 244/337 [00:07<00:03, 29.60it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  73% 247/337 [00:07<00:03, 29.64it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  74% 250/337 [00:07<00:03, 27.67it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  75% 254/337 [00:07<00:02, 28.71it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  76% 257/337 [00:07<00:03, 26.55it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  77% 260/337 [00:07<00:02, 27.29it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  78% 264/337 [00:08<00:02, 28.04it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  79% 267/337 [00:08<00:02, 26.14it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  80% 271/337 [00:08<00:02, 27.32it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  81% 274/337 [00:08<00:02, 26.10it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  82% 277/337 [00:08<00:02, 26.94it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  83% 280/337 [00:08<00:02, 26.06it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  84% 283/337 [00:08<00:02, 26.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  85% 286/337 [00:08<00:02, 24.62it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  86% 289/337 [00:09<00:01, 25.23it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  87% 292/337 [00:09<00:01, 23.99it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  88% 295/337 [00:09<00:01, 24.05it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  88% 298/337 [00:09<00:01, 23.08it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  89% 301/337 [00:09<00:01, 23.22it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  90% 304/337 [00:09<00:01, 21.14it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  91% 307/337 [00:09<00:01, 21.67it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  92% 310/337 [00:10<00:01, 21.81it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  93% 313/337 [00:10<00:01, 21.83it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  94% 316/337 [00:10<00:00, 21.96it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  95% 319/337 [00:10<00:00, 20.03it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  96% 322/337 [00:10<00:00, 20.26it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  96% 325/337 [00:10<00:00, 19.75it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  97% 327/337 [00:10<00:00, 19.43it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  98% 329/337 [00:11<00:00, 18.80it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  98% 331/337 [00:11<00:00, 18.58it/s]\u001b[A\n","epoch 010 | valid on 'valid' subset:  99% 333/337 [00:11<00:00, 17.85it/s]\u001b[A\n","epoch 010: 100% 1562/1563 [02:50<00:00, 10.39it/s, loss=2.752, ppl=6.74, wps=27685, ups=10.1, wpb=2741.2, bsz=128, num_updates=15600, lr=0.0005, gnorm=1.372, loss_scale=16, train_wall=10, gb_free=12.5, wall=1746]\n","                                                                          \u001b[A2022-05-26 08:40:58 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 5.773 | ppl 54.7 | wps 85599.3 | wpb 2891.6 | bsz 127.6 | num_updates 15627 | best_loss 5.405\n","2022-05-26 08:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15627 updates\n","2022-05-26 08:40:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:41:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_de_en/checkpoint_last.pt\n","2022-05-26 08:41:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_de_en/checkpoint_last.pt (epoch 10 @ 15627 updates, score 5.773) (writing took 3.3022510549999424 seconds)\n","2022-05-26 08:41:01 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n","2022-05-26 08:41:01 | INFO | train | epoch 010 | loss 2.635 | ppl 6.21 | wps 26222.7 | ups 8.98 | wpb 2919.2 | bsz 128 | num_updates 15627 | lr 0.0005 | gnorm 1.269 | loss_scale 16 | train_wall 155 | gb_free 5.9 | wall 1764\n","2022-05-26 08:41:01 | INFO | fairseq_cli.train | done training in 1763.5 seconds\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz ▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free ▇▆▂▇▆█▇█▇▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm ▁▄▆▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss █▆▄▄▃▂▂▂▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss_scale ▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr ▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl █▃▂▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall ▁██▁██████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups ▃▁▂▂▁▂▇█▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall ▁▂▃▃▄▅▆▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb ▁█████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps ▂▁▁▁▁▂▇█▇▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz ███▁█████████████████████████████████▁██\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free ▆▇▄▆▅▅▄▅▄▄▆▇▅▄▁█▁▇▄▅▄▅▅▅█▅▆▅▆██▅▅▇▄▃▅▁▅▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm ▃▂▁▂▂▃▅▅▄▆▅▆▅▆▅▇▅▆▇▇▆▇▆▇▆▆▇▇▆▆▇▇▆▇▇▇▆▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss █▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/loss_scale ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall ▅▅▅▅▅▅▁▁▅▁▅▅▅▅▅▅▅▅▅▅▅▅▅█▅▅█▅▅▅▅▅▅▅▅▅▅▅▅▅\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups ▇█▇█▁▇██▁██▇▇█▇█▇▇▇█▇▇▇▇█▇▇█▇▇█▇██▇▇█▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb ▅▃▄▃▅▄▂▁▆▁▃▅▆▃▅▃▅▅▅▂▃▅▆▆▃▄█▃▅▄▁▆▂▃▆▄▄▇▄▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps █▇██▁█▇▇▁▇█▇█▇█▇███▇▇██▇▇▇█▇▇█▇█▇▇█████▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss █▄▂▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz ▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss █▄▂▁▁▁▁▂▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl █▃▂▁▁▁▁▁▂▂\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb ▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps ▁▆▄▅█▆▇▄▇▅\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz 128.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free 5.9\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm 1.269\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss 2.635\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss_scale 16.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl 6.21\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall 155.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups 8.98\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall 1764.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb 2919.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps 26222.7\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz 128.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free 12.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm 1.372\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss 2.752\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/loss_scale 16.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl 6.74\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall 10.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups 10.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall 1746.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb 2741.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps 27685.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss 5.405\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz 127.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss 5.773\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl 54.7\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb 2891.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps 85599.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoints_de_en\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation/runs/m7phxnxz\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220526_081138-m7phxnxz/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["# generate backtranslations of training model with de to en model\n","! fairseq-generate ./data/binarized/ua.tokenized.de-en \\\n","    --path checkpoints_de_en/checkpoint_best.pt \\\n","    --batch-size 256 \\\n","    --beam 5 \\\n","    --seed 1 \\\n","    --wandb-project \"en-de-backtranslation\" \\\n","    --results-path saveddeen \\\n","    --gen-subset train  \\"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWrrzRPMZ05i","executionInfo":{"status":"ok","timestamp":1653556590534,"user_tz":-120,"elapsed":2119740,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"baeaedfe-56a6-4b7f-cc69-230b07933c4e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 08:41:14 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 08:41:15 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'en-de-backtranslation', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints_de_en/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'saveddeen'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'train', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': './data/binarized/ua.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2022-05-26 08:41:16 | INFO | fairseq.tasks.translation | [de] dictionary: 102504 types\n","2022-05-26 08:41:16 | INFO | fairseq.tasks.translation | [en] dictionary: 73392 types\n","2022-05-26 08:41:16 | INFO | fairseq_cli.generate | loading model(s) from checkpoints_de_en/checkpoint_best.pt\n","2022-05-26 08:41:17 | INFO | fairseq.data.data_utils | loaded 200,000 examples from: ./data/binarized/ua.tokenized.de-en/train.de-en.de\n","2022-05-26 08:41:17 | INFO | fairseq.data.data_utils | loaded 200,000 examples from: ./data/binarized/ua.tokenized.de-en/train.de-en.en\n","2022-05-26 08:41:17 | INFO | fairseq.tasks.translation | ./data/binarized/ua.tokenized.de-en train de-en 200000 examples\n","2022-05-26 09:16:29 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n","2022-05-26 09:16:29 | INFO | fairseq_cli.generate | Translated 200,000 sentences (4,292,457 tokens) in 1242.7s (160.94 sentences/s, 3454.12 tokens/s)\n"]}]},{"cell_type":"code","source":["# generate dataset for training baseline model \n","!fairseq-preprocess --source-lang en --target-lang de \\\n","  --trainpref ./en-de-backtranslation/data/datasets/unaugmented/original.train \\\n","  --validpref ./en-de-backtranslation/data/datasets/unaugmented/original.val  \\\n","  --testpref  ./en-de-backtranslation/data/datasets/unaugmented/original.test  \\\n","  --destdir ./data/binarized/ua.tokenized.en-de \\\n","  --thresholdsrc 2 \\\n","  --thresholdtgt 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P1g-OQkvh7Vk","executionInfo":{"status":"ok","timestamp":1653556692385,"user_tz":-120,"elapsed":101126,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"36a541fe-0872-41b0-b77d-0e8c694fc9e7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 09:16:32 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 09:16:33 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='./data/binarized/ua.tokenized.en-de', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='./en-de-backtranslation/data/datasets/unaugmented/original.test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer=None, tpu=False, trainpref='./en-de-backtranslation/data/datasets/unaugmented/original.train', use_plasma_view=False, user_dir=None, validpref='./en-de-backtranslation/data/datasets/unaugmented/original.val', wandb_project=None, workers=1)\n","2022-05-26 09:16:56 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 09:17:21 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.train.en: 200000 sents, 4562780 tokens, 1.62% replaced (by <unk>)\n","2022-05-26 09:17:21 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 09:17:27 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.val.en: 42989 sents, 974477 tokens, 2.91% replaced (by <unk>)\n","2022-05-26 09:17:27 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 09:17:32 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.test.en: 43291 sents, 982864 tokens, 2.96% replaced (by <unk>)\n","2022-05-26 09:17:32 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 09:17:59 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.train.de: 200000 sents, 4673897 tokens, 2.92% replaced (by <unk>)\n","2022-05-26 09:17:59 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 09:18:05 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.val.de: 42989 sents, 999226 tokens, 4.67% replaced (by <unk>)\n","2022-05-26 09:18:05 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 09:18:11 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.test.de: 43291 sents, 1006306 tokens, 4.87% replaced (by <unk>)\n","2022-05-26 09:18:11 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ./data/binarized/ua.tokenized.en-de\n"]}]},{"cell_type":"code","source":["# train en-de baseline model\n","!CHECKPOINT_DIR=checkpoints_en_de && fairseq-train ./data/binarized/ua.tokenized.en-de --fp16\\\n","  --arch transformer \\\n","  --source-lang en --target-lang de \\\n","  --dropout 0.1 \\\n","  --attention-dropout 0.1 \\\n","  --activation-dropout 0.1 \\\n","  --encoder-embed-dim 256 \\\n","  --encoder-ffn-embed-dim 512 \\\n","  --encoder-layers 3 \\\n","  --encoder-attention-heads 8 \\\n","  --encoder-learned-pos \\\n","  --decoder-embed-dim 256 \\\n","  --decoder-ffn-embed-dim 512 \\\n","  --decoder-layers 3 \\\n","  --decoder-attention-heads 8 \\\n","  --no-epoch-checkpoints \\\n","  --decoder-learned-pos \\\n","  --max-epoch 15\\\n","  --optimizer adam \\\n","  --lr 5e-4 \\\n","  --batch-size 128 \\\n","  --seed 1 \\\n","  --wandb-project \"en-de-backtranslation\" \\\n","  --save-dir $CHECKPOINT_DIR \\\n","  --finetune-from-model ./checkpoints_de_en/checkpoint_last.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WV1MpJrdiJnM","executionInfo":{"status":"ok","timestamp":1653563932742,"user_tz":-120,"elapsed":650154,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"25180541-d521-433c-abf9-f9e0b0ff6fd7"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 11:08:04 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 11:08:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'en-de-backtranslation', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 128, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 128, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 15, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints_en_de', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': './checkpoints_de_en/checkpoint_last.pt', 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.1, activation_fn='relu', adam_betas=(0.9, 0.999), adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.1, azureml_logging=False, batch_size=128, batch_size_valid=128, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='cross_entropy', cross_self_attention=False, curriculum=0, data='./data/binarized/ua.tokenized.en-de', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=256, decoder_embed_path=None, decoder_ffn_embed_dim=512, decoder_input_dim=256, decoder_layerdrop=0, decoder_layers=3, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=256, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=256, encoder_embed_path=None, encoder_ffn_embed_dim=512, encoder_layerdrop=0, encoder_layers=3, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model='./checkpoints_de_en/checkpoint_last.pt', fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=15, max_tokens=None, max_tokens_valid=None, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints_en_de', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='en', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='en-de-backtranslation', warmup_updates=0, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': './data/binarized/ua.tokenized.en-de', 'source_lang': 'en', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': [0.9, 0.999], 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2022-05-26 11:08:06 | INFO | fairseq.tasks.translation | [en] dictionary: 73392 types\n","2022-05-26 11:08:06 | INFO | fairseq.tasks.translation | [de] dictionary: 102504 types\n","2022-05-26 11:08:07 | INFO | fairseq_cli.train | TransformerModel(\n","  (encoder): TransformerEncoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(73392, 256, padding_idx=1)\n","    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n","    (layers): ModuleList(\n","      (0): TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerEncoderLayerBase(\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (dropout_module): FairseqDropout()\n","        (activation_dropout_module): FairseqDropout()\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","  )\n","  (decoder): TransformerDecoderBase(\n","    (dropout_module): FairseqDropout()\n","    (embed_tokens): Embedding(102504, 256, padding_idx=1)\n","    (embed_positions): LearnedPositionalEmbedding(1026, 256, padding_idx=1)\n","    (layers): ModuleList(\n","      (0): TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (1): TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","      (2): TransformerDecoderLayerBase(\n","        (dropout_module): FairseqDropout()\n","        (self_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (activation_dropout_module): FairseqDropout()\n","        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (encoder_attn): MultiheadAttention(\n","          (dropout_module): FairseqDropout()\n","          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","        )\n","        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (fc1): Linear(in_features=256, out_features=512, bias=True)\n","        (fc2): Linear(in_features=512, out_features=256, bias=True)\n","        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (output_projection): Linear(in_features=256, out_features=102504, bias=False)\n","  )\n",")\n","2022-05-26 11:08:07 | INFO | fairseq_cli.train | task: TranslationTask\n","2022-05-26 11:08:07 | INFO | fairseq_cli.train | model: TransformerModel\n","2022-05-26 11:08:07 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n","2022-05-26 11:08:07 | INFO | fairseq_cli.train | num. shared model params: 75,749,376 (num. trained: 75,749,376)\n","2022-05-26 11:08:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n","2022-05-26 11:08:07 | INFO | fairseq.data.data_utils | loaded 42,989 examples from: ./data/binarized/ua.tokenized.en-de/valid.en-de.en\n","2022-05-26 11:08:07 | INFO | fairseq.data.data_utils | loaded 42,989 examples from: ./data/binarized/ua.tokenized.en-de/valid.en-de.de\n","2022-05-26 11:08:07 | INFO | fairseq.tasks.translation | ./data/binarized/ua.tokenized.en-de valid en-de 42989 examples\n","2022-05-26 11:08:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2022-05-26 11:08:11 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n","2022-05-26 11:08:11 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n","2022-05-26 11:08:11 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n","2022-05-26 11:08:11 | INFO | fairseq_cli.train | max tokens per device = None and max sentences per device = 128\n","2022-05-26 11:08:11 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints_en_de/checkpoint_last.pt\n","2022-05-26 11:08:12 | INFO | fairseq.trainer | Loaded checkpoint checkpoints_en_de/checkpoint_last.pt (epoch 13 @ 18551 updates)\n","2022-05-26 11:08:12 | INFO | fairseq.trainer | loading train data for epoch 13\n","2022-05-26 11:08:12 | INFO | fairseq.data.data_utils | loaded 200,000 examples from: ./data/binarized/ua.tokenized.en-de/train.en-de.en\n","2022-05-26 11:08:12 | INFO | fairseq.data.data_utils | loaded 200,000 examples from: ./data/binarized/ua.tokenized.en-de/train.en-de.de\n","2022-05-26 11:08:12 | INFO | fairseq.tasks.translation | ./data/binarized/ua.tokenized.en-de train en-de 200000 examples\n","2022-05-26 11:08:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 013:   0% 0/1563 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mszumi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.17 is available!  To upgrade, please run:\n","\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.16\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220526_110812-nu2qq83u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcheckpoints_en_de\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation/runs/nu2qq83u\u001b[0m\n","2022-05-26 11:08:18 | INFO | fairseq.trainer | begin training epoch 13\n","2022-05-26 11:08:18 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 013:   0% 3/1563 [00:05<35:22,  1.36s/it]  2022-05-26 11:08:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.86 GiB already allocated; 4.37 GiB free; 9.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:08:19 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9069 MB |   12417 MB |   25666 MB |   16596 MB |\n","|       from large pool |    9059 MB |   12406 MB |   25507 MB |   16447 MB |\n","|       from small pool |      10 MB |      17 MB |     158 MB |     148 MB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9069 MB |   12417 MB |   25666 MB |   16596 MB |\n","|       from large pool |    9059 MB |   12406 MB |   25507 MB |   16447 MB |\n","|       from small pool |      10 MB |      17 MB |     158 MB |     148 MB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |    9368 MB |   12698 MB |   15974 MB |    6606 MB |\n","|       from large pool |    9356 MB |   12686 MB |   15942 MB |    6586 MB |\n","|       from small pool |      12 MB |      32 MB |      32 MB |      20 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |  305195 KB |    1199 MB |   11605 MB |   11307 MB |\n","|       from large pool |  303740 KB |    1185 MB |   11411 MB |   11115 MB |\n","|       from small pool |    1455 KB |      13 MB |     194 MB |     192 MB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    2790    |    2466    |\n","|       from large pool |     142    |     147    |    1501    |    1359    |\n","|       from small pool |     182    |     266    |    1289    |    1107    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    2790    |    2466    |\n","|       from large pool |     142    |     147    |    1501    |    1359    |\n","|       from small pool |     182    |     266    |    1289    |    1107    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      35    |      36    |      14    |\n","|       from large pool |      16    |      19    |      20    |       4    |\n","|       from small pool |       6    |      16    |      16    |      10    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      41    |    1322    |    1295    |\n","|       from large pool |      18    |      19    |     935    |     917    |\n","|       from small pool |       9    |      33    |     387    |     378    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:08:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:   7% 117/1563 [00:20<03:32,  6.79it/s, loss=2.133, ppl=4.39, wps=22920.6, ups=7.46, wpb=3052.3, bsz=128, num_updates=18600, lr=0.0005, gnorm=1.222, loss_scale=32, train_wall=6, gb_free=9, wall=13]2022-05-26 11:08:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.06 GiB (GPU 0; 14.76 GiB total capacity; 6.03 GiB already allocated; 1.68 GiB free; 11.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:08:33 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6178 MB |    8267 MB |  634521 MB |  628342 MB |\n","|       from large pool |    6168 MB |    8257 MB |  623972 MB |  617803 MB |\n","|       from small pool |       9 MB |      17 MB |   10549 MB |   10539 MB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6178 MB |    8267 MB |  634521 MB |  628342 MB |\n","|       from large pool |    6168 MB |    8257 MB |  623972 MB |  617803 MB |\n","|       from small pool |       9 MB |      17 MB |   10549 MB |   10539 MB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12120 MB |   12190 MB |   22646 MB |   10526 MB |\n","|       from large pool |   12108 MB |   12108 MB |   22450 MB |   10342 MB |\n","|       from small pool |      12 MB |      82 MB |     196 MB |     184 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5941 MB |    5941 MB |     903 GB |     897 GB |\n","|       from large pool |    5939 MB |    5939 MB |     891 GB |     885 GB |\n","|       from small pool |       2 MB |      17 MB |      12 GB |      12 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     323    |     326    |   93347    |   93024    |\n","|       from large pool |     142    |     147    |   42760    |   42618    |\n","|       from small pool |     181    |     266    |   50587    |   50406    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     323    |     326    |   93347    |   93024    |\n","|       from large pool |     142    |     147    |   42760    |   42618    |\n","|       from small pool |     181    |     266    |   50587    |   50406    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      57    |     120    |      98    |\n","|       from large pool |      16    |      16    |      22    |       6    |\n","|       from small pool |       6    |      41    |      98    |      92    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      41    |   48614    |   48588    |\n","|       from large pool |      17    |      18    |   26734    |   26717    |\n","|       from small pool |       9    |      33    |   21880    |   21871    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:08:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:   9% 135/1563 [00:22<03:00,  7.93it/s, loss=2.133, ppl=4.39, wps=22920.6, ups=7.46, wpb=3052.3, bsz=128, num_updates=18600, lr=0.0005, gnorm=1.222, loss_scale=32, train_wall=6, gb_free=9, wall=13]2022-05-26 11:08:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 1.68 GiB free; 11.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:08:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6372 MB |    8537 MB |  741162 MB |  734789 MB |\n","|       from large pool |    6362 MB |    8527 MB |  728766 MB |  722403 MB |\n","|       from small pool |      10 MB |      17 MB |   12395 MB |   12385 MB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6372 MB |    8537 MB |  741162 MB |  734789 MB |\n","|       from large pool |    6362 MB |    8527 MB |  728766 MB |  722403 MB |\n","|       from small pool |      10 MB |      17 MB |   12395 MB |   12385 MB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12120 MB |   12220 MB |   22746 MB |   10626 MB |\n","|       from large pool |   12108 MB |   12108 MB |   22450 MB |   10342 MB |\n","|       from small pool |      12 MB |     112 MB |     296 MB |     284 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5747 MB |    5747 MB |    1110 GB |    1105 GB |\n","|       from large pool |    5745 MB |    5745 MB |    1096 GB |    1091 GB |\n","|       from small pool |       1 MB |      15 MB |      14 GB |      14 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  107189    |  106865    |\n","|       from large pool |     142    |     147    |   48784    |   48642    |\n","|       from small pool |     182    |     266    |   58405    |   58223    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  107189    |  106865    |\n","|       from large pool |     142    |     147    |   48784    |   48642    |\n","|       from small pool |     182    |     266    |   58405    |   58223    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |     170    |     148    |\n","|       from large pool |      16    |      16    |      22    |       6    |\n","|       from small pool |       6    |      56    |     148    |     142    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      39    |   55692    |   55667    |\n","|       from large pool |      17    |      18    |   30370    |   30353    |\n","|       from small pool |       8    |      31    |   25322    |   25314    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:08:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  10% 162/1563 [00:26<03:21,  6.95it/s, loss=2.271, ppl=4.83, wps=24338.2, ups=7.75, wpb=3139.1, bsz=128, num_updates=18700, lr=0.0005, gnorm=1.226, loss_scale=32, train_wall=12, gb_free=9.1, wall=26]2022-05-26 11:08:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.50 GiB (GPU 0; 14.76 GiB total capacity; 6.56 GiB already allocated; 1.54 GiB free; 11.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:08:39 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 4            |        cudaMalloc retries: 7         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6713 MB |    9029 MB |     883 GB |     876 GB |\n","|       from large pool |    6703 MB |    9019 MB |     869 GB |     862 GB |\n","|       from small pool |      10 MB |      17 MB |      14 GB |      14 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6713 MB |    9029 MB |     883 GB |     876 GB |\n","|       from large pool |    6703 MB |    9019 MB |     869 GB |     862 GB |\n","|       from small pool |      10 MB |      17 MB |      14 GB |      14 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12270 MB |   12300 MB |   26624 MB |   14354 MB |\n","|       from large pool |   12258 MB |   12258 MB |   26204 MB |   13946 MB |\n","|       from small pool |      12 MB |      42 MB |     420 MB |     408 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5556 MB |    5556 MB |    1434 GB |    1429 GB |\n","|       from large pool |    5554 MB |    5554 MB |    1418 GB |    1412 GB |\n","|       from small pool |       1 MB |       2 MB |      16 GB |      16 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  128292    |  127968    |\n","|       from large pool |     142    |     147    |   59359    |   59217    |\n","|       from small pool |     182    |     266    |   68933    |   68751    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  128292    |  127968    |\n","|       from large pool |     142    |     147    |   59359    |   59217    |\n","|       from small pool |     182    |     266    |   68933    |   68751    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      37    |     233    |     211    |\n","|       from large pool |      16    |      16    |      23    |       7    |\n","|       from small pool |       6    |      21    |     210    |     204    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      27    |   66761    |   66736    |\n","|       from large pool |      19    |      20    |   36994    |   36975    |\n","|       from small pool |       6    |      17    |   29767    |   29761    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:08:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  11% 165/1563 [00:26<03:04,  7.57it/s, loss=2.271, ppl=4.83, wps=24338.2, ups=7.75, wpb=3139.1, bsz=128, num_updates=18700, lr=0.0005, gnorm=1.226, loss_scale=32, train_wall=12, gb_free=9.1, wall=26]2022-05-26 11:08:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.76 GiB total capacity; 5.91 GiB already allocated; 1.54 GiB free; 11.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:08:39 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 5            |        cudaMalloc retries: 8         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6047 MB |    8085 MB |     901 GB |     895 GB |\n","|       from large pool |    6037 MB |    8075 MB |     887 GB |     881 GB |\n","|       from small pool |       9 MB |      17 MB |      14 GB |      14 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6047 MB |    8085 MB |     901 GB |     895 GB |\n","|       from large pool |    6037 MB |    8075 MB |     887 GB |     881 GB |\n","|       from small pool |       9 MB |      17 MB |      14 GB |      14 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12270 MB |   12296 MB |   26650 MB |   14380 MB |\n","|       from large pool |   12258 MB |   12258 MB |   26204 MB |   13946 MB |\n","|       from small pool |      12 MB |      38 MB |     446 MB |     434 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6222 MB |    6222 MB |    1470 GB |    1464 GB |\n","|       from large pool |    6220 MB |    6220 MB |    1453 GB |    1447 GB |\n","|       from small pool |       2 MB |      17 MB |      16 GB |      16 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  130198    |  129874    |\n","|       from large pool |     142    |     147    |   60481    |   60339    |\n","|       from small pool |     182    |     266    |   69717    |   69535    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  130198    |  129874    |\n","|       from large pool |     142    |     147    |   60481    |   60339    |\n","|       from small pool |     182    |     266    |   69717    |   69535    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      35    |     246    |     224    |\n","|       from large pool |      16    |      16    |      23    |       7    |\n","|       from small pool |       6    |      19    |     223    |     217    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      48    |   67722    |   67701    |\n","|       from large pool |      13    |      14    |   37663    |   37650    |\n","|       from small pool |       8    |      40    |   30059    |   30051    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:08:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  21% 330/1563 [00:47<02:20,  8.77it/s, loss=2.088, ppl=4.25, wps=24013.5, ups=8.01, wpb=2998.9, bsz=128, num_updates=18800, lr=0.0005, gnorm=1.2, loss_scale=32, train_wall=12, gb_free=10.9, wall=39]2022-05-26 11:09:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 0; 14.76 GiB total capacity; 6.01 GiB already allocated; 1.48 GiB free; 12.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:00 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 6            |        cudaMalloc retries: 10        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6152 MB |    8291 MB |    1752 GB |    1746 GB |\n","|       from large pool |    6133 MB |    8271 MB |    1722 GB |    1716 GB |\n","|       from small pool |      19 MB |      20 MB |      30 GB |      30 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6152 MB |    8291 MB |    1752 GB |    1746 GB |\n","|       from large pool |    6133 MB |    8271 MB |    1722 GB |    1716 GB |\n","|       from small pool |      19 MB |      20 MB |      30 GB |      30 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12328 MB |   12422 MB |   30652 MB |   18324 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      20 MB |     114 MB |     644 MB |     624 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6175 MB |    6175 MB |    3281 GB |    3275 GB |\n","|       from large pool |    6174 MB |    6174 MB |    3246 GB |    3240 GB |\n","|       from small pool |       0 MB |      17 MB |      34 GB |      34 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  261238    |  260914    |\n","|       from large pool |     132    |     136    |  120742    |  120610    |\n","|       from small pool |     192    |     266    |  140496    |  140304    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  261238    |  260914    |\n","|       from large pool |     132    |     136    |  120742    |  120610    |\n","|       from small pool |     192    |     266    |  140496    |  140304    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      73    |     346    |     320    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |      10    |      57    |     322    |     312    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      41    |  136206    |  136180    |\n","|       from large pool |      14    |      15    |   75544    |   75530    |\n","|       from small pool |      12    |      33    |   60662    |   60650    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  22% 346/1563 [00:49<02:31,  8.02it/s, loss=2.088, ppl=4.25, wps=24013.5, ups=8.01, wpb=2998.9, bsz=128, num_updates=18800, lr=0.0005, gnorm=1.2, loss_scale=32, train_wall=12, gb_free=10.9, wall=39]2022-05-26 11:09:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 7.03 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:02 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 7            |        cudaMalloc retries: 11        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7194 MB |    9710 MB |    1838 GB |    1831 GB |\n","|       from large pool |    7184 MB |    9700 MB |    1806 GB |    1799 GB |\n","|       from small pool |      10 MB |      17 MB |      31 GB |      31 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7194 MB |    9710 MB |    1838 GB |    1831 GB |\n","|       from large pool |    7184 MB |    9700 MB |    1806 GB |    1799 GB |\n","|       from small pool |      10 MB |      17 MB |      31 GB |      31 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12420 MB |   30744 MB |   18424 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     112 MB |     736 MB |     724 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5125 MB |    5125 MB |    3468 GB |    3463 GB |\n","|       from large pool |    5123 MB |    5123 MB |    3431 GB |    3426 GB |\n","|       from small pool |       1 MB |       2 MB |      36 GB |      36 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  273481    |  273157    |\n","|       from large pool |     142    |     147    |  126353    |  126211    |\n","|       from small pool |     182    |     266    |  147128    |  146946    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  273481    |  273157    |\n","|       from large pool |     142    |     147    |  126353    |  126211    |\n","|       from small pool |     182    |     266    |  147128    |  146946    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |     392    |     370    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      56    |     368    |     362    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      29    |  142478    |  142451    |\n","|       from large pool |      21    |      22    |   79011    |   78990    |\n","|       from small pool |       6    |      14    |   63467    |   63461    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  24% 372/1563 [00:52<03:04,  6.46it/s, loss=2.295, ppl=4.91, wps=23975.3, ups=7.51, wpb=3190.4, bsz=128, num_updates=18900, lr=0.0005, gnorm=1.222, loss_scale=32, train_wall=13, gb_free=10.6, wall=52]2022-05-26 11:09:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.35 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:05 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 8            |        cudaMalloc retries: 12        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7523 MB |   10215 MB |    1979 GB |    1972 GB |\n","|       from large pool |    7512 MB |   10205 MB |    1945 GB |    1938 GB |\n","|       from small pool |      10 MB |      17 MB |      33 GB |      33 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7523 MB |   10215 MB |    1979 GB |    1972 GB |\n","|       from large pool |    7512 MB |   10205 MB |    1945 GB |    1938 GB |\n","|       from small pool |      10 MB |      17 MB |      33 GB |      33 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12420 MB |   30844 MB |   18524 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     112 MB |     836 MB |     824 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4796 MB |    4796 MB |    3773 GB |    3769 GB |\n","|       from large pool |    4795 MB |    4795 MB |    3734 GB |    3730 GB |\n","|       from small pool |       1 MB |       2 MB |      38 GB |      38 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  293768    |  293444    |\n","|       from large pool |     142    |     147    |  136126    |  135984    |\n","|       from small pool |     182    |     266    |  157642    |  157460    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  293768    |  293444    |\n","|       from large pool |     142    |     147    |  136126    |  135984    |\n","|       from small pool |     182    |     266    |  157642    |  157460    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |     442    |     420    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      56    |     418    |     412    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      27    |  152962    |  152938    |\n","|       from large pool |      16    |      17    |   85035    |   85019    |\n","|       from small pool |       8    |      18    |   67927    |   67919    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  26% 404/1563 [00:56<02:24,  8.04it/s, loss=2.295, ppl=4.91, wps=23975.3, ups=7.51, wpb=3190.4, bsz=128, num_updates=18900, lr=0.0005, gnorm=1.222, loss_scale=32, train_wall=13, gb_free=10.6, wall=52]2022-05-26 11:09:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.76 GiB (GPU 0; 14.76 GiB total capacity; 5.74 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:09 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 9            |        cudaMalloc retries: 13        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5880 MB |    7818 MB |    2126 GB |    2120 GB |\n","|       from large pool |    5870 MB |    7808 MB |    2089 GB |    2083 GB |\n","|       from small pool |       9 MB |      17 MB |      36 GB |      36 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5880 MB |    7818 MB |    2126 GB |    2120 GB |\n","|       from large pool |    5870 MB |    7808 MB |    2089 GB |    2083 GB |\n","|       from small pool |       9 MB |      17 MB |      36 GB |      36 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12422 MB |   30946 MB |   18626 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     114 MB |     938 MB |     926 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6439 MB |    6439 MB |    4107 GB |    4101 GB |\n","|       from large pool |    6437 MB |    6437 MB |    4065 GB |    4058 GB |\n","|       from small pool |       2 MB |      10 MB |      42 GB |      42 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  318597    |  318273    |\n","|       from large pool |     142    |     147    |  146871    |  146729    |\n","|       from small pool |     182    |     266    |  171726    |  171544    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  318597    |  318273    |\n","|       from large pool |     142    |     147    |  146871    |  146729    |\n","|       from small pool |     182    |     266    |  171726    |  171544    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |     493    |     471    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      57    |     469    |     463    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      36    |  166060    |  166030    |\n","|       from large pool |      20    |      21    |   91927    |   91907    |\n","|       from small pool |      10    |      28    |   74133    |   74123    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 013:  26% 405/1563 [00:56<02:22,  8.12it/s, loss=2.295, ppl=4.91, wps=23975.3, ups=7.51, wpb=3190.4, bsz=128, num_updates=18900, lr=0.0005, gnorm=1.222, loss_scale=32, train_wall=13, gb_free=10.6, wall=52]2022-05-26 11:09:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  32% 497/1563 [01:07<01:58,  9.03it/s, loss=2.205, ppl=4.61, wps=24005.5, ups=8.21, wpb=2922.5, bsz=128, num_updates=19000, lr=0.0005, gnorm=1.25, loss_scale=32, train_wall=12, gb_free=6.1, wall=64]2022-05-26 11:09:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.76 GiB total capacity; 5.93 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 10           |        cudaMalloc retries: 14        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6076 MB |    8114 MB |    2568 GB |    2562 GB |\n","|       from large pool |    6066 MB |    8104 MB |    2522 GB |    2516 GB |\n","|       from small pool |       9 MB |      17 MB |      46 GB |      46 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6076 MB |    8114 MB |    2568 GB |    2562 GB |\n","|       from large pool |    6066 MB |    8104 MB |    2522 GB |    2516 GB |\n","|       from small pool |       9 MB |      17 MB |      46 GB |      46 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12420 MB |   31046 MB |   18726 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     112 MB |    1038 MB |    1026 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6243 MB |    6243 MB |    5118 GB |    5112 GB |\n","|       from large pool |    6241 MB |    6241 MB |    5064 GB |    5058 GB |\n","|       from small pool |       2 MB |      19 MB |      53 GB |      53 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  391624    |  391300    |\n","|       from large pool |     142    |     147    |  178616    |  178474    |\n","|       from small pool |     182    |     266    |  213008    |  212826    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  391624    |  391300    |\n","|       from large pool |     142    |     147    |  178616    |  178474    |\n","|       from small pool |     182    |     266    |  213008    |  212826    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |     543    |     521    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      56    |     519    |     513    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      43    |  204375    |  204347    |\n","|       from large pool |      21    |      22    |  111922    |  111901    |\n","|       from small pool |       7    |      35    |   92453    |   92446    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  36% 564/1563 [01:15<02:15,  7.35it/s, loss=2.064, ppl=4.18, wps=23860.8, ups=8.5, wpb=2806.5, bsz=128, num_updates=19100, lr=0.0005, gnorm=1.232, loss_scale=32, train_wall=11, gb_free=9.4, wall=76]2022-05-26 11:09:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.81 GiB (GPU 0; 14.76 GiB total capacity; 5.68 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:28 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 11           |        cudaMalloc retries: 15        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5812 MB |    7774 MB |    2880 GB |    2874 GB |\n","|       from large pool |    5802 MB |    7764 MB |    2825 GB |    2819 GB |\n","|       from small pool |       9 MB |      17 MB |      54 GB |      54 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5812 MB |    7774 MB |    2880 GB |    2874 GB |\n","|       from large pool |    5802 MB |    7764 MB |    2825 GB |    2819 GB |\n","|       from small pool |       9 MB |      17 MB |      54 GB |      54 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12318 MB |   12422 MB |   31148 MB |   18830 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      10 MB |     114 MB |    1140 MB |    1130 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6505 MB |    6505 MB |    5833 GB |    5826 GB |\n","|       from large pool |    6505 MB |    6505 MB |    5769 GB |    5763 GB |\n","|       from small pool |       0 MB |       2 MB |      63 GB |      63 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  444095    |  443771    |\n","|       from large pool |     142    |     146    |  199267    |  199125    |\n","|       from small pool |     182    |     266    |  244828    |  244646    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  444095    |  443771    |\n","|       from large pool |     142    |     146    |  199267    |  199125    |\n","|       from small pool |     182    |     266    |  244828    |  244646    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      73    |     594    |     573    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       5    |      57    |     570    |     565    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      25    |  231920    |  231897    |\n","|       from large pool |      17    |      18    |  124748    |  124731    |\n","|       from small pool |       6    |      17    |  107172    |  107166    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  39% 608/1563 [01:20<01:52,  8.50it/s, loss=2.064, ppl=4.18, wps=23860.8, ups=8.5, wpb=2806.5, bsz=128, num_updates=19100, lr=0.0005, gnorm=1.232, loss_scale=32, train_wall=11, gb_free=9.4, wall=76]2022-05-26 11:09:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.98 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:33 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 12           |        cudaMalloc retries: 16        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7147 MB |    9713 MB |    3081 GB |    3074 GB |\n","|       from large pool |    7137 MB |    9702 MB |    3021 GB |    3014 GB |\n","|       from small pool |      10 MB |      17 MB |      59 GB |      59 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7147 MB |    9713 MB |    3081 GB |    3074 GB |\n","|       from large pool |    7137 MB |    9702 MB |    3021 GB |    3014 GB |\n","|       from small pool |      10 MB |      17 MB |      59 GB |      59 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12412 MB |   31242 MB |   18922 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     104 MB |    1234 MB |    1222 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5172 MB |    5172 MB |    6295 GB |    6290 GB |\n","|       from large pool |    5170 MB |    5170 MB |    6226 GB |    6221 GB |\n","|       from small pool |       1 MB |      17 MB |      68 GB |      68 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  478217    |  477893    |\n","|       from large pool |     142    |     146    |  213591    |  213449    |\n","|       from small pool |     182    |     266    |  264626    |  264444    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  478217    |  477893    |\n","|       from large pool |     142    |     146    |  213591    |  213449    |\n","|       from small pool |     182    |     266    |  264626    |  264444    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      68    |     641    |     619    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      52    |     617    |     611    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      18    |      42    |  249532    |  249514    |\n","|       from large pool |      10    |      11    |  133691    |  133681    |\n","|       from small pool |       8    |      34    |  115841    |  115833    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  40% 628/1563 [01:22<01:54,  8.16it/s, loss=2.064, ppl=4.18, wps=23860.8, ups=8.5, wpb=2806.5, bsz=128, num_updates=19100, lr=0.0005, gnorm=1.232, loss_scale=32, train_wall=11, gb_free=9.4, wall=76]2022-05-26 11:09:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.76 GiB (GPU 0; 14.76 GiB total capacity; 5.70 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 13           |        cudaMalloc retries: 17        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5837 MB |    7775 MB |    3182 GB |    3177 GB |\n","|       from large pool |    5827 MB |    7765 MB |    3121 GB |    3116 GB |\n","|       from small pool |       9 MB |      17 MB |      61 GB |      61 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5837 MB |    7775 MB |    3182 GB |    3177 GB |\n","|       from large pool |    5827 MB |    7765 MB |    3121 GB |    3116 GB |\n","|       from small pool |       9 MB |      17 MB |      61 GB |      61 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12318 MB |   12402 MB |   31324 MB |   19006 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      10 MB |      94 MB |    1316 MB |    1306 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6480 MB |    6480 MB |    6507 GB |    6501 GB |\n","|       from large pool |    6480 MB |    6480 MB |    6436 GB |    6430 GB |\n","|       from small pool |       0 MB |       2 MB |      71 GB |      71 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  493479    |  493155    |\n","|       from large pool |     142    |     147    |  219795    |  219653    |\n","|       from small pool |     182    |     266    |  273684    |  273502    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  493479    |  493155    |\n","|       from large pool |     142    |     147    |  219795    |  219653    |\n","|       from small pool |     182    |     266    |  273684    |  273502    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      63    |     682    |     661    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       5    |      47    |     658    |     653    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      23    |  257217    |  257197    |\n","|       from large pool |      15    |      16    |  137443    |  137428    |\n","|       from small pool |       5    |      14    |  119774    |  119769    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  43% 671/1563 [01:28<01:54,  7.80it/s, loss=2.168, ppl=4.49, wps=23122.8, ups=8.12, wpb=2848, bsz=128, num_updates=19200, lr=0.0005, gnorm=1.312, loss_scale=32, train_wall=12, gb_free=11.8, wall=88]2022-05-26 11:09:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 0; 14.76 GiB total capacity; 6.05 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:41 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 14           |        cudaMalloc retries: 18        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6195 MB |    8334 MB |    3407 GB |    3401 GB |\n","|       from large pool |    6185 MB |    8323 MB |    3342 GB |    3336 GB |\n","|       from small pool |       9 MB |      17 MB |      65 GB |      65 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6195 MB |    8334 MB |    3407 GB |    3401 GB |\n","|       from large pool |    6185 MB |    8323 MB |    3342 GB |    3336 GB |\n","|       from small pool |       9 MB |      17 MB |      65 GB |      65 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12318 MB |   12420 MB |   31426 MB |   19108 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      10 MB |     112 MB |    1418 MB |    1408 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6122 MB |    6122 MB |    6993 GB |    6987 GB |\n","|       from large pool |    6122 MB |    6122 MB |    6917 GB |    6911 GB |\n","|       from small pool |       0 MB |       5 MB |      75 GB |      75 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  527187    |  526863    |\n","|       from large pool |     142    |     146    |  235850    |  235708    |\n","|       from small pool |     182    |     266    |  291337    |  291155    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  527187    |  526863    |\n","|       from large pool |     142    |     146    |  235850    |  235708    |\n","|       from small pool |     182    |     266    |  291337    |  291155    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      72    |     733    |     712    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       5    |      56    |     709    |     704    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      19    |      24    |  274611    |  274592    |\n","|       from large pool |      13    |      14    |  147450    |  147437    |\n","|       from small pool |       6    |      16    |  127161    |  127155    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  43% 675/1563 [01:28<02:07,  6.98it/s, loss=2.168, ppl=4.49, wps=23122.8, ups=8.12, wpb=2848, bsz=128, num_updates=19200, lr=0.0005, gnorm=1.312, loss_scale=32, train_wall=12, gb_free=11.8, wall=88]2022-05-26 11:09:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.19 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:41 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 15           |        cudaMalloc retries: 19        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6337 MB |    8501 MB |    3435 GB |    3428 GB |\n","|       from large pool |    6327 MB |    8491 MB |    3369 GB |    3363 GB |\n","|       from small pool |      10 MB |      17 MB |      65 GB |      65 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6337 MB |    8501 MB |    3435 GB |    3428 GB |\n","|       from large pool |    6327 MB |    8491 MB |    3369 GB |    3363 GB |\n","|       from small pool |      10 MB |      17 MB |      65 GB |      65 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12344 MB |   31452 MB |   19132 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |      36 MB |    1444 MB |    1432 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5982 MB |    5982 MB |    7042 GB |    7036 GB |\n","|       from large pool |    5980 MB |    5980 MB |    6967 GB |    6961 GB |\n","|       from small pool |       1 MB |       2 MB |      75 GB |      75 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  529947    |  529623    |\n","|       from large pool |     142    |     147    |  237514    |  237372    |\n","|       from small pool |     182    |     266    |  292433    |  292251    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  529947    |  529623    |\n","|       from large pool |     142    |     147    |  237514    |  237372    |\n","|       from small pool |     182    |     266    |  292433    |  292251    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      34    |     746    |     724    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      18    |     722    |     716    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      26    |  276038    |  276013    |\n","|       from large pool |      17    |      18    |  148479    |  148462    |\n","|       from small pool |       8    |      17    |  127559    |  127551    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  44% 690/1563 [01:30<01:34,  9.22it/s, loss=2.168, ppl=4.49, wps=23122.8, ups=8.12, wpb=2848, bsz=128, num_updates=19200, lr=0.0005, gnorm=1.312, loss_scale=32, train_wall=12, gb_free=11.8, wall=88]2022-05-26 11:09:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 14.76 GiB total capacity; 5.92 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:09:43 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 16           |        cudaMalloc retries: 20        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6062 MB |    8125 MB |    3500 GB |    3494 GB |\n","|       from large pool |    6052 MB |    8114 MB |    3434 GB |    3428 GB |\n","|       from small pool |       9 MB |      17 MB |      66 GB |      66 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6062 MB |    8125 MB |    3500 GB |    3494 GB |\n","|       from large pool |    6052 MB |    8114 MB |    3434 GB |    3428 GB |\n","|       from small pool |       9 MB |      17 MB |      66 GB |      66 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12318 MB |   12412 MB |   31544 MB |   19226 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      10 MB |     104 MB |    1536 MB |    1526 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6255 MB |    6255 MB |    7203 GB |    7197 GB |\n","|       from large pool |    6255 MB |    6255 MB |    7126 GB |    7120 GB |\n","|       from small pool |       0 MB |      17 MB |      77 GB |      77 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  541281    |  540957    |\n","|       from large pool |     142    |     146    |  242485    |  242343    |\n","|       from small pool |     182    |     266    |  298796    |  298614    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  541281    |  540957    |\n","|       from large pool |     142    |     146    |  242485    |  242343    |\n","|       from small pool |     182    |     266    |  298796    |  298614    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      68    |     792    |     771    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       5    |      52    |     768    |     763    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      40    |  281922    |  281901    |\n","|       from large pool |      16    |      17    |  151636    |  151620    |\n","|       from small pool |       5    |      32    |  130286    |  130281    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:09:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  60% 944/1563 [02:00<01:02,  9.93it/s, loss=2.447, ppl=5.45, wps=24653.8, ups=8.14, wpb=3029.6, bsz=128, num_updates=19400, lr=0.0005, gnorm=1.367, loss_scale=32, train_wall=12, gb_free=11.9, wall=113]2022-05-26 11:10:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.04 GiB (GPU 0; 14.76 GiB total capacity; 7.08 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:13 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 17           |        cudaMalloc retries: 21        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7246 MB |    9838 MB |    4696 GB |    4689 GB |\n","|       from large pool |    7236 MB |    9828 MB |    4600 GB |    4593 GB |\n","|       from small pool |      10 MB |      17 MB |      95 GB |      95 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7246 MB |    9838 MB |    4696 GB |    4689 GB |\n","|       from large pool |    7236 MB |    9828 MB |    4600 GB |    4593 GB |\n","|       from small pool |      10 MB |      17 MB |      95 GB |      95 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12422 MB |   31648 MB |   19328 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     114 MB |    1640 MB |    1628 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5073 MB |    5073 MB |    9895 GB |    9890 GB |\n","|       from large pool |    5071 MB |    5071 MB |    9784 GB |    9779 GB |\n","|       from small pool |       1 MB |      19 MB |     110 GB |     110 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |  741600    |  741276    |\n","|       from large pool |     142    |     147    |  323917    |  323775    |\n","|       from small pool |     182    |     266    |  417683    |  417501    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |  741600    |  741276    |\n","|       from large pool |     142    |     147    |  323917    |  323775    |\n","|       from small pool |     182    |     266    |  417683    |  417501    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |     844    |     822    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      57    |     820    |     814    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      41    |  386423    |  386399    |\n","|       from large pool |      16    |      17    |  202364    |  202348    |\n","|       from small pool |       8    |      33    |  184059    |  184051    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  64% 1001/1563 [02:07<01:06,  8.41it/s, loss=2.073, ppl=4.21, wps=22937, ups=8.81, wpb=2604.7, bsz=128, num_updates=19500, lr=0.0005, gnorm=1.35, loss_scale=32, train_wall=11, gb_free=9.9, wall=124]2022-05-26 11:10:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.88 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 18           |        cudaMalloc retries: 22        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7048 MB |    9439 MB |    4980 GB |    4973 GB |\n","|       from large pool |    7037 MB |    9429 MB |    4879 GB |    4872 GB |\n","|       from small pool |      10 MB |      17 MB |     100 GB |     100 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7048 MB |    9439 MB |    4980 GB |    4973 GB |\n","|       from large pool |    7037 MB |    9429 MB |    4879 GB |    4872 GB |\n","|       from small pool |      10 MB |      17 MB |     100 GB |     100 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12422 MB |   31750 MB |   19430 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     114 MB |    1742 MB |    1730 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5271 MB |    5271 MB |   10530 GB |   10525 GB |\n","|       from large pool |    5270 MB |    5270 MB |   10413 GB |   10408 GB |\n","|       from small pool |       1 MB |      10 MB |     117 GB |     116 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     786 K  |     786 K  |\n","|       from large pool |     142    |     147    |     344 K  |     343 K  |\n","|       from small pool |     182    |     266    |     442 K  |     442 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     786 K  |     786 K  |\n","|       from large pool |     142    |     147    |     344 K  |     343 K  |\n","|       from small pool |     182    |     266    |     442 K  |     442 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |     895    |     873    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      57    |     871    |     865    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      33    |  409750    |  409720    |\n","|       from large pool |      21    |      22    |  214911    |  214890    |\n","|       from small pool |       9    |      25    |  194839    |  194830    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  67% 1048/1563 [02:12<00:54,  9.46it/s, loss=2.073, ppl=4.21, wps=22937, ups=8.81, wpb=2604.7, bsz=128, num_updates=19500, lr=0.0005, gnorm=1.35, loss_scale=32, train_wall=11, gb_free=9.9, wall=124]2022-05-26 11:10:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.11 GiB (GPU 0; 14.76 GiB total capacity; 6.03 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 013:  67% 1049/1563 [02:12<00:56,  9.10it/s, loss=2.073, ppl=4.21, wps=22937, ups=8.81, wpb=2604.7, bsz=128, num_updates=19500, lr=0.0005, gnorm=1.35, loss_scale=32, train_wall=11, gb_free=9.9, wall=124]2022-05-26 11:10:25 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 19           |        cudaMalloc retries: 23        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6173 MB |    8287 MB |    5202 GB |    5196 GB |\n","|       from large pool |    6163 MB |    8276 MB |    5096 GB |    5090 GB |\n","|       from small pool |       9 MB |      17 MB |     105 GB |     105 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6173 MB |    8287 MB |    5202 GB |    5196 GB |\n","|       from large pool |    6163 MB |    8276 MB |    5096 GB |    5090 GB |\n","|       from small pool |       9 MB |      17 MB |     105 GB |     105 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12318 MB |   12422 MB |   31852 MB |   19534 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      10 MB |     114 MB |    1844 MB |    1834 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6144 MB |    6144 MB |   11036 GB |   11030 GB |\n","|       from large pool |    6144 MB |    6144 MB |   10914 GB |   10908 GB |\n","|       from small pool |       0 MB |      13 MB |     122 GB |     122 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     823 K  |     822 K  |\n","|       from large pool |     142    |     146    |     359 K  |     359 K  |\n","|       from small pool |     182    |     266    |     463 K  |     463 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     823 K  |     822 K  |\n","|       from large pool |     142    |     146    |     359 K  |     359 K  |\n","|       from small pool |     182    |     266    |     463 K  |     463 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      73    |     946    |     925    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       5    |      57    |     922    |     917    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      35    |  428797    |  428776    |\n","|       from large pool |      16    |      17    |  224626    |  224610    |\n","|       from small pool |       5    |      27    |  204171    |  204166    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  68% 1068/1563 [02:15<00:59,  8.28it/s, loss=2.326, ppl=5.01, wps=24246.6, ups=8.06, wpb=3007.9, bsz=127.4, num_updates=19600, lr=0.0005, gnorm=1.315, loss_scale=32, train_wall=12, gb_free=9.4, wall=137]2022-05-26 11:10:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.67 GiB (GPU 0; 14.76 GiB total capacity; 5.55 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:28 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 20           |        cudaMalloc retries: 24        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5683 MB |    7570 MB |    5319 GB |    5313 GB |\n","|       from large pool |    5673 MB |    7559 MB |    5211 GB |    5206 GB |\n","|       from small pool |       9 MB |      17 MB |     107 GB |     107 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5683 MB |    7570 MB |    5319 GB |    5313 GB |\n","|       from large pool |    5673 MB |    7559 MB |    5211 GB |    5206 GB |\n","|       from small pool |       9 MB |      17 MB |     107 GB |     107 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12322 MB |   12420 MB |   31954 MB |   19632 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      14 MB |     112 MB |    1946 MB |    1932 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6638 MB |    6638 MB |   11275 GB |   11268 GB |\n","|       from large pool |    6634 MB |    6634 MB |   11150 GB |   11144 GB |\n","|       from small pool |       4 MB |      10 MB |     124 GB |     124 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     322    |     325    |     838 K  |     838 K  |\n","|       from large pool |     142    |     146    |     367 K  |     366 K  |\n","|       from small pool |     180    |     266    |     471 K  |     471 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     322    |     325    |     838 K  |     838 K  |\n","|       from large pool |     142    |     146    |     367 K  |     366 K  |\n","|       from small pool |     180    |     266    |     471 K  |     471 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      72    |     997    |     974    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       7    |      56    |     973    |     966    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      28    |  436850    |  436828    |\n","|       from large pool |      15    |      16    |  229214    |  229199    |\n","|       from small pool |       7    |      20    |  207636    |  207629    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  71% 1105/1563 [02:19<00:55,  8.29it/s, loss=2.326, ppl=5.01, wps=24246.6, ups=8.06, wpb=3007.9, bsz=127.4, num_updates=19600, lr=0.0005, gnorm=1.315, loss_scale=32, train_wall=12, gb_free=9.4, wall=137]2022-05-26 11:10:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.02 GiB already allocated; 1.48 GiB free; 12.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 013:  71% 1106/1563 [02:19<00:55,  8.29it/s, loss=2.326, ppl=5.01, wps=24246.6, ups=8.06, wpb=3007.9, bsz=127.4, num_updates=19600, lr=0.0005, gnorm=1.315, loss_scale=32, train_wall=12, gb_free=9.4, wall=137]2022-05-26 11:10:32 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 21           |        cudaMalloc retries: 25        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6168 MB |    8331 MB |    5490 GB |    5484 GB |\n","|       from large pool |    6150 MB |    8313 MB |    5379 GB |    5373 GB |\n","|       from small pool |      17 MB |      17 MB |     111 GB |     111 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6168 MB |    8331 MB |    5490 GB |    5484 GB |\n","|       from large pool |    6150 MB |    8313 MB |    5379 GB |    5373 GB |\n","|       from small pool |      17 MB |      17 MB |     111 GB |     111 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12328 MB |   12422 MB |   32054 MB |   19726 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      20 MB |     114 MB |    2046 MB |    2026 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6159 MB |    6159 MB |   11667 GB |   11661 GB |\n","|       from large pool |    6157 MB |    6157 MB |   11537 GB |   11531 GB |\n","|       from small pool |       2 MB |      41 MB |     129 GB |     129 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     867 K  |     867 K  |\n","|       from large pool |     132    |     136    |     378 K  |     378 K  |\n","|       from small pool |     192    |     266    |     489 K  |     488 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     867 K  |     867 K  |\n","|       from large pool |     132    |     136    |     378 K  |     378 K  |\n","|       from small pool |     192    |     266    |     489 K  |     488 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      73    |    1047    |    1021    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |      10    |      57    |    1023    |    1013    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      55    |  451986    |  451956    |\n","|       from large pool |      17    |      18    |  236181    |  236164    |\n","|       from small pool |      13    |      47    |  215805    |  215792    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  72% 1129/1563 [02:22<00:57,  7.53it/s, loss=2.326, ppl=5.01, wps=24246.6, ups=8.06, wpb=3007.9, bsz=127.4, num_updates=19600, lr=0.0005, gnorm=1.315, loss_scale=32, train_wall=12, gb_free=9.4, wall=137]2022-05-26 11:10:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 5.58 GiB already allocated; 1.48 GiB free; 12.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 22           |        cudaMalloc retries: 26        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5713 MB |    5717 MB |    5604 GB |    5599 GB |\n","|       from large pool |    5693 MB |    5697 MB |    5490 GB |    5485 GB |\n","|       from small pool |      19 MB |      19 MB |     114 GB |     113 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5713 MB |    5717 MB |    5604 GB |    5599 GB |\n","|       from large pool |    5693 MB |    5697 MB |    5490 GB |    5485 GB |\n","|       from small pool |      19 MB |      19 MB |     114 GB |     113 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12330 MB |   12420 MB |   32146 MB |   19816 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      22 MB |     112 MB |    2138 MB |    2116 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6616 MB |    6616 MB |   11931 GB |   11924 GB |\n","|       from large pool |    6614 MB |    6614 MB |   11798 GB |   11792 GB |\n","|       from small pool |       2 MB |      11 MB |     132 GB |     132 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     323    |     327    |     886 K  |     885 K  |\n","|       from large pool |     134    |     136    |     386 K  |     386 K  |\n","|       from small pool |     189    |     266    |     499 K  |     499 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     323    |     327    |     886 K  |     885 K  |\n","|       from large pool |     134    |     136    |     386 K  |     386 K  |\n","|       from small pool |     189    |     266    |     499 K  |     499 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      27    |      72    |    1093    |    1066    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |      11    |      56    |    1069    |    1058    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      33    |  461724    |  461694    |\n","|       from large pool |      17    |      17    |  241167    |  241150    |\n","|       from small pool |      13    |      25    |  220557    |  220544    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  75% 1178/1563 [02:28<00:38, 10.02it/s, loss=2.463, ppl=5.51, wps=23343.6, ups=7.97, wpb=2927.2, bsz=128, num_updates=19700, lr=0.0005, gnorm=1.339, loss_scale=32, train_wall=12, gb_free=12.2, wall=149]2022-05-26 11:10:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.71 GiB (GPU 0; 14.76 GiB total capacity; 5.60 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:41 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 23           |        cudaMalloc retries: 27        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5733 MB |    7645 MB |    5865 GB |    5859 GB |\n","|       from large pool |    5723 MB |    7634 MB |    5745 GB |    5740 GB |\n","|       from small pool |       9 MB |      17 MB |     119 GB |     119 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5733 MB |    7645 MB |    5865 GB |    5859 GB |\n","|       from large pool |    5723 MB |    7634 MB |    5745 GB |    5740 GB |\n","|       from small pool |       9 MB |      17 MB |     119 GB |     119 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12422 MB |   32238 MB |   19918 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     114 MB |    2230 MB |    2218 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6586 MB |    6586 MB |   12468 GB |   12461 GB |\n","|       from large pool |    6584 MB |    6584 MB |   12329 GB |   12322 GB |\n","|       from small pool |       2 MB |      28 MB |     138 GB |     138 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     924 K  |     924 K  |\n","|       from large pool |     142    |     146    |     402 K  |     402 K  |\n","|       from small pool |     182    |     266    |     522 K  |     521 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     924 K  |     924 K  |\n","|       from large pool |     142    |     146    |     402 K  |     402 K  |\n","|       from small pool |     182    |     266    |     522 K  |     521 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    1139    |    1117    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      57    |    1115    |    1109    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      49    |  481878    |  481852    |\n","|       from large pool |      19    |      20    |  251338    |  251319    |\n","|       from small pool |       7    |      41    |  230540    |  230533    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  76% 1190/1563 [02:30<00:49,  7.52it/s, loss=2.463, ppl=5.51, wps=23343.6, ups=7.97, wpb=2927.2, bsz=128, num_updates=19700, lr=0.0005, gnorm=1.339, loss_scale=32, train_wall=12, gb_free=12.2, wall=149]2022-05-26 11:10:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 14.76 GiB total capacity; 6.05 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:43 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 24           |        cudaMalloc retries: 28        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6197 MB |    8260 MB |    5933 GB |    5927 GB |\n","|       from large pool |    6186 MB |    8250 MB |    5813 GB |    5807 GB |\n","|       from small pool |      10 MB |      17 MB |     120 GB |     120 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6197 MB |    8260 MB |    5933 GB |    5927 GB |\n","|       from large pool |    6186 MB |    8250 MB |    5813 GB |    5807 GB |\n","|       from small pool |      10 MB |      17 MB |     120 GB |     120 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12322 MB |   12386 MB |   32304 MB |   19982 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      14 MB |      78 MB |    2296 MB |    2282 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6124 MB |    6124 MB |   12612 GB |   12606 GB |\n","|       from large pool |    6121 MB |    6121 MB |   12473 GB |   12467 GB |\n","|       from small pool |       3 MB |      14 MB |     139 GB |     139 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     933 K  |     933 K  |\n","|       from large pool |     142    |     147    |     407 K  |     407 K  |\n","|       from small pool |     182    |     266    |     526 K  |     525 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     933 K  |     933 K  |\n","|       from large pool |     142    |     147    |     407 K  |     407 K  |\n","|       from small pool |     182    |     266    |     526 K  |     525 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      55    |    1172    |    1149    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       7    |      39    |    1148    |    1141    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      36    |  486730    |  486702    |\n","|       from large pool |      20    |      21    |  254566    |  254546    |\n","|       from small pool |       8    |      28    |  232164    |  232156    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  78% 1220/1563 [02:33<00:36,  9.44it/s, loss=2.463, ppl=5.51, wps=23343.6, ups=7.97, wpb=2927.2, bsz=128, num_updates=19700, lr=0.0005, gnorm=1.339, loss_scale=32, train_wall=12, gb_free=12.2, wall=149]2022-05-26 11:10:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n","epoch 013:  79% 1236/1563 [02:35<00:42,  7.74it/s, loss=2.463, ppl=5.51, wps=23343.6, ups=7.97, wpb=2927.2, bsz=128, num_updates=19700, lr=0.0005, gnorm=1.339, loss_scale=32, train_wall=12, gb_free=12.2, wall=149]2022-05-26 11:10:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.25 GiB (GPU 0; 14.76 GiB total capacity; 6.16 GiB already allocated; 1.49 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:48 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 25           |        cudaMalloc retries: 29        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6312 MB |    8501 MB |    6171 GB |    6165 GB |\n","|       from large pool |    6302 MB |    8490 MB |    6046 GB |    6040 GB |\n","|       from small pool |       9 MB |      17 MB |     124 GB |     124 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6312 MB |    8501 MB |    6171 GB |    6165 GB |\n","|       from large pool |    6302 MB |    8490 MB |    6046 GB |    6040 GB |\n","|       from small pool |       9 MB |      17 MB |     124 GB |     124 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12320 MB |   12422 MB |   32404 MB |   20084 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      12 MB |     114 MB |    2396 MB |    2384 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6007 MB |    6007 MB |   13124 GB |   13118 GB |\n","|       from large pool |    6005 MB |    6005 MB |   12979 GB |   12973 GB |\n","|       from small pool |       2 MB |      17 MB |     144 GB |     144 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     969 K  |     968 K  |\n","|       from large pool |     142    |     146    |     423 K  |     423 K  |\n","|       from small pool |     182    |     266    |     545 K  |     545 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     969 K  |     968 K  |\n","|       from large pool |     142    |     146    |     423 K  |     423 K  |\n","|       from small pool |     182    |     266    |     545 K  |     545 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    1222    |    1200    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |       6    |      57    |    1198    |    1192    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      44    |  504995    |  504971    |\n","|       from large pool |      16    |      17    |  264379    |  264363    |\n","|       from small pool |       8    |      36    |  240616    |  240608    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  80% 1251/1563 [02:37<00:37,  8.40it/s, loss=2.463, ppl=5.51, wps=23343.6, ups=7.97, wpb=2927.2, bsz=128, num_updates=19700, lr=0.0005, gnorm=1.339, loss_scale=32, train_wall=12, gb_free=12.2, wall=149]2022-05-26 11:10:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.96 GiB already allocated; 1.48 GiB free; 12.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:10:50 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 26           |        cudaMalloc retries: 30        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7122 MB |    9689 MB |    6253 GB |    6246 GB |\n","|       from large pool |    7102 MB |    9668 MB |    6127 GB |    6120 GB |\n","|       from small pool |      20 MB |      20 MB |     126 GB |     126 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7122 MB |    9689 MB |    6253 GB |    6246 GB |\n","|       from large pool |    7102 MB |    9668 MB |    6127 GB |    6120 GB |\n","|       from small pool |      20 MB |      20 MB |     126 GB |     126 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12330 MB |   12422 MB |   32506 MB |   20176 MB |\n","|       from large pool |   12308 MB |   12308 MB |   30008 MB |   17700 MB |\n","|       from small pool |      22 MB |     114 MB |    2498 MB |    2476 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5207 MB |    5207 MB |   13294 GB |   13289 GB |\n","|       from large pool |    5205 MB |    5205 MB |   13147 GB |   13142 GB |\n","|       from small pool |       1 MB |      17 MB |     146 GB |     146 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |     981 K  |     980 K  |\n","|       from large pool |     132    |     136    |     428 K  |     428 K  |\n","|       from small pool |     192    |     266    |     553 K  |     552 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |     981 K  |     980 K  |\n","|       from large pool |     132    |     136    |     428 K  |     428 K  |\n","|       from small pool |     192    |     266    |     553 K  |     552 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      27    |      73    |    1273    |    1246    |\n","|       from large pool |      16    |      16    |      24    |       8    |\n","|       from small pool |      11    |      57    |    1249    |    1238    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      38    |  511270    |  511246    |\n","|       from large pool |      13    |      14    |  267281    |  267268    |\n","|       from small pool |      11    |      30    |  243989    |  243978    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:10:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  86% 1350/1563 [02:49<00:25,  8.24it/s, loss=2.378, ppl=5.2, wps=22931.7, ups=7.86, wpb=2917.8, bsz=128, num_updates=19800, lr=0.0005, gnorm=1.363, loss_scale=16, train_wall=12, gb_free=8.5, wall=162]2022-05-26 11:11:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.20 GiB (GPU 0; 14.76 GiB total capacity; 10.56 GiB already allocated; 791.75 MiB free; 12.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:02 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 27           |        cudaMalloc retries: 32        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6312 MB |   10817 MB |    6746 GB |    6740 GB |\n","|       from large pool |    6274 MB |   10778 MB |    6609 GB |    6603 GB |\n","|       from small pool |      38 MB |      39 MB |     136 GB |     136 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6312 MB |   10817 MB |    6746 GB |    6740 GB |\n","|       from large pool |    6274 MB |   10778 MB |    6609 GB |    6603 GB |\n","|       from small pool |      38 MB |      39 MB |     136 GB |     136 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13052 MB |   13052 MB |   37104 MB |   24052 MB |\n","|       from large pool |   13010 MB |   13010 MB |   34514 MB |   21504 MB |\n","|       from small pool |      42 MB |     114 MB |    2590 MB |    2548 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    2233 MB |    3784 MB |   14382 GB |   14380 GB |\n","|       from large pool |    2229 MB |    3781 MB |   14223 GB |   14221 GB |\n","|       from small pool |       3 MB |      11 MB |     158 GB |     158 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     323    |     327    |    1059 K  |    1059 K  |\n","|       from large pool |      87    |      90    |     461 K  |     461 K  |\n","|       from small pool |     236    |     266    |     597 K  |     597 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     323    |     327    |    1059 K  |    1059 K  |\n","|       from large pool |      87    |      90    |     461 K  |     461 K  |\n","|       from small pool |     236    |     266    |     597 K  |     597 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      37    |      73    |    1320    |    1283    |\n","|       from large pool |      16    |      16    |      25    |       9    |\n","|       from small pool |      21    |      57    |    1295    |    1274    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      37    |      38    |  551980    |  551943    |\n","|       from large pool |      11    |      12    |  288084    |  288073    |\n","|       from small pool |      26    |      27    |  263896    |  263870    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  88% 1378/1563 [02:53<00:24,  7.55it/s, loss=2.434, ppl=5.4, wps=23663, ups=8.05, wpb=2939.8, bsz=128, num_updates=19900, lr=0.0005, gnorm=1.351, loss_scale=16, train_wall=12, gb_free=9.7, wall=174]2022-05-26 11:11:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.76 GiB total capacity; 5.85 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 013:  88% 1380/1563 [02:53<00:22,  8.07it/s, loss=2.434, ppl=5.4, wps=23663, ups=8.05, wpb=2939.8, bsz=128, num_updates=19900, lr=0.0005, gnorm=1.351, loss_scale=16, train_wall=12, gb_free=9.7, wall=174]2022-05-26 11:11:06 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 28           |        cudaMalloc retries: 33        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5993 MB |    8031 MB |    6894 GB |    6888 GB |\n","|       from large pool |    5983 MB |    8020 MB |    6754 GB |    6748 GB |\n","|       from small pool |       9 MB |      17 MB |     139 GB |     139 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5993 MB |    8031 MB |    6894 GB |    6888 GB |\n","|       from large pool |    5983 MB |    8020 MB |    6754 GB |    6748 GB |\n","|       from small pool |       9 MB |      17 MB |     139 GB |     139 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12620 MB |   41178 MB |   28656 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     110 MB |    2658 MB |    2646 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6528 MB |    6528 MB |   14718 GB |   14711 GB |\n","|       from large pool |    6526 MB |    6526 MB |   14555 GB |   14549 GB |\n","|       from small pool |       2 MB |      31 MB |     162 GB |     162 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    1081 K  |    1081 K  |\n","|       from large pool |     142    |     146    |     471 K  |     471 K  |\n","|       from small pool |     182    |     266    |     610 K  |     610 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    1081 K  |    1081 K  |\n","|       from large pool |     142    |     146    |     471 K  |     471 K  |\n","|       from small pool |     182    |     266    |     610 K  |     610 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      71    |    1355    |    1333    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      55    |    1329    |    1323    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      54    |  563530    |  563507    |\n","|       from large pool |      17    |      18    |  293924    |  293907    |\n","|       from small pool |       6    |      46    |  269606    |  269600    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  93% 1450/1563 [03:01<00:12,  9.18it/s, loss=2.434, ppl=5.4, wps=23663, ups=8.05, wpb=2939.8, bsz=128, num_updates=19900, lr=0.0005, gnorm=1.351, loss_scale=16, train_wall=12, gb_free=9.7, wall=174]2022-05-26 11:11:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 14.76 GiB total capacity; 5.82 GiB already allocated; 1.28 GiB free; 12.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:14 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 29           |        cudaMalloc retries: 34        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5954 MB |    8017 MB |    7217 GB |    7211 GB |\n","|       from large pool |    5937 MB |    7999 MB |    7068 GB |    7062 GB |\n","|       from small pool |      17 MB |      18 MB |     148 GB |     148 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5954 MB |    8017 MB |    7217 GB |    7211 GB |\n","|       from large pool |    5937 MB |    7999 MB |    7068 GB |    7062 GB |\n","|       from small pool |      17 MB |      18 MB |     148 GB |     148 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12530 MB |   12618 MB |   41274 MB |   28744 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      20 MB |     108 MB |    2754 MB |    2734 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6575 MB |    6575 MB |   15454 GB |   15448 GB |\n","|       from large pool |    6572 MB |    6572 MB |   15282 GB |   15275 GB |\n","|       from small pool |       2 MB |      17 MB |     172 GB |     172 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    1137 K  |    1136 K  |\n","|       from large pool |     132    |     136    |     492 K  |     492 K  |\n","|       from small pool |     192    |     266    |     645 K  |     644 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    1137 K  |    1136 K  |\n","|       from large pool |     132    |     136    |     492 K  |     492 K  |\n","|       from small pool |     192    |     266    |     645 K  |     644 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      70    |    1403    |    1377    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |      10    |      54    |    1377    |    1367    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      42    |  592357    |  592330    |\n","|       from large pool |      14    |      15    |  306870    |  306856    |\n","|       from small pool |      13    |      34    |  285487    |  285474    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  94% 1472/1563 [03:04<00:11,  7.98it/s, loss=2.434, ppl=5.4, wps=23663, ups=8.05, wpb=2939.8, bsz=128, num_updates=19900, lr=0.0005, gnorm=1.351, loss_scale=16, train_wall=12, gb_free=9.7, wall=174]2022-05-26 11:11:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.44 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:17 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 30           |        cudaMalloc retries: 35        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7617 MB |   10284 MB |    7347 GB |    7339 GB |\n","|       from large pool |    7606 MB |   10274 MB |    7197 GB |    7189 GB |\n","|       from small pool |      10 MB |      17 MB |     149 GB |     149 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7617 MB |   10284 MB |    7347 GB |    7339 GB |\n","|       from large pool |    7606 MB |   10274 MB |    7197 GB |    7189 GB |\n","|       from small pool |      10 MB |      17 MB |     149 GB |     149 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12622 MB |   41366 MB |   28844 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     112 MB |    2846 MB |    2834 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4904 MB |    4904 MB |   15726 GB |   15722 GB |\n","|       from large pool |    4903 MB |    4903 MB |   15553 GB |   15548 GB |\n","|       from small pool |       1 MB |      17 MB |     173 GB |     173 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    1154 K  |    1154 K  |\n","|       from large pool |     142    |     147    |     500 K  |     500 K  |\n","|       from small pool |     182    |     266    |     653 K  |     653 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    1154 K  |    1154 K  |\n","|       from large pool |     142    |     147    |     500 K  |     500 K  |\n","|       from small pool |     182    |     266    |     653 K  |     653 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |    1449    |    1427    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      56    |    1423    |    1417    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      38    |  601259    |  601230    |\n","|       from large pool |      22    |      23    |  312370    |  312348    |\n","|       from small pool |       7    |      30    |  288889    |  288882    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  97% 1523/1563 [03:10<00:03, 10.32it/s, loss=2.477, ppl=5.57, wps=23306.3, ups=8.05, wpb=2894.5, bsz=128, num_updates=20000, lr=0.0005, gnorm=1.389, loss_scale=16, train_wall=12, gb_free=8.3, wall=187]2022-05-26 11:11:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.84 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:23 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 31           |        cudaMalloc retries: 36        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8029 MB |   10898 MB |    7588 GB |    7580 GB |\n","|       from large pool |    8019 MB |   10888 MB |    7432 GB |    7424 GB |\n","|       from small pool |      10 MB |      17 MB |     155 GB |     155 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8029 MB |   10898 MB |    7588 GB |    7580 GB |\n","|       from large pool |    8019 MB |   10888 MB |    7432 GB |    7424 GB |\n","|       from small pool |      10 MB |      17 MB |     155 GB |     155 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12624 MB |   41468 MB |   28946 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     114 MB |    2948 MB |    2936 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4492 MB |    4492 MB |   16286 GB |   16281 GB |\n","|       from large pool |    4490 MB |    4490 MB |   16105 GB |   16101 GB |\n","|       from small pool |       1 MB |      21 MB |     180 GB |     180 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    1194 K  |    1193 K  |\n","|       from large pool |     142    |     147    |     516 K  |     516 K  |\n","|       from small pool |     182    |     266    |     677 K  |     677 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    1194 K  |    1193 K  |\n","|       from large pool |     142    |     147    |     516 K  |     516 K  |\n","|       from small pool |     182    |     266    |     677 K  |     677 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    1500    |    1478    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      57    |    1474    |    1468    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      38    |  621768    |  621740    |\n","|       from large pool |      21    |      22    |  322240    |  322219    |\n","|       from small pool |       7    |      30    |  299528    |  299521    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  99% 1544/1563 [03:12<00:02,  9.48it/s, loss=2.477, ppl=5.57, wps=23306.3, ups=8.05, wpb=2894.5, bsz=128, num_updates=20000, lr=0.0005, gnorm=1.389, loss_scale=16, train_wall=12, gb_free=8.3, wall=187]2022-05-26 11:11:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.11 GiB (GPU 0; 14.76 GiB total capacity; 6.06 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:25 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 32           |        cudaMalloc retries: 37        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6209 MB |    8323 MB |    7691 GB |    7685 GB |\n","|       from large pool |    6199 MB |    8313 MB |    7533 GB |    7527 GB |\n","|       from small pool |       9 MB |      17 MB |     158 GB |     157 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6209 MB |    8323 MB |    7691 GB |    7685 GB |\n","|       from large pool |    6199 MB |    8313 MB |    7533 GB |    7527 GB |\n","|       from small pool |       9 MB |      17 MB |     158 GB |     157 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12610 MB |   41556 MB |   29034 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     100 MB |    3036 MB |    3024 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6312 MB |    6312 MB |   16524 GB |   16518 GB |\n","|       from large pool |    6310 MB |    6310 MB |   16341 GB |   16335 GB |\n","|       from small pool |       2 MB |      19 MB |     183 GB |     183 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    1210 K  |    1210 K  |\n","|       from large pool |     142    |     147    |     523 K  |     523 K  |\n","|       from small pool |     182    |     266    |     686 K  |     686 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    1210 K  |    1210 K  |\n","|       from large pool |     142    |     147    |     523 K  |     523 K  |\n","|       from small pool |     182    |     266    |     686 K  |     686 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      66    |    1544    |    1522    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      50    |    1518    |    1512    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      34    |  630127    |  630102    |\n","|       from large pool |      18    |      19    |  326217    |  326199    |\n","|       from small pool |       7    |      27    |  303910    |  303903    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  99% 1552/1563 [03:13<00:01,  9.24it/s, loss=2.477, ppl=5.57, wps=23306.3, ups=8.05, wpb=2894.5, bsz=128, num_updates=20000, lr=0.0005, gnorm=1.389, loss_scale=16, train_wall=12, gb_free=8.3, wall=187]2022-05-26 11:11:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 7.05 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:26 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 33           |        cudaMalloc retries: 38        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7217 MB |    9784 MB |    7730 GB |    7723 GB |\n","|       from large pool |    7207 MB |    9773 MB |    7571 GB |    7564 GB |\n","|       from small pool |      10 MB |      17 MB |     159 GB |     159 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7217 MB |    9784 MB |    7730 GB |    7723 GB |\n","|       from large pool |    7207 MB |    9773 MB |    7571 GB |    7564 GB |\n","|       from small pool |      10 MB |      17 MB |     159 GB |     159 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12610 MB |   41644 MB |   29122 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     100 MB |    3124 MB |    3112 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5304 MB |    5304 MB |   16605 GB |   16600 GB |\n","|       from large pool |    5302 MB |    5302 MB |   16421 GB |   16416 GB |\n","|       from small pool |       1 MB |      17 MB |     184 GB |     184 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |    1216 K  |    1215 K  |\n","|       from large pool |     142    |     147    |     525 K  |     525 K  |\n","|       from small pool |     182    |     266    |     690 K  |     690 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |    1216 K  |    1215 K  |\n","|       from large pool |     142    |     147    |     525 K  |     525 K  |\n","|       from small pool |     182    |     266    |     690 K  |     690 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      66    |    1588    |    1566    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      50    |    1562    |    1556    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      41    |  633322    |  633300    |\n","|       from large pool |      14    |      15    |  327760    |  327746    |\n","|       from small pool |       8    |      33    |  305562    |  305554    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013: 100% 1562/1563 [03:14<00:00,  8.49it/s, loss=2.477, ppl=5.57, wps=23306.3, ups=8.05, wpb=2894.5, bsz=128, num_updates=20000, lr=0.0005, gnorm=1.389, loss_scale=16, train_wall=12, gb_free=8.3, wall=187]2022-05-26 11:11:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 013 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   0% 1/337 [00:00<01:49,  3.07it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   1% 5/337 [00:00<00:26, 12.70it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   3% 9/337 [00:00<00:16, 19.68it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   4% 12/337 [00:00<00:15, 20.68it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   5% 16/337 [00:00<00:13, 24.59it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   6% 20/337 [00:00<00:11, 28.51it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   7% 24/337 [00:01<00:10, 29.74it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   8% 28/337 [00:01<00:10, 30.69it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   9% 32/337 [00:01<00:09, 32.36it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  11% 36/337 [00:01<00:09, 31.06it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  12% 40/337 [00:01<00:09, 31.79it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  13% 44/337 [00:01<00:10, 28.50it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  14% 48/337 [00:01<00:09, 30.60it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  15% 52/337 [00:01<00:09, 29.36it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  17% 56/337 [00:02<00:08, 31.29it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  18% 60/337 [00:02<00:09, 28.23it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  19% 64/337 [00:02<00:09, 29.48it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  20% 68/337 [00:02<00:08, 30.77it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  21% 72/337 [00:02<00:09, 28.71it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  23% 76/337 [00:02<00:08, 30.27it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  24% 80/337 [00:02<00:08, 29.48it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  25% 84/337 [00:03<00:08, 28.60it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  26% 88/337 [00:03<00:08, 30.12it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  27% 92/337 [00:03<00:09, 26.92it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  28% 96/337 [00:03<00:08, 28.47it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  30% 100/337 [00:03<00:07, 29.94it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  31% 104/337 [00:03<00:08, 28.07it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  32% 108/337 [00:03<00:07, 29.66it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  33% 112/337 [00:04<00:07, 30.65it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  34% 116/337 [00:04<00:07, 30.21it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  36% 120/337 [00:04<00:07, 28.30it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  37% 124/337 [00:04<00:07, 28.92it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  38% 127/337 [00:04<00:07, 29.17it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  39% 130/337 [00:04<00:08, 25.48it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  40% 134/337 [00:04<00:07, 27.22it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  41% 138/337 [00:04<00:07, 28.38it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  42% 141/337 [00:05<00:06, 28.73it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  43% 144/337 [00:05<00:07, 26.50it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  44% 148/337 [00:05<00:06, 28.29it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  45% 151/337 [00:05<00:06, 28.06it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  46% 154/337 [00:05<00:06, 27.99it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  47% 157/337 [00:05<00:07, 25.53it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  48% 161/337 [00:05<00:06, 26.99it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  49% 164/337 [00:05<00:06, 26.94it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  50% 167/337 [00:06<00:06, 27.24it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  50% 170/337 [00:06<00:07, 22.82it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  52% 174/337 [00:06<00:06, 25.17it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  53% 177/337 [00:06<00:06, 25.79it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  53% 180/337 [00:06<00:06, 25.32it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  54% 183/337 [00:06<00:06, 24.65it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  55% 186/337 [00:06<00:05, 25.89it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  56% 189/337 [00:06<00:05, 26.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  57% 192/337 [00:07<00:05, 26.31it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  58% 195/337 [00:07<00:05, 25.10it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  59% 198/337 [00:07<00:05, 26.01it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  60% 201/337 [00:07<00:05, 25.68it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  61% 204/337 [00:07<00:05, 25.07it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  61% 207/337 [00:07<00:05, 23.39it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  62% 210/337 [00:07<00:05, 24.30it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  63% 213/337 [00:07<00:05, 24.05it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  64% 216/337 [00:08<00:05, 23.69it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  65% 219/337 [00:08<00:05, 22.43it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  66% 222/337 [00:08<00:04, 23.41it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  67% 225/337 [00:08<00:04, 23.90it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  68% 228/337 [00:08<00:05, 20.51it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  69% 231/337 [00:08<00:04, 21.92it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  69% 234/337 [00:08<00:04, 22.69it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  70% 237/337 [00:09<00:04, 20.72it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  71% 240/337 [00:09<00:04, 22.26it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  72% 243/337 [00:09<00:04, 23.04it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  73% 246/337 [00:09<00:04, 21.22it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  74% 249/337 [00:09<00:04, 21.90it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  75% 252/337 [00:09<00:03, 22.50it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  76% 255/337 [00:09<00:04, 19.98it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  77% 258/337 [00:09<00:03, 21.34it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  77% 261/337 [00:10<00:03, 21.49it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  78% 264/337 [00:10<00:03, 20.42it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  79% 267/337 [00:10<00:03, 21.34it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  80% 270/337 [00:10<00:03, 20.85it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  81% 273/337 [00:10<00:03, 20.56it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  82% 276/337 [00:10<00:02, 20.96it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  83% 279/337 [00:11<00:03, 18.95it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  84% 282/337 [00:11<00:02, 19.50it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  84% 284/337 [00:11<00:03, 16.73it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  85% 287/337 [00:11<00:02, 18.33it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  86% 289/337 [00:11<00:02, 18.12it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  87% 292/337 [00:11<00:02, 19.04it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  87% 294/337 [00:11<00:02, 19.10it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  88% 296/337 [00:11<00:02, 18.43it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  89% 299/337 [00:12<00:01, 19.01it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  89% 301/337 [00:12<00:01, 18.06it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  90% 303/337 [00:12<00:01, 18.31it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  91% 305/337 [00:12<00:01, 17.66it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  91% 307/337 [00:12<00:01, 18.23it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  92% 309/337 [00:12<00:01, 16.09it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  92% 311/337 [00:12<00:01, 16.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  93% 313/337 [00:12<00:01, 16.02it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  93% 315/337 [00:13<00:01, 14.74it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  94% 317/337 [00:13<00:01, 15.53it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  95% 319/337 [00:13<00:01, 15.30it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  95% 321/337 [00:13<00:01, 14.80it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  96% 323/337 [00:13<00:00, 14.92it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  96% 325/337 [00:13<00:00, 14.56it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  97% 327/337 [00:13<00:00, 14.06it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  98% 329/337 [00:14<00:00, 14.05it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  98% 331/337 [00:14<00:00, 13.44it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  99% 333/337 [00:14<00:00, 12.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  99% 335/337 [00:14<00:00, 11.63it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset: 100% 337/337 [00:14<00:00, 12.94it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 11:11:42 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.318 | ppl 79.78 | wps 69023.7 | wpb 2965.1 | bsz 127.6 | num_updates 20080 | best_loss 5.709\n","2022-05-26 11:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20080 updates\n","2022-05-26 11:11:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_en_de/checkpoint_last.pt\n","2022-05-26 11:11:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_en_de/checkpoint_last.pt\n","2022-05-26 11:11:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_en_de/checkpoint_last.pt (epoch 13 @ 20080 updates, score 6.318) (writing took 3.5241496190010366 seconds)\n","2022-05-26 11:11:46 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2022-05-26 11:11:46 | INFO | train | epoch 013 | loss 2.272 | ppl 4.83 | wps 21609 | ups 7.36 | wpb 2935.7 | bsz 128 | num_updates 20080 | lr 0.0005 | gnorm 1.298 | loss_scale 16 | train_wall 180 | gb_free 11 | wall 215\n","2022-05-26 11:11:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","epoch 014:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 11:11:46 | INFO | fairseq.trainer | begin training epoch 14\n","2022-05-26 11:11:46 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 014:   2% 34/1563 [00:04<02:49,  9.02it/s, loss=2.214, ppl=4.64, wps=9354.9, ups=3.25, wpb=2874.6, bsz=128, num_updates=20100, lr=0.0005, gnorm=1.318, loss_scale=16, train_wall=12, gb_free=10.9, wall=218]2022-05-26 11:11:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.35 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:11:50 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 34           |        cudaMalloc retries: 39        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7523 MB |   10215 MB |    8796 GB |    8789 GB |\n","|       from large pool |    7512 MB |   10205 MB |    8624 GB |    8616 GB |\n","|       from small pool |      10 MB |      17 MB |     172 GB |     172 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7523 MB |   10215 MB |    8796 GB |    8789 GB |\n","|       from large pool |    7512 MB |   10205 MB |    8624 GB |    8616 GB |\n","|       from small pool |      10 MB |      17 MB |     172 GB |     172 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12620 MB |   41742 MB |   29220 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     110 MB |    3222 MB |    3210 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4998 MB |    4998 MB |   18789 GB |   18784 GB |\n","|       from large pool |    4997 MB |    4997 MB |   18591 GB |   18586 GB |\n","|       from small pool |       1 MB |      15 MB |     198 GB |     198 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1332 K  |    1332 K  |\n","|       from large pool |     142    |     147    |     580 K  |     580 K  |\n","|       from small pool |     184    |     268    |     752 K  |     752 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1332 K  |    1332 K  |\n","|       from large pool |     142    |     147    |     580 K  |     580 K  |\n","|       from small pool |     184    |     268    |     752 K  |     752 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      71    |    1637    |    1615    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      55    |    1611    |    1605    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      36    |  688961    |  688937    |\n","|       from large pool |      16    |      17    |  363229    |  363213    |\n","|       from small pool |       8    |      28    |  325732    |  325724    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:11:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  14% 226/1563 [00:27<02:36,  8.53it/s, loss=1.963, ppl=3.9, wps=24465.2, ups=8.3, wpb=2948.1, bsz=128, num_updates=20300, lr=0.0005, gnorm=1.235, loss_scale=16, train_wall=12, gb_free=11.7, wall=242]2022-05-26 11:12:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.76 GiB total capacity; 5.91 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  15% 227/1563 [00:27<02:39,  8.40it/s, loss=1.963, ppl=3.9, wps=24465.2, ups=8.3, wpb=2948.1, bsz=128, num_updates=20300, lr=0.0005, gnorm=1.235, loss_scale=16, train_wall=12, gb_free=11.7, wall=242]2022-05-26 11:12:13 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 35           |        cudaMalloc retries: 40        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6047 MB |    8085 MB |    9751 GB |    9745 GB |\n","|       from large pool |    6037 MB |    8075 MB |    9559 GB |    9553 GB |\n","|       from small pool |       9 MB |      17 MB |     191 GB |     191 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6047 MB |    8085 MB |    9751 GB |    9745 GB |\n","|       from large pool |    6037 MB |    8075 MB |    9559 GB |    9553 GB |\n","|       from small pool |       9 MB |      17 MB |     191 GB |     191 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12524 MB |   12624 MB |   41844 MB |   29320 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      14 MB |     114 MB |    3324 MB |    3310 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6476 MB |    6476 MB |   20915 GB |   20908 GB |\n","|       from large pool |    6472 MB |    6472 MB |   20694 GB |   20688 GB |\n","|       from small pool |       4 MB |      14 MB |     220 GB |     220 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1484 K  |    1484 K  |\n","|       from large pool |     142    |     147    |     647 K  |     647 K  |\n","|       from small pool |     184    |     268    |     837 K  |     837 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1484 K  |    1484 K  |\n","|       from large pool |     142    |     147    |     647 K  |     647 K  |\n","|       from small pool |     184    |     268    |     837 K  |     837 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      73    |    1688    |    1665    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       7    |      57    |    1662    |    1655    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      38    |     768 K  |     768 K  |\n","|       from large pool |      13    |      14    |     405 K  |     405 K  |\n","|       from small pool |      11    |      30    |     363 K  |     363 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  16% 243/1563 [00:29<02:53,  7.62it/s, loss=1.963, ppl=3.9, wps=24465.2, ups=8.3, wpb=2948.1, bsz=128, num_updates=20300, lr=0.0005, gnorm=1.235, loss_scale=16, train_wall=12, gb_free=11.7, wall=242]2022-05-26 11:12:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.25 GiB (GPU 0; 14.76 GiB total capacity; 6.16 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:15 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 36           |        cudaMalloc retries: 41        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6312 MB |    8501 MB |    9839 GB |    9833 GB |\n","|       from large pool |    6302 MB |    8490 MB |    9645 GB |    9639 GB |\n","|       from small pool |       9 MB |      17 MB |     193 GB |     193 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6312 MB |    8501 MB |    9839 GB |    9833 GB |\n","|       from large pool |    6302 MB |    8490 MB |    9645 GB |    9639 GB |\n","|       from small pool |       9 MB |      17 MB |     193 GB |     193 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12618 MB |   41938 MB |   29416 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     108 MB |    3418 MB |    3406 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6209 MB |    6209 MB |   21107 GB |   21101 GB |\n","|       from large pool |    6207 MB |    6207 MB |   20884 GB |   20878 GB |\n","|       from small pool |       2 MB |       3 MB |     222 GB |     222 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1497 K  |    1497 K  |\n","|       from large pool |     142    |     146    |     652 K  |     652 K  |\n","|       from small pool |     184    |     268    |     845 K  |     845 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1497 K  |    1497 K  |\n","|       from large pool |     142    |     146    |     652 K  |     652 K  |\n","|       from small pool |     184    |     268    |     845 K  |     845 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      70    |    1735    |    1713    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      54    |    1709    |    1703    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      26    |     775 K  |     775 K  |\n","|       from large pool |      16    |      17    |     408 K  |     408 K  |\n","|       from small pool |       9    |      16    |     367 K  |     367 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  17% 263/1563 [00:32<02:52,  7.52it/s, loss=1.963, ppl=3.9, wps=24465.2, ups=8.3, wpb=2948.1, bsz=128, num_updates=20300, lr=0.0005, gnorm=1.235, loss_scale=16, train_wall=12, gb_free=11.7, wall=242]2022-05-26 11:12:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.44 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:18 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 37           |        cudaMalloc retries: 42        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7617 MB |   10284 MB |    9941 GB |    9933 GB |\n","|       from large pool |    7606 MB |   10274 MB |    9745 GB |    9738 GB |\n","|       from small pool |      10 MB |      17 MB |     195 GB |     195 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7617 MB |   10284 MB |    9941 GB |    9933 GB |\n","|       from large pool |    7606 MB |   10274 MB |    9745 GB |    9738 GB |\n","|       from small pool |      10 MB |      17 MB |     195 GB |     195 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12612 MB |   42028 MB |   29506 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     102 MB |    3508 MB |    3496 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4904 MB |    4904 MB |   21325 GB |   21321 GB |\n","|       from large pool |    4903 MB |    4903 MB |   21101 GB |   21096 GB |\n","|       from small pool |       1 MB |       3 MB |     224 GB |     224 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1512 K  |    1512 K  |\n","|       from large pool |     142    |     147    |     659 K  |     658 K  |\n","|       from small pool |     184    |     268    |     853 K  |     853 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1512 K  |    1512 K  |\n","|       from large pool |     142    |     147    |     659 K  |     658 K  |\n","|       from small pool |     184    |     268    |     853 K  |     853 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      67    |    1780    |    1758    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      51    |    1754    |    1748    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      32    |     783 K  |     783 K  |\n","|       from large pool |      22    |      23    |     412 K  |     412 K  |\n","|       from small pool |       9    |      16    |     370 K  |     370 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  18% 280/1563 [00:34<02:33,  8.33it/s, loss=1.963, ppl=3.9, wps=24465.2, ups=8.3, wpb=2948.1, bsz=128, num_updates=20300, lr=0.0005, gnorm=1.235, loss_scale=16, train_wall=12, gb_free=11.7, wall=242]2022-05-26 11:12:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 14.76 GiB total capacity; 6.05 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 38           |        cudaMalloc retries: 43        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6197 MB |    8260 MB |   10038 GB |   10032 GB |\n","|       from large pool |    6186 MB |    8250 MB |    9842 GB |    9836 GB |\n","|       from small pool |      10 MB |      17 MB |     196 GB |     196 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6197 MB |    8260 MB |   10038 GB |   10032 GB |\n","|       from large pool |    6186 MB |    8250 MB |    9842 GB |    9836 GB |\n","|       from small pool |      10 MB |      17 MB |     196 GB |     196 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12618 MB |   42124 MB |   29602 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     108 MB |    3604 MB |    3592 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6324 MB |    6324 MB |   21535 GB |   21528 GB |\n","|       from large pool |    6323 MB |    6323 MB |   21309 GB |   21302 GB |\n","|       from small pool |       1 MB |      19 MB |     225 GB |     225 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1526 K  |    1526 K  |\n","|       from large pool |     142    |     147    |     666 K  |     666 K  |\n","|       from small pool |     184    |     268    |     859 K  |     859 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1526 K  |    1526 K  |\n","|       from large pool |     142    |     147    |     666 K  |     666 K  |\n","|       from small pool |     184    |     268    |     859 K  |     859 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      70    |    1828    |    1806    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      54    |    1802    |    1796    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      42    |     790 K  |     790 K  |\n","|       from large pool |      20    |      21    |     417 K  |     417 K  |\n","|       from small pool |       7    |      34    |     373 K  |     373 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  21% 324/1563 [00:39<02:53,  7.14it/s, loss=1.963, ppl=3.9, wps=24465.2, ups=8.3, wpb=2948.1, bsz=128, num_updates=20300, lr=0.0005, gnorm=1.235, loss_scale=16, train_wall=12, gb_free=11.7, wall=242]2022-05-26 11:12:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.76 GiB total capacity; 5.93 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:25 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 39           |        cudaMalloc retries: 44        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6076 MB |    8114 MB |   10242 GB |   10236 GB |\n","|       from large pool |    6066 MB |    8104 MB |   10040 GB |   10034 GB |\n","|       from small pool |       9 MB |      17 MB |     201 GB |     201 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6076 MB |    8114 MB |   10242 GB |   10236 GB |\n","|       from large pool |    6066 MB |    8104 MB |   10040 GB |   10034 GB |\n","|       from small pool |       9 MB |      17 MB |     201 GB |     201 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12622 MB |   42224 MB |   29702 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     112 MB |    3704 MB |    3692 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6445 MB |    6445 MB |   22017 GB |   22011 GB |\n","|       from large pool |    6443 MB |    6443 MB |   21785 GB |   21779 GB |\n","|       from small pool |       2 MB |       5 MB |     232 GB |     232 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1560 K  |    1560 K  |\n","|       from large pool |     142    |     147    |     679 K  |     679 K  |\n","|       from small pool |     184    |     268    |     881 K  |     881 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1560 K  |    1560 K  |\n","|       from large pool |     142    |     147    |     679 K  |     679 K  |\n","|       from small pool |     184    |     268    |     881 K  |     881 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |    1878    |    1856    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      56    |    1852    |    1846    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      31    |     808 K  |     808 K  |\n","|       from large pool |      21    |      22    |     424 K  |     424 K  |\n","|       from small pool |       9    |      18    |     383 K  |     383 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  22% 339/1563 [00:41<02:33,  7.96it/s, loss=2.078, ppl=4.22, wps=23263.9, ups=7.86, wpb=2958.9, bsz=128, num_updates=20400, lr=0.0005, gnorm=1.251, loss_scale=16, train_wall=12, gb_free=6.8, wall=254]2022-05-26 11:12:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.02 GiB already allocated; 1.28 GiB free; 12.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:27 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 40           |        cudaMalloc retries: 45        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6168 MB |    8331 MB |   10318 GB |   10312 GB |\n","|       from large pool |    6150 MB |    8313 MB |   10115 GB |   10109 GB |\n","|       from small pool |      17 MB |      17 MB |     202 GB |     202 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6168 MB |    8331 MB |   10318 GB |   10312 GB |\n","|       from large pool |    6150 MB |    8313 MB |   10115 GB |   10109 GB |\n","|       from small pool |      17 MB |      17 MB |     202 GB |     202 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12530 MB |   12624 MB |   42326 MB |   29796 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      20 MB |     114 MB |    3806 MB |    3786 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6361 MB |    6361 MB |   22188 GB |   22182 GB |\n","|       from large pool |    6359 MB |    6359 MB |   21955 GB |   21949 GB |\n","|       from small pool |       2 MB |       3 MB |     233 GB |     233 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1572 K  |    1572 K  |\n","|       from large pool |     132    |     136    |     685 K  |     685 K  |\n","|       from small pool |     194    |     268    |     886 K  |     886 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1572 K  |    1572 K  |\n","|       from large pool |     132    |     136    |     685 K  |     685 K  |\n","|       from small pool |     194    |     268    |     886 K  |     886 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      73    |    1929    |    1903    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |      10    |      57    |    1903    |    1893    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      31    |     814 K  |     814 K  |\n","|       from large pool |      17    |      18    |     428 K  |     428 K  |\n","|       from small pool |      13    |      16    |     385 K  |     385 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  22% 346/1563 [00:42<02:39,  7.61it/s, loss=2.078, ppl=4.22, wps=23263.9, ups=7.86, wpb=2958.9, bsz=128, num_updates=20400, lr=0.0005, gnorm=1.251, loss_scale=16, train_wall=12, gb_free=6.8, wall=254]2022-05-26 11:12:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.11 GiB (GPU 0; 14.76 GiB total capacity; 6.03 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:28 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 41           |        cudaMalloc retries: 46        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6173 MB |    8287 MB |   10356 GB |   10350 GB |\n","|       from large pool |    6163 MB |    8276 MB |   10152 GB |   10146 GB |\n","|       from small pool |       9 MB |      17 MB |     203 GB |     203 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6173 MB |    8287 MB |   10356 GB |   10350 GB |\n","|       from large pool |    6163 MB |    8276 MB |   10152 GB |   10146 GB |\n","|       from small pool |       9 MB |      17 MB |     203 GB |     203 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12560 MB |   42356 MB |   29834 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |      50 MB |    3836 MB |    3824 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6348 MB |    6348 MB |   22268 GB |   22261 GB |\n","|       from large pool |    6346 MB |    6346 MB |   22034 GB |   22027 GB |\n","|       from small pool |       2 MB |       3 MB |     233 GB |     233 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1577 K  |    1577 K  |\n","|       from large pool |     142    |     146    |     688 K  |     687 K  |\n","|       from small pool |     184    |     268    |     889 K  |     889 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1577 K  |    1577 K  |\n","|       from large pool |     142    |     146    |     688 K  |     687 K  |\n","|       from small pool |     184    |     268    |     889 K  |     889 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      41    |    1944    |    1922    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      25    |    1918    |    1912    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      26    |     816 K  |     816 K  |\n","|       from large pool |      16    |      17    |     430 K  |     430 K  |\n","|       from small pool |       8    |      16    |     386 K  |     386 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 014:  22% 347/1563 [00:42<02:35,  7.81it/s, loss=2.078, ppl=4.22, wps=23263.9, ups=7.86, wpb=2958.9, bsz=128, num_updates=20400, lr=0.0005, gnorm=1.251, loss_scale=16, train_wall=12, gb_free=6.8, wall=254]2022-05-26 11:12:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  22% 351/1563 [00:42<02:48,  7.20it/s, loss=2.078, ppl=4.22, wps=23263.9, ups=7.86, wpb=2958.9, bsz=128, num_updates=20400, lr=0.0005, gnorm=1.251, loss_scale=16, train_wall=12, gb_free=6.8, wall=254]2022-05-26 11:12:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.96 GiB already allocated; 1.28 GiB free; 12.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:29 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 42           |        cudaMalloc retries: 47        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7122 MB |    9689 MB |   10389 GB |   10382 GB |\n","|       from large pool |    7102 MB |    9668 MB |   10185 GB |   10178 GB |\n","|       from small pool |      20 MB |      20 MB |     203 GB |     203 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7122 MB |    9689 MB |   10389 GB |   10382 GB |\n","|       from large pool |    7102 MB |    9668 MB |   10185 GB |   10178 GB |\n","|       from small pool |      20 MB |      20 MB |     203 GB |     203 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12532 MB |   12602 MB |   42436 MB |   29904 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      22 MB |      92 MB |    3916 MB |    3894 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5409 MB |    5409 MB |   22331 GB |   22326 GB |\n","|       from large pool |    5407 MB |    5407 MB |   22097 GB |   22092 GB |\n","|       from small pool |       1 MB |       3 MB |     234 GB |     234 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1580 K  |    1580 K  |\n","|       from large pool |     132    |     136    |     689 K  |     689 K  |\n","|       from small pool |     194    |     268    |     891 K  |     890 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1580 K  |    1580 K  |\n","|       from large pool |     132    |     136    |     689 K  |     689 K  |\n","|       from small pool |     194    |     268    |     891 K  |     890 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      27    |      62    |    1984    |    1957    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |      11    |      46    |    1958    |    1947    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      29    |     818 K  |     818 K  |\n","|       from large pool |      13    |      14    |     431 K  |     431 K  |\n","|       from small pool |       9    |      20    |     387 K  |     387 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  26% 407/1563 [00:49<02:31,  7.62it/s, loss=2.078, ppl=4.22, wps=23263.9, ups=7.86, wpb=2958.9, bsz=128, num_updates=20400, lr=0.0005, gnorm=1.251, loss_scale=16, train_wall=12, gb_free=6.8, wall=254]2022-05-26 11:12:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 7.05 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 43           |        cudaMalloc retries: 48        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7217 MB |    9784 MB |   10677 GB |   10670 GB |\n","|       from large pool |    7207 MB |    9773 MB |   10468 GB |   10461 GB |\n","|       from small pool |      10 MB |      17 MB |     209 GB |     209 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7217 MB |    9784 MB |   10677 GB |   10670 GB |\n","|       from large pool |    7207 MB |    9773 MB |   10468 GB |   10461 GB |\n","|       from small pool |      10 MB |      17 MB |     209 GB |     209 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12620 MB |   42524 MB |   30002 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     110 MB |    4004 MB |    3992 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5304 MB |    5304 MB |   22965 GB |   22960 GB |\n","|       from large pool |    5302 MB |    5302 MB |   22724 GB |   22719 GB |\n","|       from small pool |       1 MB |      12 MB |     240 GB |     240 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1625 K  |    1624 K  |\n","|       from large pool |     142    |     147    |     708 K  |     708 K  |\n","|       from small pool |     184    |     268    |     916 K  |     915 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1625 K  |    1624 K  |\n","|       from large pool |     142    |     147    |     708 K  |     708 K  |\n","|       from small pool |     184    |     268    |     916 K  |     915 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      71    |    2028    |    2006    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      55    |    2002    |    1996    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      36    |     841 K  |     841 K  |\n","|       from large pool |      14    |      15    |     443 K  |     443 K  |\n","|       from small pool |      10    |      28    |     398 K  |     398 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  28% 431/1563 [00:52<02:00,  9.43it/s, loss=2.119, ppl=4.35, wps=23534.9, ups=7.85, wpb=2997.7, bsz=128, num_updates=20500, lr=0.0005, gnorm=1.304, loss_scale=16, train_wall=12, gb_free=11.5, wall=267]2022-05-26 11:12:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 7.03 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:38 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 44           |        cudaMalloc retries: 49        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7194 MB |    9710 MB |   10791 GB |   10784 GB |\n","|       from large pool |    7184 MB |    9700 MB |   10578 GB |   10571 GB |\n","|       from small pool |      10 MB |      17 MB |     212 GB |     212 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7194 MB |    9710 MB |   10791 GB |   10784 GB |\n","|       from large pool |    7184 MB |    9700 MB |   10578 GB |   10571 GB |\n","|       from small pool |      10 MB |      17 MB |     212 GB |     212 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12524 MB |   12624 MB |   42626 MB |   30102 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      14 MB |     114 MB |    4106 MB |    4092 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5329 MB |    5329 MB |   23225 GB |   23219 GB |\n","|       from large pool |    5325 MB |    5325 MB |   22981 GB |   22975 GB |\n","|       from small pool |       3 MB |      16 MB |     244 GB |     244 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1643 K  |    1643 K  |\n","|       from large pool |     142    |     147    |     716 K  |     716 K  |\n","|       from small pool |     184    |     267    |     927 K  |     927 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1643 K  |    1643 K  |\n","|       from large pool |     142    |     147    |     716 K  |     716 K  |\n","|       from small pool |     184    |     267    |     927 K  |     927 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      73    |    2079    |    2056    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       7    |      57    |    2053    |    2046    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      32    |      43    |     851 K  |     851 K  |\n","|       from large pool |      21    |      22    |     447 K  |     447 K  |\n","|       from small pool |      11    |      35    |     403 K  |     403 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  28% 441/1563 [00:53<02:01,  9.22it/s, loss=2.119, ppl=4.35, wps=23534.9, ups=7.85, wpb=2997.7, bsz=128, num_updates=20500, lr=0.0005, gnorm=1.304, loss_scale=16, train_wall=12, gb_free=11.5, wall=267]2022-05-26 11:12:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.91 GiB (GPU 0; 14.76 GiB total capacity; 5.76 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:39 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 45           |        cudaMalloc retries: 50        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5896 MB |    7909 MB |   10838 GB |   10832 GB |\n","|       from large pool |    5886 MB |    7898 MB |   10624 GB |   10618 GB |\n","|       from small pool |       9 MB |      17 MB |     213 GB |     213 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5896 MB |    7909 MB |   10838 GB |   10832 GB |\n","|       from large pool |    5886 MB |    7898 MB |   10624 GB |   10618 GB |\n","|       from small pool |       9 MB |      17 MB |     213 GB |     213 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12624 MB |   42726 MB |   30204 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     114 MB |    4206 MB |    4194 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6625 MB |    6625 MB |   23322 GB |   23316 GB |\n","|       from large pool |    6623 MB |    6623 MB |   23076 GB |   23069 GB |\n","|       from small pool |       2 MB |      44 MB |     246 GB |     246 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1650 K  |    1650 K  |\n","|       from large pool |     142    |     146    |     718 K  |     718 K  |\n","|       from small pool |     184    |     268    |     932 K  |     932 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1650 K  |    1650 K  |\n","|       from large pool |     142    |     146    |     718 K  |     718 K  |\n","|       from small pool |     184    |     268    |     932 K  |     932 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2129    |    2107    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      57    |    2103    |    2097    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      53    |     855 K  |     855 K  |\n","|       from large pool |      12    |      13    |     449 K  |     449 K  |\n","|       from small pool |       9    |      45    |     406 K  |     406 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  32% 496/1563 [01:00<02:45,  6.46it/s, loss=2.119, ppl=4.35, wps=23534.9, ups=7.85, wpb=2997.7, bsz=128, num_updates=20500, lr=0.0005, gnorm=1.304, loss_scale=16, train_wall=12, gb_free=11.5, wall=267]2022-05-26 11:12:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.06 GiB (GPU 0; 14.76 GiB total capacity; 6.03 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:46 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 46           |        cudaMalloc retries: 51        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6178 MB |    8267 MB |   11126 GB |   11120 GB |\n","|       from large pool |    6168 MB |    8257 MB |   10906 GB |   10900 GB |\n","|       from small pool |       9 MB |      17 MB |     219 GB |     219 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6178 MB |    8267 MB |   11126 GB |   11120 GB |\n","|       from large pool |    6168 MB |    8257 MB |   10906 GB |   10900 GB |\n","|       from small pool |       9 MB |      17 MB |     219 GB |     219 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12522 MB |   12624 MB |   42828 MB |   30306 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      12 MB |     114 MB |    4308 MB |    4296 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6343 MB |    6994 MB |   23955 GB |   23949 GB |\n","|       from large pool |    6341 MB |    6991 MB |   23702 GB |   23696 GB |\n","|       from small pool |       2 MB |       3 MB |     253 GB |     253 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     325    |     328    |    1693 K  |    1693 K  |\n","|       from large pool |     142    |     147    |     736 K  |     736 K  |\n","|       from small pool |     183    |     268    |     957 K  |     957 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     325    |     328    |    1693 K  |    1693 K  |\n","|       from large pool |     142    |     147    |     736 K  |     736 K  |\n","|       from small pool |     183    |     268    |     957 K  |     957 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2180    |    2158    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       6    |      57    |    2154    |    2148    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      28    |     877 K  |     877 K  |\n","|       from large pool |      17    |      18    |     460 K  |     460 K  |\n","|       from small pool |       7    |      18    |     417 K  |     417 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 014:  32% 497/1563 [01:00<02:37,  6.77it/s, loss=2.119, ppl=4.35, wps=23534.9, ups=7.85, wpb=2997.7, bsz=128, num_updates=20500, lr=0.0005, gnorm=1.304, loss_scale=16, train_wall=12, gb_free=11.5, wall=267]2022-05-26 11:12:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  35% 554/1563 [01:07<02:06,  7.95it/s, loss=2.168, ppl=4.5, wps=23760.9, ups=7.71, wpb=3081.4, bsz=128, num_updates=20600, lr=0.0005, gnorm=1.259, loss_scale=16, train_wall=12, gb_free=8.9, wall=280]2022-05-26 11:12:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.19 GiB already allocated; 1.29 GiB free; 12.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:54 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 47           |        cudaMalloc retries: 52        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6337 MB |    8501 MB |   11428 GB |   11421 GB |\n","|       from large pool |    6327 MB |    8491 MB |   11202 GB |   11196 GB |\n","|       from small pool |      10 MB |      17 MB |     225 GB |     225 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6337 MB |    8501 MB |   11428 GB |   11421 GB |\n","|       from large pool |    6327 MB |    8491 MB |   11202 GB |   11196 GB |\n","|       from small pool |      10 MB |      17 MB |     225 GB |     225 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12524 MB |   12624 MB |   42930 MB |   30406 MB |\n","|       from large pool |   12510 MB |   12510 MB |   38520 MB |   26010 MB |\n","|       from small pool |      14 MB |     114 MB |    4410 MB |    4396 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6186 MB |    6186 MB |   24630 GB |   24624 GB |\n","|       from large pool |    6182 MB |    6182 MB |   24370 GB |   24364 GB |\n","|       from small pool |       3 MB |      29 MB |     259 GB |     259 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1740 K  |    1740 K  |\n","|       from large pool |     142    |     147    |     757 K  |     757 K  |\n","|       from small pool |     184    |     268    |     983 K  |     982 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1740 K  |    1740 K  |\n","|       from large pool |     142    |     147    |     757 K  |     757 K  |\n","|       from small pool |     184    |     268    |     983 K  |     982 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      73    |    2231    |    2208    |\n","|       from large pool |      16    |      16    |      26    |      10    |\n","|       from small pool |       7    |      57    |    2205    |    2198    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      57    |     901 K  |     901 K  |\n","|       from large pool |      17    |      18    |     472 K  |     472 K  |\n","|       from small pool |      12    |      49    |     428 K  |     428 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  37% 581/1563 [01:10<01:36, 10.22it/s, loss=2.168, ppl=4.5, wps=23760.9, ups=7.71, wpb=3081.4, bsz=128, num_updates=20600, lr=0.0005, gnorm=1.259, loss_scale=16, train_wall=12, gb_free=8.9, wall=280]2022-05-26 11:12:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.91 GiB already allocated; 1.09 GiB free; 12.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:12:57 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 48           |        cudaMalloc retries: 54        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9123 MB |   12570 MB |   11542 GB |   11533 GB |\n","|       from large pool |    9105 MB |   12550 MB |   11313 GB |   11304 GB |\n","|       from small pool |      18 MB |      19 MB |     229 GB |     229 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9123 MB |   12570 MB |   11542 GB |   11533 GB |\n","|       from large pool |    9105 MB |   12550 MB |   11313 GB |   11304 GB |\n","|       from small pool |      18 MB |      19 MB |     229 GB |     229 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12730 MB |   12730 MB |   49888 MB |   37158 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      20 MB |     114 MB |    4510 MB |    4490 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    3606 MB |    4357 MB |   24892 GB |   24888 GB |\n","|       from large pool |    3604 MB |    4339 MB |   24628 GB |   24625 GB |\n","|       from small pool |       1 MB |      17 MB |     263 GB |     263 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1760 K  |    1759 K  |\n","|       from large pool |     132    |     136    |     764 K  |     764 K  |\n","|       from small pool |     194    |     268    |     995 K  |     995 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1760 K  |    1759 K  |\n","|       from large pool |     132    |     136    |     764 K  |     764 K  |\n","|       from small pool |     194    |     268    |     995 K  |     995 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      73    |    2282    |    2256    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |      10    |      57    |    2255    |    2245    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      35    |     911 K  |     911 K  |\n","|       from large pool |      18    |      19    |     477 K  |     477 K  |\n","|       from small pool |      10    |      27    |     434 K  |     434 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:12:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  44% 692/1563 [01:24<01:36,  9.01it/s, loss=1.98, ppl=3.95, wps=23151.5, ups=8.25, wpb=2804.9, bsz=128, num_updates=20700, lr=0.0005, gnorm=1.279, loss_scale=16, train_wall=11, gb_free=12.1, wall=292]2022-05-26 11:13:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.76 GiB (GPU 0; 14.76 GiB total capacity; 5.74 GiB already allocated; 1.10 GiB free; 12.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:10 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 49           |        cudaMalloc retries: 55        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5880 MB |    7818 MB |   12102 GB |   12096 GB |\n","|       from large pool |    5870 MB |    7808 MB |   11860 GB |   11854 GB |\n","|       from small pool |       9 MB |      17 MB |     241 GB |     241 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5880 MB |    7818 MB |   12102 GB |   12096 GB |\n","|       from large pool |    5870 MB |    7808 MB |   11860 GB |   11854 GB |\n","|       from small pool |       9 MB |      17 MB |     241 GB |     241 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12722 MB |   12824 MB |   49982 MB |   37260 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      12 MB |     114 MB |    4604 MB |    4592 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6841 MB |    6841 MB |   26172 GB |   26165 GB |\n","|       from large pool |    6839 MB |    6839 MB |   25894 GB |   25888 GB |\n","|       from small pool |       2 MB |      44 MB |     277 GB |     277 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1849 K  |    1848 K  |\n","|       from large pool |     142    |     147    |     803 K  |     802 K  |\n","|       from small pool |     184    |     268    |    1045 K  |    1045 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1849 K  |    1848 K  |\n","|       from large pool |     142    |     147    |     803 K  |     802 K  |\n","|       from small pool |     184    |     268    |    1045 K  |    1045 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2329    |    2307    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |       6    |      57    |    2302    |    2296    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      53    |     958 K  |     958 K  |\n","|       from large pool |      20    |      21    |     501 K  |     501 K  |\n","|       from small pool |       8    |      45    |     456 K  |     456 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  48% 744/1563 [01:30<01:45,  7.74it/s, loss=2.214, ppl=4.64, wps=23240.7, ups=8.05, wpb=2887, bsz=128, num_updates=20800, lr=0.0005, gnorm=1.342, loss_scale=16, train_wall=12, gb_free=10.4, wall=305]2022-05-26 11:13:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.88 GiB already allocated; 1.09 GiB free; 12.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:17 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 50           |        cudaMalloc retries: 56        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7048 MB |    9439 MB |   12363 GB |   12356 GB |\n","|       from large pool |    7037 MB |    9429 MB |   12116 GB |   12109 GB |\n","|       from small pool |      10 MB |      17 MB |     246 GB |     246 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7048 MB |    9439 MB |   12363 GB |   12356 GB |\n","|       from large pool |    7037 MB |    9429 MB |   12116 GB |   12109 GB |\n","|       from small pool |      10 MB |      17 MB |     246 GB |     246 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12726 MB |   12824 MB |   50084 MB |   37358 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      16 MB |     114 MB |    4706 MB |    4690 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5677 MB |    5677 MB |   26748 GB |   26743 GB |\n","|       from large pool |    5672 MB |    5672 MB |   26465 GB |   26459 GB |\n","|       from small pool |       5 MB |      11 MB |     283 GB |     283 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1888 K  |    1888 K  |\n","|       from large pool |     142    |     147    |     820 K  |     819 K  |\n","|       from small pool |     184    |     268    |    1068 K  |    1068 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1888 K  |    1888 K  |\n","|       from large pool |     142    |     147    |     820 K  |     819 K  |\n","|       from small pool |     184    |     268    |    1068 K  |    1068 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      24    |      73    |    2380    |    2356    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |       8    |      57    |    2353    |    2345    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      38    |     979 K  |     979 K  |\n","|       from large pool |      21    |      22    |     512 K  |     511 K  |\n","|       from small pool |      10    |      30    |     467 K  |     467 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  48% 749/1563 [01:31<01:37,  8.36it/s, loss=2.214, ppl=4.64, wps=23240.7, ups=8.05, wpb=2887, bsz=128, num_updates=20800, lr=0.0005, gnorm=1.342, loss_scale=16, train_wall=12, gb_free=10.4, wall=305]2022-05-26 11:13:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.86 GiB already allocated; 1.09 GiB free; 12.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:17 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 51           |        cudaMalloc retries: 57        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9069 MB |   12417 MB |   12391 GB |   12382 GB |\n","|       from large pool |    9059 MB |   12406 MB |   12143 GB |   12134 GB |\n","|       from small pool |      10 MB |      17 MB |     247 GB |     247 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9069 MB |   12417 MB |   12391 GB |   12382 GB |\n","|       from large pool |    9059 MB |   12406 MB |   12143 GB |   12134 GB |\n","|       from small pool |      10 MB |      17 MB |     247 GB |     247 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12724 MB |   12790 MB |   50148 MB |   37424 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      14 MB |      80 MB |    4770 MB |    4756 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    3654 MB |    4014 MB |   26797 GB |   26793 GB |\n","|       from large pool |    3650 MB |    4010 MB |   26512 GB |   26509 GB |\n","|       from small pool |       3 MB |      14 MB |     284 GB |     284 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1892 K  |    1891 K  |\n","|       from large pool |     142    |     147    |     821 K  |     821 K  |\n","|       from small pool |     184    |     268    |    1070 K  |    1070 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1892 K  |    1891 K  |\n","|       from large pool |     142    |     147    |     821 K  |     821 K  |\n","|       from small pool |     184    |     268    |    1070 K  |    1070 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      56    |    2412    |    2389    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |       7    |      40    |    2385    |    2378    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      42    |     980 K  |     980 K  |\n","|       from large pool |      18    |      19    |     512 K  |     512 K  |\n","|       from small pool |       9    |      34    |     468 K  |     468 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  49% 764/1563 [01:33<01:28,  8.99it/s, loss=2.214, ppl=4.64, wps=23240.7, ups=8.05, wpb=2887, bsz=128, num_updates=20800, lr=0.0005, gnorm=1.342, loss_scale=16, train_wall=12, gb_free=10.4, wall=305]2022-05-26 11:13:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.96 GiB (GPU 0; 14.76 GiB total capacity; 5.85 GiB already allocated; 1.10 GiB free; 12.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  49% 765/1563 [01:33<01:32,  8.66it/s, loss=2.214, ppl=4.64, wps=23240.7, ups=8.05, wpb=2887, bsz=128, num_updates=20800, lr=0.0005, gnorm=1.342, loss_scale=16, train_wall=12, gb_free=10.4, wall=305]2022-05-26 11:13:19 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 52           |        cudaMalloc retries: 58        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5993 MB |    8031 MB |   12461 GB |   12455 GB |\n","|       from large pool |    5983 MB |    8020 MB |   12212 GB |   12206 GB |\n","|       from small pool |       9 MB |      17 MB |     248 GB |     248 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5993 MB |    8031 MB |   12461 GB |   12455 GB |\n","|       from large pool |    5983 MB |    8020 MB |   12212 GB |   12206 GB |\n","|       from small pool |       9 MB |      17 MB |     248 GB |     248 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12722 MB |   12824 MB |   50248 MB |   37526 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      12 MB |     114 MB |    4870 MB |    4858 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6728 MB |    6728 MB |   26957 GB |   26950 GB |\n","|       from large pool |    6726 MB |    6726 MB |   26671 GB |   26664 GB |\n","|       from small pool |       2 MB |      52 MB |     286 GB |     286 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1903 K  |    1903 K  |\n","|       from large pool |     142    |     146    |     826 K  |     826 K  |\n","|       from small pool |     184    |     268    |    1077 K  |    1076 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1903 K  |    1903 K  |\n","|       from large pool |     142    |     146    |     826 K  |     826 K  |\n","|       from small pool |     184    |     268    |    1077 K  |    1076 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2462    |    2440    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |       6    |      57    |    2435    |    2429    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      64    |     986 K  |     986 K  |\n","|       from large pool |      17    |      18    |     516 K  |     516 K  |\n","|       from small pool |       8    |      57    |     470 K  |     470 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  52% 816/1563 [01:39<01:29,  8.39it/s, loss=2.214, ppl=4.64, wps=23240.7, ups=8.05, wpb=2887, bsz=128, num_updates=20800, lr=0.0005, gnorm=1.342, loss_scale=16, train_wall=12, gb_free=10.4, wall=305]2022-05-26 11:13:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.04 GiB (GPU 0; 14.76 GiB total capacity; 7.08 GiB already allocated; 1.09 GiB free; 12.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:26 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 53           |        cudaMalloc retries: 59        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7246 MB |    9838 MB |   12732 GB |   12725 GB |\n","|       from large pool |    7236 MB |    9828 MB |   12478 GB |   12471 GB |\n","|       from small pool |      10 MB |      17 MB |     254 GB |     254 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7246 MB |    9838 MB |   12732 GB |   12725 GB |\n","|       from large pool |    7236 MB |    9828 MB |   12478 GB |   12471 GB |\n","|       from small pool |      10 MB |      17 MB |     254 GB |     254 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12726 MB |   12822 MB |   50348 MB |   37622 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      16 MB |     112 MB |    4970 MB |    4954 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5479 MB |    5479 MB |   27558 GB |   27553 GB |\n","|       from large pool |    5473 MB |    5473 MB |   27266 GB |   27261 GB |\n","|       from small pool |       5 MB |      30 MB |     292 GB |     292 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1944 K  |    1944 K  |\n","|       from large pool |     142    |     147    |     844 K  |     844 K  |\n","|       from small pool |     184    |     268    |    1100 K  |    1099 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1944 K  |    1944 K  |\n","|       from large pool |     142    |     147    |     844 K  |     844 K  |\n","|       from small pool |     184    |     268    |    1100 K  |    1099 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      24    |      72    |    2512    |    2488    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |       8    |      56    |    2485    |    2477    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      62    |    1008 K  |    1008 K  |\n","|       from large pool |      16    |      17    |     527 K  |     527 K  |\n","|       from small pool |       9    |      54    |     481 K  |     481 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 014:  52% 817/1563 [01:39<01:32,  8.03it/s, loss=2.214, ppl=4.64, wps=23240.7, ups=8.05, wpb=2887, bsz=128, num_updates=20800, lr=0.0005, gnorm=1.342, loss_scale=16, train_wall=12, gb_free=10.4, wall=305]2022-05-26 11:13:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  56% 870/1563 [01:46<01:27,  7.90it/s, loss=2.196, ppl=4.58, wps=22810.6, ups=7.83, wpb=2914.5, bsz=128, num_updates=20900, lr=0.0005, gnorm=1.331, loss_scale=16, train_wall=12, gb_free=12.3, wall=317]2022-05-26 11:13:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.01 GiB (GPU 0; 14.76 GiB total capacity; 5.82 GiB already allocated; 1.09 GiB free; 12.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:32 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 54           |        cudaMalloc retries: 60        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    5954 MB |    8017 MB |   12992 GB |   12986 GB |\n","|       from large pool |    5937 MB |    7999 MB |   12732 GB |   12726 GB |\n","|       from small pool |      17 MB |      18 MB |     259 GB |     259 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    5954 MB |    8017 MB |   12992 GB |   12986 GB |\n","|       from large pool |    5937 MB |    7999 MB |   12732 GB |   12726 GB |\n","|       from small pool |      17 MB |      18 MB |     259 GB |     259 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   12730 MB |   12824 MB |   50446 MB |   37716 MB |\n","|       from large pool |   12710 MB |   12710 MB |   45378 MB |   32668 MB |\n","|       from small pool |      20 MB |     114 MB |    5068 MB |    5048 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6775 MB |    6775 MB |   28163 GB |   28156 GB |\n","|       from large pool |    6772 MB |    6772 MB |   27864 GB |   27858 GB |\n","|       from small pool |       2 MB |      17 MB |     298 GB |     298 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    1986 K  |    1986 K  |\n","|       from large pool |     132    |     136    |     862 K  |     862 K  |\n","|       from small pool |     194    |     268    |    1124 K  |    1124 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    1986 K  |    1986 K  |\n","|       from large pool |     132    |     136    |     862 K  |     862 K  |\n","|       from small pool |     194    |     268    |    1124 K  |    1124 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      73    |    2561    |    2535    |\n","|       from large pool |      16    |      16    |      27    |      11    |\n","|       from small pool |      10    |      57    |    2534    |    2524    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      42    |    1030 K  |    1030 K  |\n","|       from large pool |      14    |      15    |     538 K  |     538 K  |\n","|       from small pool |      12    |      34    |     492 K  |     492 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  57% 888/1563 [01:48<01:47,  6.25it/s, loss=2.196, ppl=4.58, wps=22810.6, ups=7.83, wpb=2914.5, bsz=128, num_updates=20900, lr=0.0005, gnorm=1.331, loss_scale=16, train_wall=12, gb_free=12.3, wall=317]2022-05-26 11:13:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:34 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 55           |        cudaMalloc retries: 62        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6372 MB |    8537 MB |   13081 GB |   13075 GB |\n","|       from large pool |    6362 MB |    8527 MB |   12819 GB |   12813 GB |\n","|       from small pool |      10 MB |      17 MB |     262 GB |     262 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6372 MB |    8537 MB |   13081 GB |   13075 GB |\n","|       from large pool |    6362 MB |    8527 MB |   12819 GB |   12813 GB |\n","|       from small pool |      10 MB |      17 MB |     262 GB |     262 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13312 MB |   55090 MB |   41868 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     102 MB |    5206 MB |    5194 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6849 MB |    6849 MB |   28363 GB |   28357 GB |\n","|       from large pool |    6847 MB |    6847 MB |   28062 GB |   28055 GB |\n","|       from small pool |       1 MB |      34 MB |     301 GB |     301 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2001 K  |    2000 K  |\n","|       from large pool |     142    |     147    |     867 K  |     867 K  |\n","|       from small pool |     184    |     268    |    1133 K  |    1133 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2001 K  |    2000 K  |\n","|       from large pool |     142    |     147    |     867 K  |     867 K  |\n","|       from small pool |     184    |     268    |    1133 K  |    1133 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      67    |    2631    |    2609    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      51    |    2603    |    2597    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      55    |    1038 K  |    1037 K  |\n","|       from large pool |      17    |      18    |     541 K  |     541 K  |\n","|       from small pool |      10    |      47    |     496 K  |     496 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  61% 957/1563 [01:56<01:15,  7.98it/s, loss=2.136, ppl=4.4, wps=23175.6, ups=8.1, wpb=2860.2, bsz=128, num_updates=21000, lr=0.0005, gnorm=1.321, loss_scale=16, train_wall=12, gb_free=9.9, wall=330]2022-05-26 11:13:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.98 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:13:42 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 56           |        cudaMalloc retries: 63        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7147 MB |    9713 MB |   13415 GB |   13408 GB |\n","|       from large pool |    7137 MB |    9702 MB |   13145 GB |   13138 GB |\n","|       from small pool |      10 MB |      17 MB |     269 GB |     269 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7147 MB |    9713 MB |   13415 GB |   13408 GB |\n","|       from large pool |    7137 MB |    9702 MB |   13145 GB |   13138 GB |\n","|       from small pool |      10 MB |      17 MB |     269 GB |     269 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13322 MB |   55190 MB |   41968 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     112 MB |    5306 MB |    5294 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6074 MB |    6074 MB |   29168 GB |   29162 GB |\n","|       from large pool |    6072 MB |    6072 MB |   28858 GB |   28852 GB |\n","|       from small pool |       1 MB |      19 MB |     309 GB |     309 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2054 K  |    2054 K  |\n","|       from large pool |     142    |     146    |     890 K  |     890 K  |\n","|       from small pool |     184    |     268    |    1164 K  |    1163 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2054 K  |    2054 K  |\n","|       from large pool |     142    |     146    |     890 K  |     890 K  |\n","|       from small pool |     184    |     268    |    1164 K  |    1163 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |    2681    |    2659    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      56    |    2653    |    2647    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      17    |      43    |    1066 K  |    1066 K  |\n","|       from large pool |      10    |      11    |     555 K  |     555 K  |\n","|       from small pool |       7    |      35    |     510 K  |     510 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:13:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  78% 1225/1563 [02:30<00:38,  8.83it/s, loss=2.257, ppl=4.78, wps=24402.6, ups=8.09, wpb=3016.7, bsz=128, num_updates=21200, lr=0.0005, gnorm=1.336, loss_scale=16, train_wall=12, gb_free=11.3, wall=355]2022-05-26 11:14:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.50 GiB (GPU 0; 14.76 GiB total capacity; 6.56 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  79% 1227/1563 [02:30<00:37,  9.01it/s, loss=2.257, ppl=4.78, wps=24402.6, ups=8.09, wpb=3016.7, bsz=128, num_updates=21200, lr=0.0005, gnorm=1.336, loss_scale=16, train_wall=12, gb_free=11.3, wall=355]2022-05-26 11:14:16 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 57           |        cudaMalloc retries: 64        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6713 MB |    9029 MB |   14845 GB |   14839 GB |\n","|       from large pool |    6703 MB |    9019 MB |   14551 GB |   14545 GB |\n","|       from small pool |      10 MB |      17 MB |     294 GB |     294 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6713 MB |    9029 MB |   14845 GB |   14839 GB |\n","|       from large pool |    6703 MB |    9019 MB |   14551 GB |   14545 GB |\n","|       from small pool |      10 MB |      17 MB |     294 GB |     294 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13324 MB |   55292 MB |   42070 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     114 MB |    5408 MB |    5396 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6508 MB |    6508 MB |   32531 GB |   32525 GB |\n","|       from large pool |    6506 MB |    6506 MB |   32193 GB |   32186 GB |\n","|       from small pool |       1 MB |      39 MB |     338 GB |     338 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2268 K  |    2267 K  |\n","|       from large pool |     142    |     147    |     987 K  |     986 K  |\n","|       from small pool |     184    |     268    |    1281 K  |    1280 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2268 K  |    2267 K  |\n","|       from large pool |     142    |     147    |     987 K  |     986 K  |\n","|       from small pool |     184    |     268    |    1281 K  |    1280 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2732    |    2710    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      57    |    2704    |    2698    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      63    |    1177 K  |    1177 K  |\n","|       from large pool |      19    |      20    |     615 K  |     615 K  |\n","|       from small pool |      10    |      55    |     561 K  |     561 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:14:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  81% 1271/1563 [02:36<00:30,  9.60it/s, loss=2.36, ppl=5.14, wps=23858.6, ups=7.72, wpb=3089.2, bsz=128, num_updates=21300, lr=0.0005, gnorm=1.314, loss_scale=16, train_wall=12, gb_free=10.3, wall=368]2022-05-26 11:14:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.84 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:14:22 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 58           |        cudaMalloc retries: 65        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8029 MB |   10898 MB |   15057 GB |   15049 GB |\n","|       from large pool |    8019 MB |   10888 MB |   14758 GB |   14750 GB |\n","|       from small pool |      10 MB |      17 MB |     299 GB |     299 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8029 MB |   10898 MB |   15057 GB |   15049 GB |\n","|       from large pool |    8019 MB |   10888 MB |   14758 GB |   14750 GB |\n","|       from small pool |      10 MB |      17 MB |     299 GB |     299 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13324 MB |   55394 MB |   42172 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     114 MB |    5510 MB |    5498 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5192 MB |    5192 MB |   33049 GB |   33043 GB |\n","|       from large pool |    5190 MB |    5190 MB |   32704 GB |   32699 GB |\n","|       from small pool |       1 MB |      42 MB |     344 GB |     344 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2304 K  |    2303 K  |\n","|       from large pool |     142    |     147    |    1000 K  |    1000 K  |\n","|       from small pool |     184    |     268    |    1303 K  |    1303 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2304 K  |    2303 K  |\n","|       from large pool |     142    |     147    |    1000 K  |    1000 K  |\n","|       from small pool |     184    |     268    |    1303 K  |    1303 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2783    |    2761    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      57    |    2755    |    2749    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      52    |    1196 K  |    1195 K  |\n","|       from large pool |      21    |      22    |     624 K  |     624 K  |\n","|       from small pool |       8    |      44    |     571 K  |     571 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:14:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014: 100% 1562/1563 [03:10<00:00, 10.68it/s, loss=2.288, ppl=4.88, wps=24412.8, ups=8.52, wpb=2865.7, bsz=128, num_updates=21600, lr=0.0005, gnorm=1.405, loss_scale=16, train_wall=11, gb_free=12.2, wall=403]2022-05-26 11:14:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 014 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   0% 1/337 [00:00<00:46,  7.20it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   1% 5/337 [00:00<00:17, 18.61it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   3% 9/337 [00:00<00:13, 25.20it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   4% 12/337 [00:00<00:13, 24.06it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   5% 16/337 [00:00<00:11, 27.24it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   6% 21/337 [00:00<00:10, 29.36it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   8% 26/337 [00:00<00:09, 32.89it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   9% 30/337 [00:01<00:09, 33.25it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  10% 34/337 [00:01<00:09, 30.31it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  12% 39/337 [00:01<00:08, 33.75it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  13% 43/337 [00:01<00:09, 30.16it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  14% 47/337 [00:01<00:09, 31.92it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  15% 51/337 [00:01<00:10, 28.44it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  16% 55/337 [00:01<00:09, 30.99it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  18% 59/337 [00:02<00:08, 31.47it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  19% 63/337 [00:02<00:09, 29.68it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  20% 67/337 [00:02<00:08, 31.20it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  21% 71/337 [00:02<00:09, 29.42it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  22% 75/337 [00:02<00:08, 31.15it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  23% 79/337 [00:02<00:08, 32.03it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  25% 83/337 [00:02<00:08, 29.18it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  26% 87/337 [00:02<00:08, 30.72it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  27% 91/337 [00:03<00:07, 30.88it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  28% 95/337 [00:03<00:08, 28.84it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  29% 99/337 [00:03<00:07, 30.30it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  31% 103/337 [00:03<00:08, 29.23it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  31% 106/337 [00:03<00:08, 27.98it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  33% 110/337 [00:03<00:07, 29.81it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  34% 114/337 [00:03<00:07, 29.71it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  35% 117/337 [00:03<00:08, 26.91it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  36% 121/337 [00:04<00:07, 28.91it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  37% 125/337 [00:04<00:07, 29.82it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  38% 129/337 [00:04<00:06, 30.02it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  39% 133/337 [00:04<00:07, 27.29it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  41% 137/337 [00:04<00:07, 28.34it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  42% 141/337 [00:04<00:06, 28.73it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  43% 144/337 [00:04<00:07, 26.48it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  44% 147/337 [00:05<00:06, 27.26it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  45% 150/337 [00:05<00:06, 27.51it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  45% 153/337 [00:05<00:06, 27.26it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  46% 156/337 [00:05<00:07, 25.22it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  47% 160/337 [00:05<00:06, 27.15it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  48% 163/337 [00:05<00:06, 27.31it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  49% 166/337 [00:05<00:06, 26.99it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  50% 169/337 [00:05<00:07, 22.59it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  51% 172/337 [00:06<00:06, 24.24it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  52% 175/337 [00:06<00:06, 24.59it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  53% 178/337 [00:06<00:06, 25.29it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  54% 181/337 [00:06<00:06, 23.54it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  55% 184/337 [00:06<00:06, 24.22it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  55% 187/337 [00:06<00:05, 25.09it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  56% 190/337 [00:06<00:05, 25.52it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  57% 193/337 [00:06<00:05, 24.05it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  58% 196/337 [00:07<00:05, 24.98it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  59% 199/337 [00:07<00:05, 25.94it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  60% 202/337 [00:07<00:05, 26.34it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  61% 205/337 [00:07<00:05, 23.91it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  62% 208/337 [00:07<00:05, 24.98it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  63% 211/337 [00:07<00:04, 25.82it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  64% 214/337 [00:07<00:04, 25.54it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  64% 217/337 [00:07<00:05, 22.65it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  65% 220/337 [00:07<00:04, 24.01it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  66% 223/337 [00:08<00:04, 24.49it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  67% 226/337 [00:08<00:04, 23.14it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  68% 229/337 [00:08<00:05, 20.42it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  69% 232/337 [00:08<00:04, 21.27it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  70% 235/337 [00:08<00:04, 21.91it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  71% 238/337 [00:08<00:05, 19.75it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  72% 241/337 [00:09<00:04, 21.23it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  72% 244/337 [00:09<00:04, 22.27it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  73% 247/337 [00:09<00:04, 21.17it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  74% 250/337 [00:09<00:04, 21.66it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  75% 253/337 [00:09<00:03, 22.09it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  76% 256/337 [00:09<00:04, 20.24it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  77% 259/337 [00:09<00:03, 21.15it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  78% 262/337 [00:09<00:03, 21.75it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  79% 265/337 [00:10<00:03, 21.24it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  80% 268/337 [00:10<00:03, 21.58it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  80% 271/337 [00:10<00:03, 19.66it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  81% 274/337 [00:10<00:03, 20.62it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  82% 277/337 [00:10<00:03, 19.24it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  83% 280/337 [00:10<00:02, 20.27it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  84% 283/337 [00:11<00:02, 20.20it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  85% 286/337 [00:11<00:02, 17.98it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  86% 289/337 [00:11<00:02, 18.19it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  87% 292/337 [00:11<00:02, 19.24it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  87% 294/337 [00:11<00:02, 19.31it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  88% 296/337 [00:11<00:02, 18.72it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  89% 299/337 [00:11<00:01, 19.42it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  89% 301/337 [00:12<00:02, 17.90it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  90% 303/337 [00:12<00:01, 18.25it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  91% 305/337 [00:12<00:01, 17.70it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  91% 307/337 [00:12<00:01, 18.15it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  92% 309/337 [00:12<00:01, 15.91it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  92% 311/337 [00:12<00:01, 16.49it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  93% 313/337 [00:12<00:01, 16.05it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  93% 315/337 [00:12<00:01, 14.66it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  94% 317/337 [00:13<00:01, 15.32it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  95% 319/337 [00:13<00:01, 15.50it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  95% 321/337 [00:13<00:01, 15.07it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  96% 323/337 [00:13<00:00, 15.04it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  96% 325/337 [00:13<00:00, 14.67it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  97% 327/337 [00:13<00:00, 14.16it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  98% 329/337 [00:13<00:00, 14.05it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  98% 331/337 [00:14<00:00, 13.49it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  99% 333/337 [00:14<00:00, 12.21it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  99% 335/337 [00:14<00:00, 11.61it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset: 100% 337/337 [00:14<00:00, 13.01it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 11:15:11 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.475 | ppl 88.93 | wps 69186.5 | wpb 2965.1 | bsz 127.6 | num_updates 21618 | best_loss 5.709\n","2022-05-26 11:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21618 updates\n","2022-05-26 11:15:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_en_de/checkpoint_last.pt\n","epoch 014: 100% 1562/1563 [03:26<00:00, 10.68it/s, loss=2.288, ppl=4.88, wps=24412.8, ups=8.52, wpb=2865.7, bsz=128, num_updates=21600, lr=0.0005, gnorm=1.405, loss_scale=16, train_wall=11, gb_free=12.2, wall=403]2022-05-26 11:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_en_de/checkpoint_last.pt\n","2022-05-26 11:15:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_en_de/checkpoint_last.pt (epoch 14 @ 21618 updates, score 6.475) (writing took 3.661625605000154 seconds)\n","2022-05-26 11:15:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2022-05-26 11:15:14 | INFO | train | epoch 014 | loss 2.174 | ppl 4.51 | wps 21702.3 | ups 7.36 | wpb 2947.6 | bsz 128 | num_updates 21618 | lr 0.0005 | gnorm 1.31 | loss_scale 16 | train_wall 182 | gb_free 7.7 | wall 424\n","epoch 015:   0% 0/1563 [00:00<?, ?it/s]2022-05-26 11:15:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1563\n","2022-05-26 11:15:14 | INFO | fairseq.trainer | begin training epoch 15\n","2022-05-26 11:15:14 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 015:  11% 177/1563 [00:21<02:27,  9.37it/s, loss=1.895, ppl=3.72, wps=9246.6, ups=3.33, wpb=2779.4, bsz=128, num_updates=21700, lr=0.0005, gnorm=1.243, loss_scale=16, train_wall=11, gb_free=10.2, wall=433]2022-05-26 11:15:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.98 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:15:36 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 59           |        cudaMalloc retries: 66        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7147 MB |    9713 MB |   18164 GB |   18157 GB |\n","|       from large pool |    7137 MB |    9702 MB |   17806 GB |   17799 GB |\n","|       from small pool |      10 MB |      17 MB |     358 GB |     358 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7147 MB |    9713 MB |   18164 GB |   18157 GB |\n","|       from large pool |    7137 MB |    9702 MB |   17806 GB |   17799 GB |\n","|       from small pool |      10 MB |      17 MB |     358 GB |     358 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13324 MB |   55496 MB |   42272 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      14 MB |     114 MB |    5612 MB |    5598 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6076 MB |    6076 MB |   40429 GB |   40423 GB |\n","|       from large pool |    6072 MB |    6072 MB |   40016 GB |   40010 GB |\n","|       from small pool |       3 MB |      17 MB |     413 GB |     412 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2755 K  |    2754 K  |\n","|       from large pool |     142    |     146    |    1196 K  |    1195 K  |\n","|       from small pool |     184    |     268    |    1559 K  |    1559 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2755 K  |    2754 K  |\n","|       from large pool |     142    |     146    |    1196 K  |    1195 K  |\n","|       from small pool |     184    |     268    |    1559 K  |    1559 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      73    |    2834    |    2811    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       7    |      57    |    2806    |    2799    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      40    |    1432 K  |    1432 K  |\n","|       from large pool |      10    |      11    |     747 K  |     747 K  |\n","|       from small pool |      11    |      32    |     685 K  |     685 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:15:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  24% 374/1563 [00:46<02:35,  7.65it/s, loss=2.05, ppl=4.14, wps=23209, ups=7.96, wpb=2916.4, bsz=127.4, num_updates=21900, lr=0.0005, gnorm=1.285, loss_scale=16, train_wall=12, gb_free=12.2, wall=458]2022-05-26 11:16:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.44 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:16:02 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 60           |        cudaMalloc retries: 67        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7617 MB |   10284 MB |   19223 GB |   19216 GB |\n","|       from large pool |    7606 MB |   10274 MB |   18846 GB |   18839 GB |\n","|       from small pool |      10 MB |      17 MB |     377 GB |     377 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7617 MB |   10284 MB |   19223 GB |   19216 GB |\n","|       from large pool |    7606 MB |   10274 MB |   18846 GB |   18839 GB |\n","|       from small pool |      10 MB |      17 MB |     377 GB |     377 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13324 MB |   55596 MB |   42374 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     114 MB |    5712 MB |    5700 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5604 MB |    5604 MB |   42887 GB |   42882 GB |\n","|       from large pool |    5603 MB |    5603 MB |   42453 GB |   42447 GB |\n","|       from small pool |       1 MB |      38 MB |     434 GB |     434 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2912 K  |    2912 K  |\n","|       from large pool |     142    |     147    |    1266 K  |    1266 K  |\n","|       from small pool |     184    |     268    |    1645 K  |    1645 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2912 K  |    2912 K  |\n","|       from large pool |     142    |     147    |    1266 K  |    1266 K  |\n","|       from small pool |     184    |     268    |    1645 K  |    1645 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    2884    |    2862    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      57    |    2856    |    2850    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      67    |    1514 K  |    1514 K  |\n","|       from large pool |      22    |      23    |     790 K  |     790 K  |\n","|       from small pool |       9    |      59    |     723 K  |     723 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:16:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  24% 378/1563 [00:47<02:28,  7.98it/s, loss=2.05, ppl=4.14, wps=23209, ups=7.96, wpb=2916.4, bsz=127.4, num_updates=21900, lr=0.0005, gnorm=1.285, loss_scale=16, train_wall=12, gb_free=12.2, wall=458]2022-05-26 11:16:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.86 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:16:02 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 61           |        cudaMalloc retries: 68        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9069 MB |   12417 MB |   19245 GB |   19236 GB |\n","|       from large pool |    9059 MB |   12406 MB |   18868 GB |   18859 GB |\n","|       from small pool |      10 MB |      17 MB |     377 GB |     377 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9069 MB |   12417 MB |   19245 GB |   19236 GB |\n","|       from large pool |    9059 MB |   12406 MB |   18868 GB |   18859 GB |\n","|       from small pool |      10 MB |      17 MB |     377 GB |     377 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13246 MB |   55620 MB |   42398 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |      36 MB |    5736 MB |    5724 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4152 MB |    4512 MB |   42923 GB |   42919 GB |\n","|       from large pool |    4150 MB |    4510 MB |   42488 GB |   42484 GB |\n","|       from small pool |       1 MB |       6 MB |     434 GB |     434 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2914 K  |    2914 K  |\n","|       from large pool |     142    |     147    |    1267 K  |    1267 K  |\n","|       from small pool |     184    |     268    |    1646 K  |    1646 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2914 K  |    2914 K  |\n","|       from large pool |     142    |     147    |    1267 K  |    1267 K  |\n","|       from small pool |     184    |     268    |    1646 K  |    1646 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      34    |    2896    |    2874    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      18    |    2868    |    2862    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      28    |    1515 K  |    1515 K  |\n","|       from large pool |      18    |      19    |     791 K  |     791 K  |\n","|       from small pool |       9    |      16    |     723 K  |     723 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:16:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  28% 439/1563 [00:54<02:32,  7.37it/s, loss=2.133, ppl=4.39, wps=24258.7, ups=7.55, wpb=3214.2, bsz=128, num_updates=22000, lr=0.0005, gnorm=1.294, loss_scale=16, train_wall=13, gb_free=10.1, wall=472]2022-05-26 11:16:09 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 7.05 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:16:09 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 62           |        cudaMalloc retries: 69        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7217 MB |    9784 MB |   19549 GB |   19542 GB |\n","|       from large pool |    7207 MB |    9773 MB |   19165 GB |   19158 GB |\n","|       from small pool |      10 MB |      17 MB |     383 GB |     383 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7217 MB |    9784 MB |   19549 GB |   19542 GB |\n","|       from large pool |    7207 MB |    9773 MB |   19165 GB |   19158 GB |\n","|       from small pool |      10 MB |      17 MB |     383 GB |     383 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13322 MB |   55720 MB |   42498 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     112 MB |    5836 MB |    5824 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6004 MB |    6004 MB |   43655 GB |   43650 GB |\n","|       from large pool |    6002 MB |    6002 MB |   43213 GB |   43207 GB |\n","|       from small pool |       1 MB |       5 MB |     442 GB |     442 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2962 K  |    2962 K  |\n","|       from large pool |     142    |     147    |    1288 K  |    1288 K  |\n","|       from small pool |     184    |     268    |    1674 K  |    1673 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2962 K  |    2962 K  |\n","|       from large pool |     142    |     147    |    1288 K  |    1288 K  |\n","|       from small pool |     184    |     268    |    1674 K  |    1673 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |    2946    |    2924    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      56    |    2918    |    2912    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      25    |    1540 K  |    1540 K  |\n","|       from large pool |      14    |      15    |     804 K  |     804 K  |\n","|       from small pool |       9    |      17    |     735 K  |     735 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:16:09 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  31% 480/1563 [00:59<02:15,  7.99it/s, loss=2.133, ppl=4.39, wps=24258.7, ups=7.55, wpb=3214.2, bsz=128, num_updates=22000, lr=0.0005, gnorm=1.294, loss_scale=16, train_wall=13, gb_free=10.1, wall=472]2022-05-26 11:16:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.04 GiB (GPU 0; 14.76 GiB total capacity; 7.08 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:16:14 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 63           |        cudaMalloc retries: 70        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7246 MB |    9838 MB |   19757 GB |   19749 GB |\n","|       from large pool |    7236 MB |    9828 MB |   19369 GB |   19362 GB |\n","|       from small pool |      10 MB |      17 MB |     387 GB |     387 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7246 MB |    9838 MB |   19757 GB |   19749 GB |\n","|       from large pool |    7236 MB |    9828 MB |   19369 GB |   19362 GB |\n","|       from small pool |      10 MB |      17 MB |     387 GB |     387 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13322 MB |   55820 MB |   42598 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     112 MB |    5936 MB |    5924 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5975 MB |    5975 MB |   44134 GB |   44128 GB |\n","|       from large pool |    5973 MB |    5973 MB |   43688 GB |   43682 GB |\n","|       from small pool |       1 MB |      19 MB |     446 GB |     446 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    2994 K  |    2994 K  |\n","|       from large pool |     142    |     147    |    1302 K  |    1302 K  |\n","|       from small pool |     184    |     268    |    1691 K  |    1691 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    2994 K  |    2994 K  |\n","|       from large pool |     142    |     147    |    1302 K  |    1302 K  |\n","|       from small pool |     184    |     268    |    1691 K  |    1691 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      72    |    2996    |    2974    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      56    |    2968    |    2962    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      47    |    1557 K  |    1557 K  |\n","|       from large pool |      16    |      17    |     813 K  |     813 K  |\n","|       from small pool |      10    |      39    |     743 K  |     743 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:16:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  32% 495/1563 [01:01<01:45, 10.08it/s, loss=1.983, ppl=3.95, wps=23267.5, ups=7.97, wpb=2920.9, bsz=128, num_updates=22100, lr=0.0005, gnorm=1.281, loss_scale=16, train_wall=12, gb_free=10.5, wall=484]2022-05-26 11:16:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.50 GiB (GPU 0; 14.76 GiB total capacity; 6.56 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:16:16 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 64           |        cudaMalloc retries: 71        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6713 MB |    9029 MB |   19825 GB |   19819 GB |\n","|       from large pool |    6703 MB |    9019 MB |   19436 GB |   19429 GB |\n","|       from small pool |      10 MB |      17 MB |     389 GB |     389 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6713 MB |    9029 MB |   19825 GB |   19819 GB |\n","|       from large pool |    6703 MB |    9019 MB |   19436 GB |   19429 GB |\n","|       from small pool |      10 MB |      17 MB |     389 GB |     389 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13312 MB |   55910 MB |   42688 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     102 MB |    6026 MB |    6014 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6508 MB |    6508 MB |   44301 GB |   44295 GB |\n","|       from large pool |    6506 MB |    6506 MB |   43853 GB |   43846 GB |\n","|       from small pool |       1 MB |      31 MB |     448 GB |     448 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3005 K  |    3005 K  |\n","|       from large pool |     142    |     147    |    1307 K  |    1307 K  |\n","|       from small pool |     184    |     268    |    1698 K  |    1697 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3005 K  |    3005 K  |\n","|       from large pool |     142    |     147    |    1307 K  |    1307 K  |\n","|       from small pool |     184    |     268    |    1698 K  |    1697 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      67    |    3041    |    3019    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      51    |    3013    |    3007    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      49    |    1563 K  |    1563 K  |\n","|       from large pool |      19    |      20    |     816 K  |     816 K  |\n","|       from small pool |       8    |      41    |     746 K  |     746 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:16:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  35% 544/1563 [01:07<01:55,  8.81it/s, loss=1.983, ppl=3.95, wps=23267.5, ups=7.97, wpb=2920.9, bsz=128, num_updates=22100, lr=0.0005, gnorm=1.281, loss_scale=16, train_wall=12, gb_free=10.5, wall=484]2022-05-26 11:16:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.84 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:16:22 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 65           |        cudaMalloc retries: 72        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8029 MB |   10898 MB |   20057 GB |   20049 GB |\n","|       from large pool |    8019 MB |   10888 MB |   19662 GB |   19654 GB |\n","|       from small pool |      10 MB |      17 MB |     394 GB |     394 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8029 MB |   10898 MB |   20057 GB |   20049 GB |\n","|       from large pool |    8019 MB |   10888 MB |   19662 GB |   19654 GB |\n","|       from small pool |      10 MB |      17 MB |     394 GB |     394 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13320 MB |   56008 MB |   42784 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      14 MB |     110 MB |    6124 MB |    6110 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5194 MB |    5194 MB |   44862 GB |   44857 GB |\n","|       from large pool |    5190 MB |    5190 MB |   44407 GB |   44402 GB |\n","|       from small pool |       3 MB |      33 MB |     454 GB |     454 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3044 K  |    3043 K  |\n","|       from large pool |     142    |     147    |    1323 K  |    1323 K  |\n","|       from small pool |     184    |     268    |    1720 K  |    1720 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3044 K  |    3043 K  |\n","|       from large pool |     142    |     147    |    1323 K  |    1323 K  |\n","|       from small pool |     184    |     268    |    1720 K  |    1720 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      23    |      71    |    3090    |    3067    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       7    |      55    |    3062    |    3055    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      56    |    1583 K  |    1583 K  |\n","|       from large pool |      21    |      22    |     826 K  |     826 K  |\n","|       from small pool |      10    |      49    |     757 K  |     757 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:16:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  42% 660/1563 [01:21<01:46,  8.48it/s, loss=2.094, ppl=4.27, wps=24377.8, ups=8.03, wpb=3036, bsz=128, num_updates=22200, lr=0.0005, gnorm=1.294, loss_scale=16, train_wall=12, gb_free=7.5, wall=497]2022-05-26 11:16:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n","epoch 015:  55% 858/1563 [01:46<01:21,  8.70it/s, loss=2.138, ppl=4.4, wps=23854.1, ups=7.8, wpb=3058.2, bsz=128, num_updates=22400, lr=0.0005, gnorm=1.325, loss_scale=8, train_wall=13, gb_free=9.6, wall=522]2022-05-26 11:17:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.96 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 015:  55% 859/1563 [01:46<01:25,  8.20it/s, loss=2.138, ppl=4.4, wps=23854.1, ups=7.8, wpb=3058.2, bsz=128, num_updates=22400, lr=0.0005, gnorm=1.325, loss_scale=8, train_wall=13, gb_free=9.6, wall=522]2022-05-26 11:17:01 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 66           |        cudaMalloc retries: 73        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7122 MB |    9689 MB |   21673 GB |   21666 GB |\n","|       from large pool |    7102 MB |    9668 MB |   21245 GB |   21238 GB |\n","|       from small pool |      20 MB |      20 MB |     427 GB |     427 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7122 MB |    9689 MB |   21673 GB |   21666 GB |\n","|       from large pool |    7102 MB |    9668 MB |   21245 GB |   21238 GB |\n","|       from small pool |      20 MB |      20 MB |     427 GB |     427 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13324 MB |   56108 MB |   42876 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      22 MB |     114 MB |    6224 MB |    6202 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6109 MB |    6109 MB |   48700 GB |   48694 GB |\n","|       from large pool |    6107 MB |    6107 MB |   48207 GB |   48201 GB |\n","|       from small pool |       1 MB |      21 MB |     492 GB |     492 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3292 K  |    3292 K  |\n","|       from large pool |     132    |     136    |    1431 K  |    1431 K  |\n","|       from small pool |     194    |     268    |    1861 K  |    1861 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3292 K  |    3292 K  |\n","|       from large pool |     132    |     136    |    1431 K  |    1431 K  |\n","|       from small pool |     194    |     268    |    1861 K  |    1861 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      27    |      73    |    3140    |    3113    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |      11    |      57    |    3112    |    3101    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      45    |    1713 K  |    1713 K  |\n","|       from large pool |      13    |      14    |     893 K  |     893 K  |\n","|       from small pool |      10    |      37    |     819 K  |     819 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:17:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  86% 1351/1563 [02:45<00:23,  8.85it/s, loss=2.009, ppl=4.03, wps=24469, ups=8.44, wpb=2900.3, bsz=128, num_updates=22900, lr=0.0005, gnorm=1.311, loss_scale=8, train_wall=11, gb_free=12.7, wall=582]2022-05-26 11:18:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 7.03 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:18:00 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 67           |        cudaMalloc retries: 74        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7194 MB |    9710 MB |   24096 GB |   24089 GB |\n","|       from large pool |    7184 MB |    9700 MB |   23618 GB |   23611 GB |\n","|       from small pool |      10 MB |      17 MB |     478 GB |     478 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7194 MB |    9710 MB |   24096 GB |   24089 GB |\n","|       from large pool |    7184 MB |    9700 MB |   23618 GB |   23611 GB |\n","|       from small pool |      10 MB |      17 MB |     478 GB |     478 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13324 MB |   56200 MB |   42978 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     114 MB |    6316 MB |    6304 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6027 MB |    6027 MB |   54601 GB |   54595 GB |\n","|       from large pool |    6025 MB |    6025 MB |   54050 GB |   54044 GB |\n","|       from small pool |       1 MB |      17 MB |     551 GB |     551 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3683 K  |    3683 K  |\n","|       from large pool |     142    |     147    |    1600 K  |    1599 K  |\n","|       from small pool |     184    |     268    |    2083 K  |    2083 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3683 K  |    3683 K  |\n","|       from large pool |     142    |     147    |    1600 K  |    1599 K  |\n","|       from small pool |     184    |     268    |    2083 K  |    2083 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    3186    |    3164    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      57    |    3158    |    3152    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      34    |    1917 K  |    1917 K  |\n","|       from large pool |      21    |      22    |     998 K  |     998 K  |\n","|       from small pool |      10    |      26    |     918 K  |     918 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:18:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  87% 1366/1563 [02:47<00:27,  7.21it/s, loss=2.009, ppl=4.03, wps=24469, ups=8.44, wpb=2900.3, bsz=128, num_updates=22900, lr=0.0005, gnorm=1.311, loss_scale=8, train_wall=11, gb_free=12.7, wall=582]2022-05-26 11:18:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.35 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:18:02 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 68           |        cudaMalloc retries: 75        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7523 MB |   10215 MB |   24177 GB |   24169 GB |\n","|       from large pool |    7512 MB |   10205 MB |   23697 GB |   23690 GB |\n","|       from small pool |      10 MB |      17 MB |     479 GB |     479 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7523 MB |   10215 MB |   24177 GB |   24169 GB |\n","|       from large pool |    7512 MB |   10205 MB |   23697 GB |   23690 GB |\n","|       from small pool |      10 MB |      17 MB |     479 GB |     479 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13320 MB |   56298 MB |   43076 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     110 MB |    6414 MB |    6402 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5698 MB |    5698 MB |   54781 GB |   54776 GB |\n","|       from large pool |    5697 MB |    5697 MB |   54229 GB |   54223 GB |\n","|       from small pool |       1 MB |       3 MB |     552 GB |     552 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3695 K  |    3695 K  |\n","|       from large pool |     142    |     147    |    1605 K  |    1605 K  |\n","|       from small pool |     184    |     268    |    2090 K  |    2090 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3695 K  |    3695 K  |\n","|       from large pool |     142    |     147    |    1605 K  |    1605 K  |\n","|       from small pool |     184    |     268    |    2090 K  |    2090 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      71    |    3235    |    3213    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      55    |    3207    |    3201    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      26    |    1923 K  |    1923 K  |\n","|       from large pool |      16    |      17    |    1001 K  |    1001 K  |\n","|       from small pool |       9    |      18    |     921 K  |     921 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:18:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  89% 1384/1563 [02:49<00:23,  7.69it/s, loss=2.009, ppl=4.03, wps=24469, ups=8.44, wpb=2900.3, bsz=128, num_updates=22900, lr=0.0005, gnorm=1.311, loss_scale=8, train_wall=11, gb_free=12.7, wall=582]2022-05-26 11:18:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:18:05 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 69           |        cudaMalloc retries: 76        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6372 MB |    8537 MB |   24277 GB |   24271 GB |\n","|       from large pool |    6362 MB |    8527 MB |   23796 GB |   23789 GB |\n","|       from small pool |      10 MB |      17 MB |     481 GB |     481 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6372 MB |    8537 MB |   24277 GB |   24271 GB |\n","|       from large pool |    6362 MB |    8527 MB |   23796 GB |   23789 GB |\n","|       from small pool |      10 MB |      17 MB |     481 GB |     481 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13286 MB |   56362 MB |   43140 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |      76 MB |    6478 MB |    6466 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6849 MB |    6849 MB |   54993 GB |   54987 GB |\n","|       from large pool |    6847 MB |    6847 MB |   54438 GB |   54432 GB |\n","|       from small pool |       1 MB |      29 MB |     554 GB |     554 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3709 K  |    3709 K  |\n","|       from large pool |     142    |     147    |    1611 K  |    1610 K  |\n","|       from small pool |     184    |     268    |    2098 K  |    2098 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3709 K  |    3709 K  |\n","|       from large pool |     142    |     147    |    1611 K  |    1610 K  |\n","|       from small pool |     184    |     268    |    2098 K  |    2098 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      54    |    3267    |    3245    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      38    |    3239    |    3233    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      54    |    1930 K  |    1930 K  |\n","|       from large pool |      17    |      18    |    1005 K  |    1005 K  |\n","|       from small pool |       9    |      46    |     925 K  |     925 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:18:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  90% 1403/1563 [02:52<00:18,  8.64it/s, loss=2.3, ppl=4.93, wps=23453.3, ups=7.77, wpb=3018, bsz=128, num_updates=23000, lr=0.0005, gnorm=1.376, loss_scale=8, train_wall=12, gb_free=11.4, wall=595]2022-05-26 11:18:07 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.19 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:18:07 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 70           |        cudaMalloc retries: 77        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6337 MB |    8501 MB |   24379 GB |   24373 GB |\n","|       from large pool |    6327 MB |    8491 MB |   23895 GB |   23889 GB |\n","|       from small pool |      10 MB |      17 MB |     483 GB |     483 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6337 MB |    8501 MB |   24379 GB |   24373 GB |\n","|       from large pool |    6327 MB |    8491 MB |   23895 GB |   23889 GB |\n","|       from small pool |      10 MB |      17 MB |     483 GB |     483 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13324 MB |   56464 MB |   43242 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     114 MB |    6580 MB |    6568 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6884 MB |    6884 MB |   55224 GB |   55217 GB |\n","|       from large pool |    6882 MB |    6882 MB |   54667 GB |   54660 GB |\n","|       from small pool |       1 MB |      16 MB |     557 GB |     557 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3723 K  |    3723 K  |\n","|       from large pool |     142    |     147    |    1617 K  |    1616 K  |\n","|       from small pool |     184    |     268    |    2106 K  |    2106 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3723 K  |    3723 K  |\n","|       from large pool |     142    |     147    |    1617 K  |    1616 K  |\n","|       from small pool |     184    |     268    |    2106 K  |    2106 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    3318    |    3296    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      57    |    3290    |    3284    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      39    |    1937 K  |    1937 K  |\n","|       from large pool |      17    |      18    |    1009 K  |    1009 K  |\n","|       from small pool |       7    |      32    |     928 K  |     928 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:18:07 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  97% 1520/1563 [03:06<00:05,  7.92it/s, loss=2.206, ppl=4.61, wps=24368.1, ups=8.14, wpb=2995.1, bsz=128, num_updates=23100, lr=0.0005, gnorm=1.381, loss_scale=8, train_wall=12, gb_free=8.2, wall=607]2022-05-26 11:18:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.88 GiB already allocated; 621.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:18:21 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 71           |        cudaMalloc retries: 78        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7048 MB |    9439 MB |   24984 GB |   24977 GB |\n","|       from large pool |    7037 MB |    9429 MB |   24489 GB |   24483 GB |\n","|       from small pool |      10 MB |      17 MB |     494 GB |     494 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7048 MB |    9439 MB |   24984 GB |   24977 GB |\n","|       from large pool |    7037 MB |    9429 MB |   24489 GB |   24483 GB |\n","|       from small pool |      10 MB |      17 MB |     494 GB |     494 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13222 MB |   13324 MB |   56566 MB |   43344 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      12 MB |     114 MB |    6682 MB |    6670 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6173 MB |    6173 MB |   56637 GB |   56631 GB |\n","|       from large pool |    6172 MB |    6172 MB |   56067 GB |   56061 GB |\n","|       from small pool |       1 MB |       5 MB |     570 GB |     570 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3816 K  |    3816 K  |\n","|       from large pool |     142    |     147    |    1658 K  |    1657 K  |\n","|       from small pool |     184    |     268    |    2158 K  |    2158 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3816 K  |    3816 K  |\n","|       from large pool |     142    |     147    |    1658 K  |    1657 K  |\n","|       from small pool |     184    |     268    |    2158 K  |    2158 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      22    |      73    |    3369    |    3347    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |       6    |      57    |    3341    |    3335    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      32    |    1986 K  |    1985 K  |\n","|       from large pool |      21    |      22    |    1034 K  |    1034 K  |\n","|       from small pool |       9    |      19    |     951 K  |     951 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 015:  97% 1521/1563 [03:06<00:05,  7.53it/s, loss=2.206, ppl=4.61, wps=24368.1, ups=8.14, wpb=2995.1, bsz=128, num_updates=23100, lr=0.0005, gnorm=1.381, loss_scale=8, train_wall=12, gb_free=8.2, wall=607]2022-05-26 11:18:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  99% 1545/1563 [03:09<00:02,  8.20it/s, loss=2.206, ppl=4.61, wps=24368.1, ups=8.14, wpb=2995.1, bsz=128, num_updates=23100, lr=0.0005, gnorm=1.381, loss_scale=8, train_wall=12, gb_free=8.2, wall=607]2022-05-26 11:18:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.91 GiB already allocated; 613.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 11:18:25 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 72           |        cudaMalloc retries: 79        |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9123 MB |   12570 MB |   25112 GB |   25104 GB |\n","|       from large pool |    9105 MB |   12550 MB |   24615 GB |   24606 GB |\n","|       from small pool |      18 MB |      19 MB |     497 GB |     497 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9123 MB |   12570 MB |   25112 GB |   25104 GB |\n","|       from large pool |    9105 MB |   12550 MB |   24615 GB |   24606 GB |\n","|       from small pool |      18 MB |      19 MB |     497 GB |     497 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13230 MB |   13324 MB |   56668 MB |   43438 MB |\n","|       from large pool |   13210 MB |   13210 MB |   49884 MB |   36674 MB |\n","|       from small pool |      20 MB |     114 MB |    6784 MB |    6764 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4106 MB |    4538 MB |   56924 GB |   56920 GB |\n","|       from large pool |    4104 MB |    4536 MB |   56350 GB |   56346 GB |\n","|       from small pool |       1 MB |      45 MB |     573 GB |     573 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |    3835 K  |    3835 K  |\n","|       from large pool |     132    |     136    |    1665 K  |    1665 K  |\n","|       from small pool |     194    |     268    |    2170 K  |    2170 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |    3835 K  |    3835 K  |\n","|       from large pool |     132    |     136    |    1665 K  |    1665 K  |\n","|       from small pool |     194    |     268    |    2170 K  |    2170 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      26    |      73    |    3420    |    3394    |\n","|       from large pool |      16    |      16    |      28    |      12    |\n","|       from small pool |      10    |      57    |    3392    |    3382    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      55    |    1995 K  |    1995 K  |\n","|       from large pool |      18    |      19    |    1039 K  |    1039 K  |\n","|       from small pool |      13    |      48    |     956 K  |     956 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 11:18:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015: 100% 1562/1563 [03:12<00:00,  7.53it/s, loss=2.206, ppl=4.61, wps=24368.1, ups=8.14, wpb=2995.1, bsz=128, num_updates=23100, lr=0.0005, gnorm=1.381, loss_scale=8, train_wall=12, gb_free=8.2, wall=607]2022-05-26 11:18:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 015 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   0% 1/337 [00:00<00:49,  6.79it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   1% 5/337 [00:00<00:18, 18.39it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   3% 9/337 [00:00<00:13, 23.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   4% 12/337 [00:00<00:15, 21.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   5% 16/337 [00:00<00:12, 25.95it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   6% 21/337 [00:00<00:10, 29.02it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   7% 25/337 [00:00<00:09, 31.73it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   9% 29/337 [00:01<00:09, 32.39it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  10% 33/337 [00:01<00:08, 34.42it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  11% 37/337 [00:01<00:08, 33.63it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  12% 41/337 [00:01<00:08, 33.73it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  13% 45/337 [00:01<00:09, 30.99it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  15% 49/337 [00:01<00:08, 32.34it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  16% 53/337 [00:01<00:09, 30.87it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  17% 57/337 [00:01<00:08, 32.68it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  18% 61/337 [00:02<00:09, 28.61it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  19% 65/337 [00:02<00:09, 29.85it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  20% 69/337 [00:02<00:08, 30.61it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  22% 73/337 [00:02<00:09, 29.11it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  23% 77/337 [00:02<00:08, 30.44it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  24% 81/337 [00:02<00:09, 27.24it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  25% 85/337 [00:02<00:08, 29.16it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  26% 89/337 [00:03<00:08, 30.40it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  28% 93/337 [00:03<00:08, 27.30it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  29% 97/337 [00:03<00:08, 28.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  30% 101/337 [00:03<00:08, 29.43it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  31% 105/337 [00:03<00:08, 27.55it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  32% 109/337 [00:03<00:07, 28.63it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  33% 112/337 [00:03<00:07, 28.72it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  34% 115/337 [00:03<00:07, 28.95it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  35% 118/337 [00:04<00:08, 26.07it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  36% 122/337 [00:04<00:07, 28.05it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  37% 126/337 [00:04<00:07, 28.83it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  38% 129/337 [00:04<00:07, 26.75it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  39% 132/337 [00:04<00:08, 25.04it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  40% 136/337 [00:04<00:07, 27.56it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  41% 139/337 [00:04<00:07, 27.57it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  42% 142/337 [00:04<00:07, 27.32it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  43% 145/337 [00:05<00:07, 25.27it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  44% 149/337 [00:05<00:06, 26.92it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  45% 152/337 [00:05<00:06, 26.95it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  46% 155/337 [00:05<00:06, 26.20it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  47% 158/337 [00:05<00:07, 24.79it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  48% 161/337 [00:05<00:06, 26.06it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  49% 164/337 [00:05<00:06, 26.63it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  50% 167/337 [00:05<00:06, 26.29it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  50% 170/337 [00:06<00:07, 22.57it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  51% 173/337 [00:06<00:06, 23.53it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  52% 176/337 [00:06<00:06, 24.29it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  53% 179/337 [00:06<00:06, 24.82it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  54% 182/337 [00:06<00:06, 23.09it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  55% 185/337 [00:06<00:06, 24.63it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  56% 188/337 [00:06<00:05, 25.11it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  57% 191/337 [00:06<00:05, 25.05it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  58% 194/337 [00:07<00:06, 23.57it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  59% 198/337 [00:07<00:05, 25.30it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  60% 201/337 [00:07<00:05, 25.36it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  61% 204/337 [00:07<00:05, 24.40it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  61% 207/337 [00:07<00:05, 23.66it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  62% 210/337 [00:07<00:05, 23.67it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  63% 213/337 [00:07<00:05, 24.24it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  64% 216/337 [00:07<00:05, 23.69it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  65% 219/337 [00:08<00:05, 23.17it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  66% 222/337 [00:08<00:04, 24.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  67% 225/337 [00:08<00:04, 24.13it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  68% 228/337 [00:08<00:05, 21.25it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  69% 231/337 [00:08<00:04, 22.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  69% 234/337 [00:08<00:04, 22.75it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  70% 237/337 [00:08<00:04, 20.27it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  71% 240/337 [00:09<00:04, 21.08it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  72% 243/337 [00:09<00:04, 22.15it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  73% 246/337 [00:09<00:04, 20.76it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  74% 249/337 [00:09<00:04, 21.75it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  75% 252/337 [00:09<00:03, 21.37it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  76% 255/337 [00:09<00:04, 19.58it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  77% 258/337 [00:09<00:03, 21.03it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  77% 261/337 [00:10<00:03, 21.60it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  78% 264/337 [00:10<00:03, 20.68it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  79% 267/337 [00:10<00:03, 21.81it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  80% 270/337 [00:10<00:03, 20.98it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  81% 273/337 [00:10<00:03, 20.49it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  82% 276/337 [00:10<00:03, 20.23it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  83% 279/337 [00:11<00:03, 19.24it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  83% 281/337 [00:11<00:02, 19.35it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  84% 283/337 [00:11<00:02, 19.36it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  85% 285/337 [00:11<00:03, 16.97it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  85% 288/337 [00:11<00:02, 18.24it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  86% 290/337 [00:11<00:02, 18.26it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  87% 293/337 [00:11<00:02, 18.98it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  88% 295/337 [00:11<00:02, 18.08it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  88% 298/337 [00:12<00:02, 18.81it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  89% 300/337 [00:12<00:02, 17.75it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  90% 303/337 [00:12<00:01, 18.32it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  91% 305/337 [00:12<00:01, 17.71it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  91% 307/337 [00:12<00:01, 18.14it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  92% 309/337 [00:12<00:01, 16.23it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  92% 311/337 [00:12<00:01, 16.74it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  93% 313/337 [00:12<00:01, 16.29it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  93% 315/337 [00:13<00:01, 15.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  94% 317/337 [00:13<00:01, 15.84it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  95% 319/337 [00:13<00:01, 15.81it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  95% 321/337 [00:13<00:01, 15.29it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  96% 323/337 [00:13<00:00, 15.30it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  96% 325/337 [00:13<00:00, 14.80it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  97% 327/337 [00:13<00:00, 14.21it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  98% 329/337 [00:14<00:00, 14.19it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  98% 331/337 [00:14<00:00, 13.56it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  99% 333/337 [00:14<00:00, 12.27it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  99% 335/337 [00:14<00:00, 11.67it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset: 100% 337/337 [00:14<00:00, 12.98it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 11:18:42 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.431 | ppl 86.28 | wps 68391.8 | wpb 2965.1 | bsz 127.6 | num_updates 23166 | best_loss 5.709\n","2022-05-26 11:18:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23166 updates\n","2022-05-26 11:18:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_en_de/checkpoint_last.pt\n","2022-05-26 11:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_en_de/checkpoint_last.pt\n","2022-05-26 11:18:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_en_de/checkpoint_last.pt (epoch 15 @ 23166 updates, score 6.431) (writing took 3.5473199030002434 seconds)\n","2022-05-26 11:18:45 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2022-05-26 11:18:45 | INFO | train | epoch 015 | loss 2.093 | ppl 4.27 | wps 21742.9 | ups 7.34 | wpb 2960.9 | bsz 128 | num_updates 23166 | lr 0.0005 | gnorm 1.316 | loss_scale 8 | train_wall 185 | gb_free 10.9 | wall 634\n","2022-05-26 11:18:45 | INFO | fairseq_cli.train | done training in 633.0 seconds\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz ▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free █▁█\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm ▁▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss █▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss_scale ██▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr ▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl █▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall ▁▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups ██▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall ▁▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb ▁▄█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps ▁▆█\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz █████████▁████▁██████████████▁██████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free ▄▄▆▆▁▄▇▇▅▄▇▄▅▃▆▇▂▇▄▇█▅▄▇▅▆▄▅▇▇▅▆▂▅▆▄█▅█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm ▂▂▁▂▃▂▅▇▇▅▆▇▇█▂▂▃▅▃▄▆▅▆▆▅▇▆▃▆▄▄▄▄▆▆▅▆▆▅█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss ▄▆▃▆▅▃▄█▃▆█▇▇█▂▂▃▄▄▂▅▄▆▅▇▅▅▁▃▃▄▂▃▄▂▃▅▃▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/loss_scale ███████████▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl ▄▅▃▆▄▃▄█▃▆█▇▇█▂▂▃▃▄▂▄▄▆▅▆▅▅▁▂▃▄▂▃▄▂▃▄▃▂▆\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall ▁▇▇█▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▆▇▇█▇▇█▆▇▇▆▆▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups ▆▇▇▆▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇▇▆▇▇▇█▇▇██▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb ▆▇▆█▅▃▄▆▁▆▅▅▅▄▅▅▅▆▆▃▅▄▇▆▇▄▅▃▅▅█▅▆▆▃▆▅▅▄▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps ▇█████▇█▇█▇▇▇▇██▇▇▇▇▇▇███▇█▁▇▇█▇██▇████▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss ▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz ▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss ▁█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl ▁█▆\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb ▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps ▇█▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz 128.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free 10.9\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm 1.316\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss 2.093\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss_scale 8.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl 4.27\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall 185.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups 7.34\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall 634.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb 2960.9\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps 21742.9\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz 128.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free 8.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm 1.381\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss 2.206\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/loss_scale 8.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl 4.61\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall 12.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups 8.14\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall 607.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb 2995.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps 24368.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss 5.709\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz 127.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss 6.431\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl 86.28\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb 2965.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps 68391.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoints_en_de\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation/runs/nu2qq83u\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220526_110812-nu2qq83u/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["# evaluate baseline model\n","! fairseq-generate ./data/binarized/ua.tokenized.en-de \\\n","    --path checkpoints_en_de/checkpoint_best.pt \\\n","    --batch-size 256 \\\n","    --beam 5 \\\n","    --seed 1 \\\n","    --wandb-project \"en-de-backtranslation\" \\\n","    --results-path saved \\\n","    --scoring bleu\n","\n","!grep ^H ./saved/generate-test.txt | cut -f3- > gen.out.sys\n","!grep ^T ./saved/generate-test.txt | cut -f2- > gen.out.ref\n","\n","!fairseq-score --sys gen.out.sys --ref gen.out.ref"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PiKiEvCrcwb","executionInfo":{"status":"ok","timestamp":1653564480198,"user_tz":-120,"elapsed":547462,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"4836fdbb-2617-4084-9c1f-d1bb4e4699c1"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 11:18:54 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 11:18:56 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'en-de-backtranslation', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints_en_de/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'saved'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': './data/binarized/ua.tokenized.en-de', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2022-05-26 11:18:56 | INFO | fairseq.tasks.translation | [en] dictionary: 73392 types\n","2022-05-26 11:18:56 | INFO | fairseq.tasks.translation | [de] dictionary: 102504 types\n","2022-05-26 11:18:56 | INFO | fairseq_cli.generate | loading model(s) from checkpoints_en_de/checkpoint_best.pt\n","2022-05-26 11:18:58 | INFO | fairseq.data.data_utils | loaded 43,291 examples from: ./data/binarized/ua.tokenized.en-de/test.en-de.en\n","2022-05-26 11:18:58 | INFO | fairseq.data.data_utils | loaded 43,291 examples from: ./data/binarized/ua.tokenized.en-de/test.en-de.de\n","2022-05-26 11:18:58 | INFO | fairseq.tasks.translation | ./data/binarized/ua.tokenized.en-de test en-de 43291 examples\n","2022-05-26 11:27:46 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n","2022-05-26 11:27:46 | INFO | fairseq_cli.generate | Translated 43,291 sentences (851,529 tokens) in 342.0s (126.60 sentences/s, 2490.15 tokens/s)\n","2022-05-26 11:27:49 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","Namespace(ignore_case=False, order=4, ref='gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='gen.out.sys')\n","BLEU4 = 8.41, 37.0/14.3/6.6/3.1 (BP=0.826, ratio=0.839, syslen=808238, reflen=963015)\n"]}]},{"cell_type":"markdown","source":["#### wytrenuj model en-de jeszcze raz po manualnym dodaniu backtranslacji wygenerowanychh przez model de-en do zbioru unaugmented"],"metadata":{"id":"ln3Gu7r9r9n3"}},{"cell_type":"code","source":["!grep ^S ./saveddeen/generate-train.txt | cut -f2- > train_src.txt \n","!grep ^H ./saveddeen/generate-train.txt | cut -f3- > train_hyps.txt\n","!grep ^T ./saveddeen/generate-train.txt | cut -f2- > train_trgs.txt"],"metadata":{"id":"M6JECPJ4IyZ6","executionInfo":{"status":"ok","timestamp":1653564754654,"user_tz":-120,"elapsed":1342,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["!cat train_hyps.txt >> ./en-de-backtranslation/data/datasets/unaugmented/original.train.en\n","!cat train_src.txt >> ./en-de-backtranslation/data/datasets/unaugmented/original.train.de"],"metadata":{"id":"GrxX-E-oJDL9","executionInfo":{"status":"ok","timestamp":1653564891031,"user_tz":-120,"elapsed":378,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["!cat ./en-de-backtranslation/data/datasets/unaugmented/original.train.de | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpuh6FpzJe-1","executionInfo":{"status":"ok","timestamp":1653564908784,"user_tz":-120,"elapsed":231,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"41387829-2aef-4217-9370-bbb96f4e1fcc"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["399999\n"]}]},{"cell_type":"code","source":["!cat ./en-de-backtranslation/data/datasets/unaugmented/original.train.de | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vlPx-6jkJiHe","executionInfo":{"status":"ok","timestamp":1653564928004,"user_tz":-120,"elapsed":228,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"ec7dfbab-064b-40bf-cd5a-f30f17f8ba0f"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["399999\n"]}]},{"cell_type":"code","source":["!fairseq-preprocess --source-lang en --target-lang de \\\n","  --trainpref ./en-de-backtranslation/data/datasets/unaugmented/original.train \\\n","  --validpref ./en-de-backtranslation/data/datasets/unaugmented/original.val  \\\n","  --testpref  ./en-de-backtranslation/data/datasets/unaugmented/original.test  \\\n","  --destdir ./data/binarized/augmented.tokenized.en-de \\\n","  --thresholdsrc 2 \\\n","  --thresholdtgt 2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tJ4Lnencr8d1","executionInfo":{"status":"ok","timestamp":1653565102782,"user_tz":-120,"elapsed":172685,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"8bca1e95-c635-488e-a23a-dbc8e4cfc2bb"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 11:35:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 11:35:31 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='./data/binarized/augmented.tokenized.en-de', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref='./en-de-backtranslation/data/datasets/unaugmented/original.test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=2, thresholdtgt=2, tokenizer=None, tpu=False, trainpref='./en-de-backtranslation/data/datasets/unaugmented/original.train', use_plasma_view=False, user_dir=None, validpref='./en-de-backtranslation/data/datasets/unaugmented/original.val', wandb_project=None, workers=1)\n","2022-05-26 11:36:15 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 11:37:04 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.train.en: 399999 sents, 8855235 tokens, 0.836% replaced (by <unk>)\n","2022-05-26 11:37:04 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 11:37:10 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.val.en: 42989 sents, 974477 tokens, 2.91% replaced (by <unk>)\n","2022-05-26 11:37:10 | INFO | fairseq_cli.preprocess | [en] Dictionary: 73392 types\n","2022-05-26 11:37:16 | INFO | fairseq_cli.preprocess | [en] ./en-de-backtranslation/data/datasets/unaugmented/original.test.en: 43291 sents, 982864 tokens, 2.96% replaced (by <unk>)\n","2022-05-26 11:37:16 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 11:38:10 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.train.de: 399999 sents, 9347792 tokens, 1.46% replaced (by <unk>)\n","2022-05-26 11:38:10 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 11:38:16 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.val.de: 42989 sents, 999226 tokens, 4.67% replaced (by <unk>)\n","2022-05-26 11:38:16 | INFO | fairseq_cli.preprocess | [de] Dictionary: 102504 types\n","2022-05-26 11:38:22 | INFO | fairseq_cli.preprocess | [de] ./en-de-backtranslation/data/datasets/unaugmented/original.test.de: 43291 sents, 1006306 tokens, 4.87% replaced (by <unk>)\n","2022-05-26 11:38:22 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to ./data/binarized/augmented.tokenized.en-de\n"]}]},{"cell_type":"code","source":["# train en-de model with backtranslation\n","!CHECKPOINT_DIR=checkpoints_en_de_with_backtranslations && fairseq-train ./data/binarized/augmented.tokenized.en-de --fp16\\\n","  --arch transformer \\\n","  --source-lang en --target-lang de \\\n","  --dropout 0.1 \\\n","  --attention-dropout 0.1 \\\n","  --activation-dropout 0.1 \\\n","  --encoder-embed-dim 256 \\\n","  --encoder-ffn-embed-dim 512 \\\n","  --encoder-layers 3 \\\n","  --encoder-attention-heads 8 \\\n","  --encoder-learned-pos \\\n","  --decoder-embed-dim 256 \\\n","  --decoder-ffn-embed-dim 512 \\\n","  --decoder-layers 3 \\\n","  --decoder-attention-heads 8 \\\n","  --no-epoch-checkpoints \\\n","  --decoder-learned-pos \\\n","  --max-epoch 15\\\n","  --optimizer adam \\\n","  --lr 5e-4 \\\n","  --batch-size 128 \\\n","  --seed 1 \\\n","  --wandb-project \"en-de-backtranslation\" \\\n","  --save-dir $CHECKPOINT_DIR \\\n","  #--finetune-from-model ./checkpoints_de_en_backtranslation /checkpoint_last.pt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnIZWLlmDvNt","executionInfo":{"status":"ok","timestamp":1653571070026,"user_tz":-120,"elapsed":5947078,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"3a0cf875-06d4-4dc4-88f9-dcf5a5041ebd"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      48    |   22693    |   22685    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      44    |   15506 K  |   15506 K  |\n","|       from large pool |      18    |      19    |    7609 K  |    7609 K  |\n","|       from small pool |      10    |      36    |    7896 K  |    7896 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 12:59:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  24% 746/3126 [01:30<04:58,  7.97it/s, loss=1.845, ppl=3.59, wps=24905.1, ups=7.96, wpb=3129.4, bsz=128, num_updates=37700, lr=0.0005, gnorm=1.156, loss_scale=8, train_wall=12, gb_free=12.3, wall=4832]2022-05-26 12:59:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.98 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 12:59:34 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 462          |        cudaMalloc retries: 526       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9199 MB |   12646 MB |  197517 GB |  197508 GB |\n","|       from large pool |    9189 MB |   12636 MB |  193329 GB |  193320 GB |\n","|       from small pool |      10 MB |      17 MB |    4188 GB |    4188 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9199 MB |   12646 MB |  197517 GB |  197508 GB |\n","|       from large pool |    9189 MB |   12636 MB |  193329 GB |  193320 GB |\n","|       from small pool |      10 MB |      17 MB |    4188 GB |    4188 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13322 MB |  544374 MB |  531146 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     110 MB |   45480 MB |   45464 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4028 MB |    4518 MB |  393455 GB |  393451 GB |\n","|       from large pool |    4022 MB |    4512 MB |  388650 GB |  388646 GB |\n","|       from small pool |       5 MB |      11 MB |    4804 GB |    4804 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31027 K  |   31027 K  |\n","|       from large pool |     142    |     147    |   13214 K  |   13214 K  |\n","|       from small pool |     184    |     268    |   17813 K  |   17813 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31027 K  |   31027 K  |\n","|       from large pool |     142    |     147    |   13214 K  |   13214 K  |\n","|       from small pool |     184    |     268    |   17813 K  |   17813 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      64    |   22930    |   22913    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      55    |   22740    |   22732    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      32    |   15518 K  |   15518 K  |\n","|       from large pool |      16    |      17    |    7615 K  |    7615 K  |\n","|       from small pool |      12    |      24    |    7903 K  |    7903 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 12:59:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  37% 1141/3126 [02:16<03:48,  8.68it/s, loss=1.849, ppl=3.6, wps=23662.2, ups=8.42, wpb=2808.6, bsz=128, num_updates=38100, lr=0.0005, gnorm=1.217, loss_scale=8, train_wall=12, gb_free=11, wall=4879]2022-05-26 13:00:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.46 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:00:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 463          |        cudaMalloc retries: 527       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6610 MB |    8875 MB |  199364 GB |  199357 GB |\n","|       from large pool |    6600 MB |    8865 MB |  195130 GB |  195123 GB |\n","|       from small pool |      10 MB |      17 MB |    4233 GB |    4233 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6610 MB |    8875 MB |  199364 GB |  199357 GB |\n","|       from large pool |    6600 MB |    8865 MB |  195130 GB |  195123 GB |\n","|       from small pool |      10 MB |      17 MB |    4233 GB |    4233 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  544472 MB |  531244 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   45578 MB |   45562 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6617 MB |    6617 MB |  397286 GB |  397280 GB |\n","|       from large pool |    6611 MB |    6611 MB |  392430 GB |  392423 GB |\n","|       from small pool |       5 MB |      21 MB |    4856 GB |    4856 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31338 K  |   31338 K  |\n","|       from large pool |     142    |     147    |   13340 K  |   13340 K  |\n","|       from small pool |     184    |     268    |   17998 K  |   17997 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31338 K  |   31338 K  |\n","|       from large pool |     142    |     147    |   13340 K  |   13340 K  |\n","|       from small pool |     184    |     268    |   17998 K  |   17997 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   22979    |   22962    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   22789    |   22781    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      46    |   15674 K  |   15674 K  |\n","|       from large pool |      17    |      18    |    7688 K  |    7688 K  |\n","|       from small pool |      10    |      38    |    7986 K  |    7986 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:00:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  43% 1343/3126 [02:41<03:34,  8.31it/s, loss=1.882, ppl=3.69, wps=24780.6, ups=8.32, wpb=2978.2, bsz=128, num_updates=38300, lr=0.0005, gnorm=1.204, loss_scale=8, train_wall=12, gb_free=10.2, wall=4903]2022-05-26 13:00:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:00:45 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 464          |        cudaMalloc retries: 528       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6371 MB |    8511 MB |  200373 GB |  200367 GB |\n","|       from large pool |    6361 MB |    8500 MB |  196118 GB |  196112 GB |\n","|       from small pool |      10 MB |      17 MB |    4254 GB |    4254 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6371 MB |    8511 MB |  200373 GB |  200367 GB |\n","|       from large pool |    6361 MB |    8500 MB |  196118 GB |  196112 GB |\n","|       from small pool |      10 MB |      17 MB |    4254 GB |    4254 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  544570 MB |  531342 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   45676 MB |   45660 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6856 MB |    6856 MB |  399332 GB |  399326 GB |\n","|       from large pool |    6850 MB |    6850 MB |  394452 GB |  394445 GB |\n","|       from small pool |       5 MB |      12 MB |    4880 GB |    4880 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31498 K  |   31497 K  |\n","|       from large pool |     142    |     147    |   13407 K  |   13407 K  |\n","|       from small pool |     184    |     268    |   18090 K  |   18090 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31498 K  |   31497 K  |\n","|       from large pool |     142    |     147    |   13407 K  |   13407 K  |\n","|       from small pool |     184    |     268    |   18090 K  |   18090 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23028    |   23011    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   22838    |   22830    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      30    |   15753 K  |   15753 K  |\n","|       from large pool |      12    |      13    |    7726 K  |    7726 K  |\n","|       from small pool |       9    |      21    |    8026 K  |    8026 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:00:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  47% 1456/3126 [02:54<03:34,  7.77it/s, loss=1.892, ppl=3.71, wps=23667.9, ups=8.17, wpb=2898.6, bsz=128, num_updates=38400, lr=0.0005, gnorm=1.261, loss_scale=8, train_wall=12, gb_free=10.6, wall=4915]2022-05-26 13:00:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.28 GiB already allocated; 613.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:00:58 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 465          |        cudaMalloc retries: 529       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7453 MB |   10120 MB |  200911 GB |  200904 GB |\n","|       from large pool |    7443 MB |   10110 MB |  196644 GB |  196637 GB |\n","|       from small pool |      10 MB |      17 MB |    4266 GB |    4266 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7453 MB |   10120 MB |  200911 GB |  200904 GB |\n","|       from large pool |    7443 MB |   10110 MB |  196644 GB |  196637 GB |\n","|       from small pool |      10 MB |      17 MB |    4266 GB |    4266 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13230 MB |   13326 MB |  544668 MB |  531438 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      18 MB |     114 MB |   45774 MB |   45756 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5776 MB |    5776 MB |  400454 GB |  400448 GB |\n","|       from large pool |    5768 MB |    5768 MB |  395560 GB |  395554 GB |\n","|       from small pool |       7 MB |       9 MB |    4893 GB |    4893 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31586 K  |   31586 K  |\n","|       from large pool |     142    |     147    |   13446 K  |   13446 K  |\n","|       from small pool |     184    |     268    |   18140 K  |   18140 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31586 K  |   31586 K  |\n","|       from large pool |     142    |     147    |   13446 K  |   13446 K  |\n","|       from small pool |     184    |     268    |   18140 K  |   18140 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      18    |      66    |   23077    |   23059    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       9    |      57    |   22887    |   22878    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      32    |   15797 K  |   15797 K  |\n","|       from large pool |      13    |      14    |    7748 K  |    7748 K  |\n","|       from small pool |      10    |      24    |    8049 K  |    8049 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:00:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  48% 1496/3126 [03:00<04:14,  6.40it/s, loss=1.983, ppl=3.95, wps=24857, ups=8.26, wpb=3010, bsz=128, num_updates=38500, lr=0.0005, gnorm=1.237, loss_scale=8, train_wall=12, gb_free=4.9, wall=4927]2022-05-26 13:01:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.22 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:03 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 466          |        cudaMalloc retries: 530       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7393 MB |   10060 MB |  201141 GB |  201134 GB |\n","|       from large pool |    7383 MB |   10049 MB |  196870 GB |  196863 GB |\n","|       from small pool |      10 MB |      17 MB |    4270 GB |    4270 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7393 MB |   10060 MB |  201141 GB |  201134 GB |\n","|       from large pool |    7383 MB |   10049 MB |  196870 GB |  196863 GB |\n","|       from small pool |      10 MB |      17 MB |    4270 GB |    4270 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13320 MB |  544758 MB |  531530 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     108 MB |   45864 MB |   45848 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5834 MB |    5834 MB |  400902 GB |  400896 GB |\n","|       from large pool |    5828 MB |    5828 MB |  396004 GB |  395998 GB |\n","|       from small pool |       5 MB |      43 MB |    4898 GB |    4898 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31619 K  |   31619 K  |\n","|       from large pool |     142    |     146    |   13460 K  |   13460 K  |\n","|       from small pool |     184    |     268    |   18158 K  |   18158 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31619 K  |   31619 K  |\n","|       from large pool |     142    |     146    |   13460 K  |   13460 K  |\n","|       from small pool |     184    |     268    |   18158 K  |   18158 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      63    |   23122    |   23105    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      54    |   22932    |   22924    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      56    |   15813 K  |   15813 K  |\n","|       from large pool |      12    |      13    |    7757 K  |    7757 K  |\n","|       from small pool |      10    |      48    |    8056 K  |    8056 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  48% 1511/3126 [03:02<03:11,  8.45it/s, loss=1.983, ppl=3.95, wps=24857, ups=8.26, wpb=3010, bsz=128, num_updates=38500, lr=0.0005, gnorm=1.237, loss_scale=8, train_wall=12, gb_free=4.9, wall=4927]2022-05-26 13:01:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.87 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:05 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 467          |        cudaMalloc retries: 531       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7031 MB |    9422 MB |  201214 GB |  201207 GB |\n","|       from large pool |    7020 MB |    9412 MB |  196941 GB |  196934 GB |\n","|       from small pool |      10 MB |      17 MB |    4272 GB |    4272 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7031 MB |    9422 MB |  201214 GB |  201207 GB |\n","|       from large pool |    7020 MB |    9412 MB |  196941 GB |  196934 GB |\n","|       from small pool |      10 MB |      17 MB |    4272 GB |    4272 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  544856 MB |  531628 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   45962 MB |   45946 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6196 MB |    6196 MB |  401047 GB |  401041 GB |\n","|       from large pool |    6191 MB |    6191 MB |  396147 GB |  396141 GB |\n","|       from small pool |       5 MB |      19 MB |    4900 GB |    4900 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31630 K  |   31629 K  |\n","|       from large pool |     142    |     147    |   13465 K  |   13465 K  |\n","|       from small pool |     184    |     268    |   18164 K  |   18164 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31630 K  |   31629 K  |\n","|       from large pool |     142    |     147    |   13465 K  |   13465 K  |\n","|       from small pool |     184    |     268    |   18164 K  |   18164 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23171    |   23154    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   22981    |   22973    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      45    |   15819 K  |   15819 K  |\n","|       from large pool |      19    |      20    |    7759 K  |    7759 K  |\n","|       from small pool |      10    |      37    |    8059 K  |    8059 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  52% 1613/3126 [03:14<03:18,  7.64it/s, loss=1.939, ppl=3.83, wps=24980.3, ups=7.78, wpb=3211.8, bsz=128, num_updates=38600, lr=0.0005, gnorm=1.196, loss_scale=8, train_wall=12, gb_free=11.5, wall=4940]2022-05-26 13:01:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.83 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:17 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 468          |        cudaMalloc retries: 532       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8020 MB |   10889 MB |  201712 GB |  201705 GB |\n","|       from large pool |    8010 MB |   10879 MB |  197430 GB |  197422 GB |\n","|       from small pool |      10 MB |      17 MB |    4282 GB |    4282 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8020 MB |   10889 MB |  201712 GB |  201705 GB |\n","|       from large pool |    8010 MB |   10879 MB |  197430 GB |  197422 GB |\n","|       from small pool |      10 MB |      17 MB |    4282 GB |    4282 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  544954 MB |  531726 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46060 MB |   46044 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5207 MB |    5207 MB |  402079 GB |  402074 GB |\n","|       from large pool |    5201 MB |    5201 MB |  397167 GB |  397162 GB |\n","|       from small pool |       5 MB |       7 MB |    4911 GB |    4911 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31710 K  |   31710 K  |\n","|       from large pool |     142    |     147    |   13501 K  |   13500 K  |\n","|       from small pool |     184    |     268    |   18209 K  |   18209 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31710 K  |   31710 K  |\n","|       from large pool |     142    |     147    |   13501 K  |   13500 K  |\n","|       from small pool |     184    |     268    |   18209 K  |   18209 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23220    |   23203    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23030    |   23022    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      29    |   15859 K  |   15859 K  |\n","|       from large pool |      17    |      18    |    7780 K  |    7780 K  |\n","|       from small pool |       9    |      20    |    8079 K  |    8079 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  52% 1626/3126 [03:15<03:12,  7.80it/s, loss=1.939, ppl=3.83, wps=24980.3, ups=7.78, wpb=3211.8, bsz=128, num_updates=38600, lr=0.0005, gnorm=1.196, loss_scale=8, train_wall=12, gb_free=11.5, wall=4940]2022-05-26 13:01:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.96 GiB already allocated; 609.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:19 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 469          |        cudaMalloc retries: 533       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7124 MB |    9690 MB |  201778 GB |  201771 GB |\n","|       from large pool |    7104 MB |    9669 MB |  197494 GB |  197487 GB |\n","|       from small pool |      20 MB |      20 MB |    4283 GB |    4283 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7124 MB |    9690 MB |  201778 GB |  201771 GB |\n","|       from large pool |    7104 MB |    9669 MB |  197494 GB |  197487 GB |\n","|       from small pool |      20 MB |      20 MB |    4283 GB |    4283 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13234 MB |   13306 MB |  545032 MB |  531798 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      22 MB |      94 MB |   46138 MB |   46116 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6109 MB |    6109 MB |  402204 GB |  402198 GB |\n","|       from large pool |    6107 MB |    6107 MB |  397291 GB |  397285 GB |\n","|       from small pool |       1 MB |      15 MB |    4913 GB |    4912 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31720 K  |   31719 K  |\n","|       from large pool |     132    |     136    |   13505 K  |   13505 K  |\n","|       from small pool |     194    |     268    |   18214 K  |   18214 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31720 K  |   31719 K  |\n","|       from large pool |     132    |     136    |   13505 K  |   13505 K  |\n","|       from small pool |     194    |     268    |   18214 K  |   18214 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      20    |      56    |   23259    |   23239    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      11    |      47    |   23069    |   23058    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      41    |   15864 K  |   15864 K  |\n","|       from large pool |      10    |      12    |    7783 K  |    7783 K  |\n","|       from small pool |      12    |      35    |    8081 K  |    8081 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  53% 1655/3126 [03:19<02:53,  8.46it/s, loss=1.939, ppl=3.83, wps=24980.3, ups=7.78, wpb=3211.8, bsz=128, num_updates=38600, lr=0.0005, gnorm=1.196, loss_scale=8, train_wall=12, gb_free=11.5, wall=4940]2022-05-26 13:01:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.31 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:22 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 470          |        cudaMalloc retries: 534       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6463 MB |    8727 MB |  201913 GB |  201907 GB |\n","|       from large pool |    6453 MB |    8716 MB |  197626 GB |  197620 GB |\n","|       from small pool |       9 MB |      17 MB |    4287 GB |    4287 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6463 MB |    8727 MB |  201913 GB |  201907 GB |\n","|       from large pool |    6453 MB |    8716 MB |  197626 GB |  197620 GB |\n","|       from small pool |       9 MB |      17 MB |    4287 GB |    4287 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545124 MB |  531896 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46230 MB |   46214 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6764 MB |    6764 MB |  402484 GB |  402477 GB |\n","|       from large pool |    6758 MB |    6758 MB |  397567 GB |  397560 GB |\n","|       from small pool |       6 MB |      38 MB |    4916 GB |    4916 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31742 K  |   31742 K  |\n","|       from large pool |     142    |     146    |   13514 K  |   13514 K  |\n","|       from small pool |     184    |     268    |   18227 K  |   18227 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31742 K  |   31742 K  |\n","|       from large pool |     142    |     146    |   13514 K  |   13514 K  |\n","|       from small pool |     184    |     268    |   18227 K  |   18227 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23305    |   23288    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23115    |   23107    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      18    |      58    |   15875 K  |   15875 K  |\n","|       from large pool |       8    |       9    |    7788 K  |    7788 K  |\n","|       from small pool |      10    |      50    |    8087 K  |    8087 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  57% 1793/3126 [03:36<02:35,  8.59it/s, loss=2.034, ppl=4.1, wps=25714.5, ups=8.17, wpb=3146.5, bsz=128, num_updates=38800, lr=0.0005, gnorm=1.226, loss_scale=8, train_wall=12, gb_free=10.1, wall=4964]2022-05-26 13:01:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.78 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:39 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 471          |        cudaMalloc retries: 535       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8989 MB |   12335 MB |  202627 GB |  202618 GB |\n","|       from large pool |    8978 MB |   12325 MB |  198327 GB |  198318 GB |\n","|       from small pool |      10 MB |      17 MB |    4300 GB |    4300 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8989 MB |   12335 MB |  202627 GB |  202618 GB |\n","|       from large pool |    8978 MB |   12325 MB |  198327 GB |  198318 GB |\n","|       from small pool |      10 MB |      17 MB |    4300 GB |    4300 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545222 MB |  531994 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46328 MB |   46312 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4238 MB |    4517 MB |  403938 GB |  403934 GB |\n","|       from large pool |    4233 MB |    4510 MB |  399007 GB |  399003 GB |\n","|       from small pool |       5 MB |       7 MB |    4931 GB |    4931 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31851 K  |   31851 K  |\n","|       from large pool |     142    |     147    |   13563 K  |   13563 K  |\n","|       from small pool |     184    |     268    |   18288 K  |   18288 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31851 K  |   31851 K  |\n","|       from large pool |     142    |     147    |   13563 K  |   13563 K  |\n","|       from small pool |     184    |     268    |   18288 K  |   18288 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23354    |   23337    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23164    |   23156    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      27    |   15929 K  |   15929 K  |\n","|       from large pool |      14    |      15    |    7816 K  |    7816 K  |\n","|       from small pool |      11    |      20    |    8113 K  |    8113 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  59% 1851/3126 [03:42<02:28,  8.57it/s, loss=2.034, ppl=4.1, wps=25714.5, ups=8.17, wpb=3146.5, bsz=128, num_updates=38800, lr=0.0005, gnorm=1.226, loss_scale=8, train_wall=12, gb_free=10.1, wall=4964]2022-05-26 13:01:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:01:46 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 472          |        cudaMalloc retries: 536       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6372 MB |    8537 MB |  202899 GB |  202893 GB |\n","|       from large pool |    6362 MB |    8527 MB |  198592 GB |  198586 GB |\n","|       from small pool |      10 MB |      17 MB |    4307 GB |    4307 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6372 MB |    8537 MB |  202899 GB |  202893 GB |\n","|       from large pool |    6362 MB |    8527 MB |  198592 GB |  198586 GB |\n","|       from small pool |      10 MB |      17 MB |    4307 GB |    4307 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545320 MB |  532092 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46426 MB |   46410 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6855 MB |    6855 MB |  404489 GB |  404483 GB |\n","|       from large pool |    6849 MB |    6849 MB |  399550 GB |  399543 GB |\n","|       from small pool |       5 MB |      26 MB |    4939 GB |    4939 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   31897 K  |   31897 K  |\n","|       from large pool |     142    |     147    |   13581 K  |   13581 K  |\n","|       from small pool |     184    |     268    |   18316 K  |   18315 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   31897 K  |   31897 K  |\n","|       from large pool |     142    |     147    |   13581 K  |   13581 K  |\n","|       from small pool |     184    |     268    |   18316 K  |   18315 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23403    |   23386    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23213    |   23205    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      46    |   15953 K  |   15953 K  |\n","|       from large pool |      19    |      20    |    7826 K  |    7826 K  |\n","|       from small pool |       8    |      39    |    8126 K  |    8126 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:01:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  67% 2089/3126 [04:10<02:02,  8.49it/s, loss=1.849, ppl=3.6, wps=24446.6, ups=8.78, wpb=2784.1, bsz=128, num_updates=39100, lr=0.0005, gnorm=1.234, loss_scale=8, train_wall=11, gb_free=10.6, wall=5000]2022-05-26 13:02:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 7.02 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:02:14 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 473          |        cudaMalloc retries: 537       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7193 MB |    9710 MB |  204030 GB |  204023 GB |\n","|       from large pool |    7183 MB |    9699 MB |  199697 GB |  199690 GB |\n","|       from small pool |      10 MB |      17 MB |    4332 GB |    4332 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7193 MB |    9710 MB |  204030 GB |  204023 GB |\n","|       from large pool |    7183 MB |    9699 MB |  199697 GB |  199690 GB |\n","|       from small pool |      10 MB |      17 MB |    4332 GB |    4332 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545418 MB |  532190 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46524 MB |   46508 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6034 MB |    6034 MB |  406805 GB |  406800 GB |\n","|       from large pool |    6028 MB |    6028 MB |  401837 GB |  401831 GB |\n","|       from small pool |       5 MB |      22 MB |    4968 GB |    4968 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32084 K  |   32084 K  |\n","|       from large pool |     142    |     147    |   13660 K  |   13659 K  |\n","|       from small pool |     184    |     268    |   18424 K  |   18424 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32084 K  |   32084 K  |\n","|       from large pool |     142    |     147    |   13660 K  |   13659 K  |\n","|       from small pool |     184    |     268    |   18424 K  |   18424 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23452    |   23435    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23262    |   23254    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      47    |   16047 K  |   16047 K  |\n","|       from large pool |      19    |      20    |    7872 K  |    7871 K  |\n","|       from small pool |       9    |      39    |    8175 K  |    8175 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:02:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  71% 2223/3126 [04:27<01:39,  9.09it/s, loss=1.976, ppl=3.93, wps=24819.6, ups=7.94, wpb=3124.9, bsz=128, num_updates=39200, lr=0.0005, gnorm=1.241, loss_scale=8, train_wall=12, gb_free=9.3, wall=5012]2022-05-26 13:02:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.69 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:02:31 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 474          |        cudaMalloc retries: 538       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7877 MB |   10546 MB |  204711 GB |  204703 GB |\n","|       from large pool |    7866 MB |   10535 MB |  200363 GB |  200355 GB |\n","|       from small pool |      10 MB |      17 MB |    4348 GB |    4348 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7877 MB |   10546 MB |  204711 GB |  204703 GB |\n","|       from large pool |    7866 MB |   10535 MB |  200363 GB |  200355 GB |\n","|       from small pool |      10 MB |      17 MB |    4348 GB |    4348 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545516 MB |  532288 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46622 MB |   46606 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5350 MB |    5350 MB |  408162 GB |  408157 GB |\n","|       from large pool |    5345 MB |    5345 MB |  403177 GB |  403171 GB |\n","|       from small pool |       5 MB |      28 MB |    4985 GB |    4985 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32190 K  |   32190 K  |\n","|       from large pool |     142    |     147    |   13704 K  |   13704 K  |\n","|       from small pool |     184    |     268    |   18486 K  |   18486 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32190 K  |   32190 K  |\n","|       from large pool |     142    |     147    |   13704 K  |   13704 K  |\n","|       from small pool |     184    |     268    |   18486 K  |   18486 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23501    |   23484    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23311    |   23303    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      54    |   16099 K  |   16099 K  |\n","|       from large pool |      20    |      21    |    7896 K  |    7896 K  |\n","|       from small pool |      10    |      46    |    8202 K  |    8202 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:02:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  74% 2308/3126 [04:37<01:35,  8.55it/s, loss=1.979, ppl=3.94, wps=23340.3, ups=8.21, wpb=2843.5, bsz=127.9, num_updates=39300, lr=0.0005, gnorm=1.326, loss_scale=8, train_wall=12, gb_free=12.8, wall=5025]2022-05-26 13:02:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.13 GiB (GPU 0; 14.76 GiB total capacity; 7.27 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:02:41 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 475          |        cudaMalloc retries: 539       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7441 MB |   10084 MB |  205145 GB |  205137 GB |\n","|       from large pool |    7431 MB |   10074 MB |  200788 GB |  200781 GB |\n","|       from small pool |      10 MB |      17 MB |    4356 GB |    4356 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7441 MB |   10084 MB |  205145 GB |  205137 GB |\n","|       from large pool |    7431 MB |   10074 MB |  200788 GB |  200781 GB |\n","|       from small pool |      10 MB |      17 MB |    4356 GB |    4356 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545614 MB |  532386 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46720 MB |   46704 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5786 MB |    5786 MB |  409042 GB |  409036 GB |\n","|       from large pool |    5780 MB |    5780 MB |  404047 GB |  404041 GB |\n","|       from small pool |       5 MB |      40 MB |    4994 GB |    4994 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32257 K  |   32256 K  |\n","|       from large pool |     142    |     147    |   13733 K  |   13733 K  |\n","|       from small pool |     184    |     268    |   18523 K  |   18523 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32257 K  |   32256 K  |\n","|       from large pool |     142    |     147    |   13733 K  |   13733 K  |\n","|       from small pool |     184    |     268    |   18523 K  |   18523 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23550    |   23533    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23360    |   23352    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      56    |   16132 K  |   16132 K  |\n","|       from large pool |      18    |      19    |    7913 K  |    7913 K  |\n","|       from small pool |       9    |      48    |    8218 K  |    8218 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:02:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  76% 2367/3126 [04:45<01:29,  8.52it/s, loss=1.979, ppl=3.94, wps=23340.3, ups=8.21, wpb=2843.5, bsz=127.9, num_updates=39300, lr=0.0005, gnorm=1.326, loss_scale=8, train_wall=12, gb_free=12.8, wall=5025]2022-05-26 13:02:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.84 GiB (GPU 0; 14.76 GiB total capacity; 6.82 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:02:48 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 476          |        cudaMalloc retries: 540       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6985 MB |    9475 MB |  205449 GB |  205442 GB |\n","|       from large pool |    6974 MB |    9464 MB |  201086 GB |  201079 GB |\n","|       from small pool |      10 MB |      17 MB |    4362 GB |    4362 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6985 MB |    9475 MB |  205449 GB |  205442 GB |\n","|       from large pool |    6974 MB |    9464 MB |  201086 GB |  201079 GB |\n","|       from small pool |      10 MB |      17 MB |    4362 GB |    4362 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545712 MB |  532484 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46818 MB |   46802 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6242 MB |    6242 MB |  409676 GB |  409670 GB |\n","|       from large pool |    6237 MB |    6237 MB |  404674 GB |  404667 GB |\n","|       from small pool |       5 MB |      18 MB |    5002 GB |    5002 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32303 K  |   32303 K  |\n","|       from large pool |     142    |     146    |   13753 K  |   13752 K  |\n","|       from small pool |     184    |     268    |   18550 K  |   18550 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32303 K  |   32303 K  |\n","|       from large pool |     142    |     146    |   13753 K  |   13752 K  |\n","|       from small pool |     184    |     268    |   18550 K  |   18550 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23599    |   23582    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23409    |   23401    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      45    |   16155 K  |   16155 K  |\n","|       from large pool |      11    |      12    |    7924 K  |    7924 K  |\n","|       from small pool |       9    |      37    |    8230 K  |    8230 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:02:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  79% 2464/3126 [04:56<01:24,  7.80it/s, loss=2.02, ppl=4.06, wps=24655.7, ups=7.97, wpb=3093, bsz=128, num_updates=39400, lr=0.0005, gnorm=1.209, loss_scale=8, train_wall=12, gb_free=5.5, wall=5037]2022-05-26 13:03:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.04 GiB (GPU 0; 14.76 GiB total capacity; 7.17 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:00 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 477          |        cudaMalloc retries: 541       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7340 MB |    9932 MB |  205907 GB |  205899 GB |\n","|       from large pool |    7330 MB |    9922 MB |  201535 GB |  201527 GB |\n","|       from small pool |      10 MB |      17 MB |    4371 GB |    4371 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7340 MB |    9932 MB |  205907 GB |  205899 GB |\n","|       from large pool |    7330 MB |    9922 MB |  201535 GB |  201527 GB |\n","|       from small pool |      10 MB |      17 MB |    4371 GB |    4371 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  545810 MB |  532582 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   46916 MB |   46900 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5887 MB |    5887 MB |  410615 GB |  410610 GB |\n","|       from large pool |    5881 MB |    5881 MB |  405603 GB |  405597 GB |\n","|       from small pool |       5 MB |      27 MB |    5012 GB |    5012 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32378 K  |   32378 K  |\n","|       from large pool |     142    |     147    |   13784 K  |   13784 K  |\n","|       from small pool |     184    |     268    |   18593 K  |   18593 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32378 K  |   32378 K  |\n","|       from large pool |     142    |     147    |   13784 K  |   13784 K  |\n","|       from small pool |     184    |     268    |   18593 K  |   18593 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23648    |   23631    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23458    |   23450    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      55    |   16192 K  |   16192 K  |\n","|       from large pool |      17    |      18    |    7943 K  |    7943 K  |\n","|       from small pool |      10    |      47    |    8249 K  |    8249 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:03:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  79% 2483/3126 [04:58<01:08,  9.43it/s, loss=1.993, ppl=3.98, wps=23493.8, ups=8.34, wpb=2815.7, bsz=128, num_updates=39500, lr=0.0005, gnorm=1.324, loss_scale=8, train_wall=11, gb_free=12.4, wall=5049]2022-05-26 13:03:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.43 GiB already allocated; 613.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:02 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 478          |        cudaMalloc retries: 542       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7603 MB |   10271 MB |  205990 GB |  205983 GB |\n","|       from large pool |    7593 MB |   10261 MB |  201616 GB |  201609 GB |\n","|       from small pool |      10 MB |      17 MB |    4374 GB |    4374 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7603 MB |   10271 MB |  205990 GB |  205983 GB |\n","|       from large pool |    7593 MB |   10261 MB |  201616 GB |  201609 GB |\n","|       from small pool |      10 MB |      17 MB |    4374 GB |    4374 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13230 MB |   13316 MB |  545898 MB |  532668 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      18 MB |     104 MB |   47004 MB |   46986 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5626 MB |    5626 MB |  410778 GB |  410772 GB |\n","|       from large pool |    5618 MB |    5618 MB |  405762 GB |  405757 GB |\n","|       from small pool |       7 MB |      19 MB |    5015 GB |    5015 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32393 K  |   32393 K  |\n","|       from large pool |     142    |     147    |   13790 K  |   13790 K  |\n","|       from small pool |     184    |     268    |   18603 K  |   18603 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32393 K  |   32393 K  |\n","|       from large pool |     142    |     147    |   13790 K  |   13790 K  |\n","|       from small pool |     184    |     268    |   18603 K  |   18603 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      18    |      61    |   23692    |   23674    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       9    |      52    |   23502    |   23493    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      45    |   16200 K  |   16200 K  |\n","|       from large pool |      13    |      14    |    7946 K  |    7946 K  |\n","|       from small pool |       9    |      37    |    8253 K  |    8253 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:03:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  80% 2496/3126 [05:00<01:16,  8.19it/s, loss=1.993, ppl=3.98, wps=23493.8, ups=8.34, wpb=2815.7, bsz=128, num_updates=39500, lr=0.0005, gnorm=1.324, loss_scale=8, train_wall=11, gb_free=12.4, wall=5049]2022-05-26 13:03:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.71 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:04 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 479          |        cudaMalloc retries: 543       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6868 MB |    9259 MB |  206058 GB |  206051 GB |\n","|       from large pool |    6858 MB |    9249 MB |  201682 GB |  201675 GB |\n","|       from small pool |      10 MB |      17 MB |    4375 GB |    4375 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6868 MB |    9259 MB |  206058 GB |  206051 GB |\n","|       from large pool |    6858 MB |    9249 MB |  201682 GB |  201675 GB |\n","|       from small pool |      10 MB |      17 MB |    4375 GB |    4375 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13320 MB |  545988 MB |  532760 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     108 MB |   47094 MB |   47078 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6359 MB |    6359 MB |  410905 GB |  410899 GB |\n","|       from large pool |    6353 MB |    6353 MB |  405888 GB |  405882 GB |\n","|       from small pool |       5 MB |      13 MB |    5016 GB |    5016 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32403 K  |   32403 K  |\n","|       from large pool |     142    |     147    |   13795 K  |   13794 K  |\n","|       from small pool |     184    |     268    |   18608 K  |   18608 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32403 K  |   32403 K  |\n","|       from large pool |     142    |     147    |   13795 K  |   13794 K  |\n","|       from small pool |     184    |     268    |   18608 K  |   18608 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      63    |   23737    |   23720    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      54    |   23547    |   23539    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      34    |      38    |   16205 K  |   16205 K  |\n","|       from large pool |      24    |      25    |    7949 K  |    7949 K  |\n","|       from small pool |      10    |      31    |    8256 K  |    8256 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:03:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  84% 2632/3126 [05:16<00:55,  8.93it/s, loss=1.779, ppl=3.43, wps=22991.7, ups=8.46, wpb=2716.7, bsz=128, num_updates=39600, lr=0.0005, gnorm=1.246, loss_scale=8, train_wall=11, gb_free=8.1, wall=5061]2022-05-26 13:03:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 7.01 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 480          |        cudaMalloc retries: 544       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7178 MB |    9744 MB |  206703 GB |  206696 GB |\n","|       from large pool |    7168 MB |    9733 MB |  202313 GB |  202306 GB |\n","|       from small pool |      10 MB |      17 MB |    4390 GB |    4390 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7178 MB |    9744 MB |  206703 GB |  206696 GB |\n","|       from large pool |    7168 MB |    9733 MB |  202313 GB |  202306 GB |\n","|       from small pool |      10 MB |      17 MB |    4390 GB |    4390 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  546086 MB |  532858 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   47192 MB |   47176 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6049 MB |    6049 MB |  412211 GB |  412205 GB |\n","|       from large pool |    6043 MB |    6043 MB |  407178 GB |  407172 GB |\n","|       from small pool |       5 MB |      39 MB |    5033 GB |    5033 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32510 K  |   32510 K  |\n","|       from large pool |     142    |     146    |   13840 K  |   13840 K  |\n","|       from small pool |     184    |     268    |   18669 K  |   18669 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32510 K  |   32510 K  |\n","|       from large pool |     142    |     146    |   13840 K  |   13840 K  |\n","|       from small pool |     184    |     268    |   18669 K  |   18669 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23786    |   23769    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23596    |   23588    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      62    |   16259 K  |   16258 K  |\n","|       from large pool |       9    |      10    |    7975 K  |    7975 K  |\n","|       from small pool |      11    |      54    |    8283 K  |    8283 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:03:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  86% 2703/3126 [05:24<00:44,  9.50it/s, loss=1.979, ppl=3.94, wps=24344.8, ups=8.42, wpb=2891.7, bsz=128, num_updates=39700, lr=0.0005, gnorm=1.336, loss_scale=8, train_wall=11, gb_free=10.6, wall=5073]2022-05-26 13:03:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.91 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:28 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 481          |        cudaMalloc retries: 545       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9122 MB |   12568 MB |  207033 GB |  207024 GB |\n","|       from large pool |    9103 MB |   12549 MB |  202634 GB |  202625 GB |\n","|       from small pool |      18 MB |      19 MB |    4398 GB |    4398 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9122 MB |   12568 MB |  207033 GB |  207024 GB |\n","|       from large pool |    9103 MB |   12549 MB |  202634 GB |  202625 GB |\n","|       from small pool |      18 MB |      19 MB |    4398 GB |    4398 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13326 MB |  546184 MB |  532952 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |     114 MB |   47290 MB |   47270 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4109 MB |    4542 MB |  412873 GB |  412869 GB |\n","|       from large pool |    4108 MB |    4540 MB |  407830 GB |  407826 GB |\n","|       from small pool |       1 MB |      41 MB |    5042 GB |    5042 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32566 K  |   32566 K  |\n","|       from large pool |     132    |     136    |   13864 K  |   13864 K  |\n","|       from small pool |     194    |     268    |   18702 K  |   18701 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32566 K  |   32566 K  |\n","|       from large pool |     132    |     136    |   13864 K  |   13864 K  |\n","|       from small pool |     194    |     268    |   18702 K  |   18701 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      66    |   23835    |   23816    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      57    |   23645    |   23635    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      53    |   16287 K  |   16287 K  |\n","|       from large pool |      13    |      14    |    7989 K  |    7989 K  |\n","|       from small pool |      11    |      45    |    8297 K  |    8297 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:03:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  92% 2884/3126 [05:46<00:31,  7.80it/s, loss=1.934, ppl=3.82, wps=24906.3, ups=8.69, wpb=2865.8, bsz=128, num_updates=39900, lr=0.0005, gnorm=1.246, loss_scale=8, train_wall=11, gb_free=10.9, wall=5097]2022-05-26 13:03:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.50 GiB (GPU 0; 14.76 GiB total capacity; 6.56 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:50 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 482          |        cudaMalloc retries: 546       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6712 MB |    9028 MB |  207929 GB |  207922 GB |\n","|       from large pool |    6702 MB |    9017 MB |  203510 GB |  203503 GB |\n","|       from small pool |      10 MB |      17 MB |    4418 GB |    4418 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6712 MB |    9028 MB |  207929 GB |  207922 GB |\n","|       from large pool |    6702 MB |    9017 MB |  203510 GB |  203503 GB |\n","|       from small pool |      10 MB |      17 MB |    4418 GB |    4418 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  546278 MB |  533050 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   47384 MB |   47368 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6515 MB |    6515 MB |  414692 GB |  414686 GB |\n","|       from large pool |    6509 MB |    6509 MB |  409626 GB |  409620 GB |\n","|       from small pool |       5 MB |      53 MB |    5065 GB |    5065 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32710 K  |   32709 K  |\n","|       from large pool |     142    |     147    |   13924 K  |   13924 K  |\n","|       from small pool |     184    |     268    |   18785 K  |   18785 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32710 K  |   32709 K  |\n","|       from large pool |     142    |     147    |   13924 K  |   13924 K  |\n","|       from small pool |     184    |     268    |   18785 K  |   18785 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23882    |   23865    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23692    |   23684    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      63    |   16358 K  |   16358 K  |\n","|       from large pool |      20    |      21    |    8024 K  |    8024 K  |\n","|       from small pool |       9    |      55    |    8334 K  |    8334 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:03:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  95% 2956/3126 [05:54<00:23,  7.29it/s, loss=1.934, ppl=3.82, wps=24906.3, ups=8.69, wpb=2865.8, bsz=128, num_updates=39900, lr=0.0005, gnorm=1.246, loss_scale=8, train_wall=11, gb_free=10.9, wall=5097]2022-05-26 13:03:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.55 GiB (GPU 0; 14.76 GiB total capacity; 6.44 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:03:58 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 483          |        cudaMalloc retries: 547       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6592 MB |    8932 MB |  208285 GB |  208278 GB |\n","|       from large pool |    6573 MB |    8912 MB |  203858 GB |  203852 GB |\n","|       from small pool |      18 MB |      19 MB |    4426 GB |    4426 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6592 MB |    8932 MB |  208285 GB |  208278 GB |\n","|       from large pool |    6573 MB |    8912 MB |  203858 GB |  203852 GB |\n","|       from small pool |      18 MB |      19 MB |    4426 GB |    4426 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13326 MB |  546376 MB |  533144 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |     114 MB |   47482 MB |   47462 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6639 MB |    6639 MB |  415418 GB |  415412 GB |\n","|       from large pool |    6638 MB |    6638 MB |  410344 GB |  410338 GB |\n","|       from small pool |       1 MB |      11 MB |    5073 GB |    5073 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32766 K  |   32765 K  |\n","|       from large pool |     132    |     136    |   13949 K  |   13949 K  |\n","|       from small pool |     194    |     268    |   18816 K  |   18816 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32766 K  |   32765 K  |\n","|       from large pool |     132    |     136    |   13949 K  |   13949 K  |\n","|       from small pool |     194    |     268    |   18816 K  |   18816 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      66    |   23931    |   23912    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      57    |   23741    |   23731    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      27    |   16386 K  |   16386 K  |\n","|       from large pool |      10    |      11    |    8038 K  |    8038 K  |\n","|       from small pool |      13    |      20    |    8348 K  |    8348 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 013:  95% 2957/3126 [05:55<00:22,  7.38it/s, loss=1.934, ppl=3.82, wps=24906.3, ups=8.69, wpb=2865.8, bsz=128, num_updates=39900, lr=0.0005, gnorm=1.246, loss_scale=8, train_wall=11, gb_free=10.9, wall=5097]2022-05-26 13:03:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  95% 2985/3126 [05:58<00:15,  9.11it/s, loss=1.963, ppl=3.9, wps=24089.3, ups=8.11, wpb=2971.1, bsz=128, num_updates=40000, lr=0.0005, gnorm=1.247, loss_scale=8, train_wall=12, gb_free=10.4, wall=5109]2022-05-26 13:04:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.27 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:04:01 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 484          |        cudaMalloc retries: 548       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7445 MB |   10112 MB |  208415 GB |  208408 GB |\n","|       from large pool |    7435 MB |   10102 MB |  203986 GB |  203979 GB |\n","|       from small pool |      10 MB |      17 MB |    4429 GB |    4429 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7445 MB |   10112 MB |  208415 GB |  208408 GB |\n","|       from large pool |    7435 MB |   10102 MB |  203986 GB |  203979 GB |\n","|       from small pool |      10 MB |      17 MB |    4429 GB |    4429 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  546470 MB |  533242 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   47576 MB |   47560 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5782 MB |    5782 MB |  415682 GB |  415677 GB |\n","|       from large pool |    5776 MB |    5776 MB |  410605 GB |  410599 GB |\n","|       from small pool |       5 MB |      19 MB |    5077 GB |    5077 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32788 K  |   32788 K  |\n","|       from large pool |     142    |     147    |   13958 K  |   13958 K  |\n","|       from small pool |     184    |     268    |   18829 K  |   18829 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32788 K  |   32788 K  |\n","|       from large pool |     142    |     147    |   13958 K  |   13958 K  |\n","|       from small pool |     184    |     268    |   18829 K  |   18829 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   23978    |   23961    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23788    |   23780    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      47    |   16398 K  |   16398 K  |\n","|       from large pool |      16    |      17    |    8044 K  |    8044 K  |\n","|       from small pool |      11    |      39    |    8353 K  |    8353 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:04:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  96% 3005/3126 [06:00<00:13,  8.84it/s, loss=1.963, ppl=3.9, wps=24089.3, ups=8.11, wpb=2971.1, bsz=128, num_updates=40000, lr=0.0005, gnorm=1.247, loss_scale=8, train_wall=12, gb_free=10.4, wall=5109]2022-05-26 13:04:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.86 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:04:04 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 485          |        cudaMalloc retries: 549       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9069 MB |   12416 MB |  208529 GB |  208520 GB |\n","|       from large pool |    9059 MB |   12406 MB |  204097 GB |  204088 GB |\n","|       from small pool |      10 MB |      17 MB |    4431 GB |    4431 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9069 MB |   12416 MB |  208529 GB |  208520 GB |\n","|       from large pool |    9059 MB |   12406 MB |  204097 GB |  204088 GB |\n","|       from small pool |      10 MB |      17 MB |    4431 GB |    4431 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  546568 MB |  533340 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   47674 MB |   47658 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4158 MB |    4518 MB |  415912 GB |  415908 GB |\n","|       from large pool |    4152 MB |    4512 MB |  410832 GB |  410827 GB |\n","|       from small pool |       5 MB |      40 MB |    5080 GB |    5080 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32804 K  |   32804 K  |\n","|       from large pool |     142    |     147    |   13965 K  |   13965 K  |\n","|       from small pool |     184    |     268    |   18839 K  |   18839 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32804 K  |   32804 K  |\n","|       from large pool |     142    |     147    |   13965 K  |   13965 K  |\n","|       from small pool |     184    |     268    |   18839 K  |   18839 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   24027    |   24010    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23837    |   23829    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      51    |   16406 K  |   16406 K  |\n","|       from large pool |      14    |      15    |    8047 K  |    8047 K  |\n","|       from small pool |      11    |      44    |    8358 K  |    8358 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:04:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013:  98% 3062/3126 [06:07<00:07,  8.84it/s, loss=1.963, ppl=3.9, wps=24089.3, ups=8.11, wpb=2971.1, bsz=128, num_updates=40000, lr=0.0005, gnorm=1.247, loss_scale=8, train_wall=12, gb_free=10.4, wall=5109]2022-05-26 13:04:11 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.28 GiB already allocated; 609.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:04:11 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 486          |        cudaMalloc retries: 550       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6432 MB |    8696 MB |  208786 GB |  208780 GB |\n","|       from large pool |    6412 MB |    8676 MB |  204348 GB |  204342 GB |\n","|       from small pool |      19 MB |      20 MB |    4437 GB |    4437 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6432 MB |    8696 MB |  208786 GB |  208780 GB |\n","|       from large pool |    6412 MB |    8676 MB |  204348 GB |  204342 GB |\n","|       from small pool |      19 MB |      20 MB |    4437 GB |    4437 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13234 MB |   13326 MB |  546666 MB |  533432 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      22 MB |     114 MB |   47772 MB |   47750 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6801 MB |    6801 MB |  416440 GB |  416434 GB |\n","|       from large pool |    6799 MB |    6799 MB |  411353 GB |  411346 GB |\n","|       from small pool |       2 MB |      37 MB |    5087 GB |    5087 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   32848 K  |   32847 K  |\n","|       from large pool |     132    |     136    |   13983 K  |   13983 K  |\n","|       from small pool |     194    |     268    |   18864 K  |   18864 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   32848 K  |   32847 K  |\n","|       from large pool |     132    |     136    |   13983 K  |   13983 K  |\n","|       from small pool |     194    |     268    |   18864 K  |   18864 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      20    |      66    |   24076    |   24056    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      11    |      57    |   23886    |   23875    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      56    |   16427 K  |   16427 K  |\n","|       from large pool |      10    |      11    |    8058 K  |    8058 K  |\n","|       from small pool |      13    |      48    |    8369 K  |    8369 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:04:11 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 013: 100% 3125/3126 [06:14<00:00,  9.17it/s, loss=1.977, ppl=3.94, wps=23377.8, ups=8.1, wpb=2885.8, bsz=128, num_updates=40100, lr=0.0005, gnorm=1.272, loss_scale=8, train_wall=11, gb_free=10.5, wall=5121]2022-05-26 13:04:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 013 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   0% 1/337 [00:00<00:54,  6.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   1% 5/337 [00:00<00:18, 18.24it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   3% 9/337 [00:00<00:13, 24.70it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   4% 12/337 [00:00<00:13, 24.29it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   5% 16/337 [00:00<00:11, 28.07it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   6% 20/337 [00:00<00:10, 30.44it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   7% 24/337 [00:00<00:09, 31.64it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:   8% 28/337 [00:01<00:09, 32.71it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  10% 33/337 [00:01<00:08, 35.39it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  11% 37/337 [00:01<00:08, 34.60it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  12% 41/337 [00:01<00:08, 33.97it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  13% 45/337 [00:01<00:09, 30.76it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  15% 49/337 [00:01<00:09, 31.04it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  16% 53/337 [00:01<00:09, 29.12it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  17% 57/337 [00:01<00:09, 30.01it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  18% 61/337 [00:02<00:09, 27.63it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  19% 65/337 [00:02<00:09, 28.91it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  20% 69/337 [00:02<00:08, 30.04it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  22% 73/337 [00:02<00:09, 28.71it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  23% 77/337 [00:02<00:08, 30.46it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  24% 81/337 [00:02<00:09, 28.25it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  25% 84/337 [00:02<00:09, 27.99it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  26% 88/337 [00:03<00:08, 29.83it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  27% 92/337 [00:03<00:08, 27.35it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  28% 96/337 [00:03<00:08, 28.93it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  30% 100/337 [00:03<00:07, 30.75it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  31% 104/337 [00:03<00:08, 28.85it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  32% 108/337 [00:03<00:07, 29.98it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  33% 112/337 [00:03<00:07, 30.65it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  34% 116/337 [00:03<00:07, 30.18it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  36% 120/337 [00:04<00:07, 27.72it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  37% 124/337 [00:04<00:07, 28.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  38% 127/337 [00:04<00:07, 28.98it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  39% 130/337 [00:04<00:08, 25.55it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  40% 134/337 [00:04<00:07, 27.27it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  41% 138/337 [00:04<00:07, 28.38it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  42% 141/337 [00:04<00:07, 26.99it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  43% 144/337 [00:05<00:07, 25.37it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  44% 148/337 [00:05<00:06, 27.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  45% 151/337 [00:05<00:06, 27.37it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  46% 154/337 [00:05<00:06, 27.43it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  47% 157/337 [00:05<00:07, 25.63it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  48% 161/337 [00:05<00:06, 27.23it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  49% 164/337 [00:05<00:06, 27.85it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  50% 167/337 [00:05<00:06, 26.96it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  50% 170/337 [00:06<00:07, 23.24it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  51% 173/337 [00:06<00:06, 24.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  52% 176/337 [00:06<00:06, 24.82it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  53% 179/337 [00:06<00:06, 25.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  54% 182/337 [00:06<00:06, 23.39it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  55% 185/337 [00:06<00:06, 24.96it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  56% 188/337 [00:06<00:05, 25.02it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  57% 191/337 [00:06<00:05, 25.31it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  58% 194/337 [00:07<00:05, 23.85it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  58% 197/337 [00:07<00:05, 25.30it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  59% 200/337 [00:07<00:05, 26.04it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  60% 203/337 [00:07<00:05, 25.93it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  61% 206/337 [00:07<00:05, 24.02it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  62% 209/337 [00:07<00:05, 24.94it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  63% 212/337 [00:07<00:05, 24.01it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  64% 215/337 [00:07<00:05, 23.65it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  65% 218/337 [00:08<00:05, 21.94it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  66% 221/337 [00:08<00:05, 22.79it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  66% 224/337 [00:08<00:04, 23.26it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  67% 227/337 [00:08<00:05, 20.89it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  68% 230/337 [00:08<00:04, 22.49it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  69% 233/337 [00:08<00:04, 23.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  70% 236/337 [00:08<00:04, 23.36it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  71% 239/337 [00:08<00:04, 21.78it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  72% 242/337 [00:09<00:04, 22.82it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  73% 245/337 [00:09<00:03, 23.22it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  74% 248/337 [00:09<00:04, 22.14it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  74% 251/337 [00:09<00:03, 21.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  75% 254/337 [00:09<00:03, 21.40it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  76% 257/337 [00:09<00:03, 20.02it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  77% 260/337 [00:09<00:03, 21.01it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  78% 263/337 [00:10<00:03, 19.53it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  79% 266/337 [00:10<00:03, 20.12it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  80% 269/337 [00:10<00:03, 20.65it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  81% 272/337 [00:10<00:03, 19.58it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  82% 275/337 [00:10<00:03, 20.65it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  82% 278/337 [00:10<00:03, 19.41it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  83% 280/337 [00:10<00:02, 19.19it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  84% 283/337 [00:11<00:02, 19.60it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  85% 285/337 [00:11<00:03, 17.24it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  85% 288/337 [00:11<00:02, 18.38it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  86% 290/337 [00:11<00:02, 18.35it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  87% 293/337 [00:11<00:02, 19.07it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  88% 295/337 [00:11<00:02, 18.22it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  88% 298/337 [00:11<00:02, 19.14it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  89% 300/337 [00:12<00:02, 18.01it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  90% 303/337 [00:12<00:01, 18.67it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  91% 305/337 [00:12<00:01, 18.18it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  91% 307/337 [00:12<00:01, 18.30it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  92% 309/337 [00:12<00:01, 16.29it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  92% 311/337 [00:12<00:01, 16.45it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  93% 313/337 [00:12<00:01, 15.78it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  93% 315/337 [00:13<00:01, 14.90it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  94% 317/337 [00:13<00:01, 15.77it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  95% 319/337 [00:13<00:01, 15.84it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  95% 321/337 [00:13<00:01, 15.38it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  96% 323/337 [00:13<00:00, 15.44it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  96% 325/337 [00:13<00:00, 14.89it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  97% 327/337 [00:13<00:00, 14.23it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  98% 329/337 [00:13<00:00, 14.12it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  98% 331/337 [00:14<00:00, 13.50it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  99% 333/337 [00:14<00:00, 12.27it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset:  99% 335/337 [00:14<00:00, 11.63it/s]\u001b[A\n","epoch 013 | valid on 'valid' subset: 100% 337/337 [00:14<00:00, 12.84it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 13:04:33 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.896 | ppl 119.13 | wps 68963.7 | wpb 2965.1 | bsz 127.6 | num_updates 40147 | best_loss 5.602\n","2022-05-26 13:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 40147 updates\n","2022-05-26 13:04:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_en_de_with_backtranslations/checkpoint_last.pt\n","2022-05-26 13:04:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_en_de_with_backtranslations/checkpoint_last.pt\n","2022-05-26 13:04:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_en_de_with_backtranslations/checkpoint_last.pt (epoch 13 @ 40147 updates, score 6.896) (writing took 3.5941918429998623 seconds)\n","2022-05-26 13:04:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n","2022-05-26 13:04:36 | INFO | train | epoch 013 | loss 1.892 | ppl 3.71 | wps 23232.2 | ups 7.85 | wpb 2958.2 | bsz 128 | num_updates 40147 | lr 0.0005 | gnorm 1.225 | loss_scale 8 | train_wall 359 | gb_free 8.9 | wall 5145\n","epoch 014:   0% 0/3126 [00:00<?, ?it/s]2022-05-26 13:04:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3126\n","2022-05-26 13:04:36 | INFO | fairseq.trainer | begin training epoch 14\n","2022-05-26 13:04:36 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 014:   4% 112/3126 [00:13<05:34,  9.00it/s, loss=1.751, ppl=3.37, wps=9268.8, ups=3.32, wpb=2791.3, bsz=128, num_updates=40200, lr=0.0005, gnorm=1.184, loss_scale=8, train_wall=11, gb_free=12.3, wall=5152]2022-05-26 13:04:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.60 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:04:50 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 487          |        cudaMalloc retries: 551       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7787 MB |   10480 MB |  210459 GB |  210451 GB |\n","|       from large pool |    7776 MB |   10470 MB |  205992 GB |  205984 GB |\n","|       from small pool |      10 MB |      17 MB |    4466 GB |    4466 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7787 MB |   10480 MB |  210459 GB |  210451 GB |\n","|       from large pool |    7776 MB |   10470 MB |  205992 GB |  205984 GB |\n","|       from small pool |      10 MB |      17 MB |    4466 GB |    4466 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  546758 MB |  533534 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   47864 MB |   47852 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5436 MB |    5436 MB |  419997 GB |  419992 GB |\n","|       from large pool |    5435 MB |    5435 MB |  414877 GB |  414871 GB |\n","|       from small pool |       1 MB |      21 MB |    5120 GB |    5120 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33068 K  |   33067 K  |\n","|       from large pool |     142    |     147    |   14079 K  |   14079 K  |\n","|       from small pool |     184    |     268    |   18988 K  |   18988 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33068 K  |   33067 K  |\n","|       from large pool |     142    |     147    |   14079 K  |   14079 K  |\n","|       from small pool |     184    |     268    |   18988 K  |   18988 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24122    |   24107    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   23932    |   23926    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      31    |      47    |   16539 K  |   16539 K  |\n","|       from large pool |      23    |      24    |    8115 K  |    8115 K  |\n","|       from small pool |       8    |      39    |    8423 K  |    8423 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:04:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:   6% 186/3126 [00:22<05:35,  8.76it/s, loss=1.612, ppl=3.06, wps=24308.2, ups=8.4, wpb=2895.5, bsz=128, num_updates=40300, lr=0.0005, gnorm=1.153, loss_scale=8, train_wall=11, gb_free=10.6, wall=5163]2022-05-26 13:04:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.13 GiB (GPU 0; 14.76 GiB total capacity; 7.27 GiB already allocated; 615.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:04:58 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 488          |        cudaMalloc retries: 552       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7441 MB |   10084 MB |  210805 GB |  210797 GB |\n","|       from large pool |    7431 MB |   10074 MB |  206330 GB |  206323 GB |\n","|       from small pool |      10 MB |      17 MB |    4474 GB |    4474 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7441 MB |   10084 MB |  210805 GB |  210797 GB |\n","|       from large pool |    7431 MB |   10074 MB |  206330 GB |  206323 GB |\n","|       from small pool |      10 MB |      17 MB |    4474 GB |    4474 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13228 MB |   13326 MB |  546860 MB |  533632 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      16 MB |     114 MB |   47966 MB |   47950 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5786 MB |    5786 MB |  420724 GB |  420718 GB |\n","|       from large pool |    5780 MB |    5780 MB |  415594 GB |  415589 GB |\n","|       from small pool |       5 MB |      43 MB |    5129 GB |    5129 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33125 K  |   33125 K  |\n","|       from large pool |     142    |     147    |   14104 K  |   14104 K  |\n","|       from small pool |     184    |     268    |   19021 K  |   19020 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33125 K  |   33125 K  |\n","|       from large pool |     142    |     147    |   14104 K  |   14104 K  |\n","|       from small pool |     184    |     268    |   19021 K  |   19020 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      17    |      66    |   24173    |   24156    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       8    |      57    |   23983    |   23975    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      58    |   16568 K  |   16568 K  |\n","|       from large pool |      18    |      19    |    8129 K  |    8129 K  |\n","|       from small pool |      11    |      50    |    8438 K  |    8438 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:04:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:   8% 247/3126 [00:29<06:47,  7.06it/s, loss=1.612, ppl=3.06, wps=24308.2, ups=8.4, wpb=2895.5, bsz=128, num_updates=40300, lr=0.0005, gnorm=1.153, loss_scale=8, train_wall=11, gb_free=10.6, wall=5163]2022-05-26 13:05:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.45 GiB (GPU 0; 14.76 GiB total capacity; 6.42 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:05:06 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 489          |        cudaMalloc retries: 553       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6576 MB |    8865 MB |  211128 GB |  211121 GB |\n","|       from large pool |    6566 MB |    8854 MB |  206647 GB |  206640 GB |\n","|       from small pool |      10 MB |      17 MB |    4480 GB |    4480 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6576 MB |    8865 MB |  211128 GB |  211121 GB |\n","|       from large pool |    6566 MB |    8854 MB |  206647 GB |  206640 GB |\n","|       from small pool |      10 MB |      17 MB |    4480 GB |    4480 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  546958 MB |  533734 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   48064 MB |   48052 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6647 MB |    6647 MB |  421378 GB |  421371 GB |\n","|       from large pool |    6645 MB |    6645 MB |  416241 GB |  416235 GB |\n","|       from small pool |       1 MB |      37 MB |    5136 GB |    5136 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33174 K  |   33174 K  |\n","|       from large pool |     142    |     146    |   14126 K  |   14126 K  |\n","|       from small pool |     184    |     268    |   19048 K  |   19048 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33174 K  |   33174 K  |\n","|       from large pool |     142    |     146    |   14126 K  |   14126 K  |\n","|       from small pool |     184    |     268    |   19048 K  |   19048 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24222    |   24207    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24032    |   24026    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      54    |   16592 K  |   16592 K  |\n","|       from large pool |      17    |      18    |    8142 K  |    8142 K  |\n","|       from small pool |       9    |      46    |    8450 K  |    8450 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:05:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  10% 299/3126 [00:35<05:10,  9.10it/s, loss=1.663, ppl=3.17, wps=24571.2, ups=8.11, wpb=3028.2, bsz=128, num_updates=40400, lr=0.0005, gnorm=1.137, loss_scale=8, train_wall=12, gb_free=11.9, wall=5176]2022-05-26 13:05:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.84 GiB (GPU 0; 14.76 GiB total capacity; 6.85 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:05:12 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 490          |        cudaMalloc retries: 554       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7011 MB |    9502 MB |  211375 GB |  211368 GB |\n","|       from large pool |    7001 MB |    9491 MB |  206888 GB |  206881 GB |\n","|       from small pool |      10 MB |      17 MB |    4486 GB |    4486 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7011 MB |    9502 MB |  211375 GB |  211368 GB |\n","|       from large pool |    7001 MB |    9491 MB |  206888 GB |  206881 GB |\n","|       from small pool |      10 MB |      17 MB |    4486 GB |    4486 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  547060 MB |  533836 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   48166 MB |   48154 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6212 MB |    6212 MB |  421867 GB |  421861 GB |\n","|       from large pool |    6210 MB |    6210 MB |  416723 GB |  416717 GB |\n","|       from small pool |       1 MB |      14 MB |    5143 GB |    5143 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33215 K  |   33215 K  |\n","|       from large pool |     142    |     146    |   14143 K  |   14143 K  |\n","|       from small pool |     184    |     268    |   19072 K  |   19072 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33215 K  |   33215 K  |\n","|       from large pool |     142    |     146    |   14143 K  |   14143 K  |\n","|       from small pool |     184    |     268    |   19072 K  |   19072 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24273    |   24258    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24083    |   24077    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      19    |      35    |   16613 K  |   16613 K  |\n","|       from large pool |       9    |      10    |    8152 K  |    8152 K  |\n","|       from small pool |      10    |      27    |    8461 K  |    8461 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:05:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  10% 323/3126 [00:38<05:25,  8.60it/s, loss=1.663, ppl=3.17, wps=24571.2, ups=8.11, wpb=3028.2, bsz=128, num_updates=40400, lr=0.0005, gnorm=1.137, loss_scale=8, train_wall=12, gb_free=11.9, wall=5176]2022-05-26 13:05:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.41 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:05:15 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 491          |        cudaMalloc retries: 555       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6559 MB |    8824 MB |  211481 GB |  211474 GB |\n","|       from large pool |    6549 MB |    8814 MB |  206992 GB |  206985 GB |\n","|       from small pool |      10 MB |      17 MB |    4489 GB |    4489 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6559 MB |    8824 MB |  211481 GB |  211474 GB |\n","|       from large pool |    6549 MB |    8814 MB |  206992 GB |  206985 GB |\n","|       from small pool |      10 MB |      17 MB |    4489 GB |    4489 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13304 MB |  547140 MB |  533916 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |      92 MB |   48246 MB |   48234 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6664 MB |    6664 MB |  422082 GB |  422076 GB |\n","|       from large pool |    6662 MB |    6662 MB |  416936 GB |  416929 GB |\n","|       from small pool |       1 MB |      15 MB |    5146 GB |    5146 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33232 K  |   33232 K  |\n","|       from large pool |     142    |     147    |   14150 K  |   14150 K  |\n","|       from small pool |     184    |     268    |   19081 K  |   19081 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33232 K  |   33232 K  |\n","|       from large pool |     142    |     147    |   14150 K  |   14150 K  |\n","|       from small pool |     184    |     268    |   19081 K  |   19081 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      55    |   24313    |   24298    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      46    |   24123    |   24117    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      40    |   16621 K  |   16621 K  |\n","|       from large pool |      16    |      17    |    8156 K  |    8156 K  |\n","|       from small pool |       9    |      33    |    8465 K  |    8465 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:05:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  11% 330/3126 [00:39<05:50,  7.98it/s, loss=1.663, ppl=3.17, wps=24571.2, ups=8.11, wpb=3028.2, bsz=128, num_updates=40400, lr=0.0005, gnorm=1.137, loss_scale=8, train_wall=12, gb_free=11.9, wall=5176]2022-05-26 13:05:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.31 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:05:16 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 492          |        cudaMalloc retries: 556       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6463 MB |    8727 MB |  211520 GB |  211514 GB |\n","|       from large pool |    6453 MB |    8716 MB |  207030 GB |  207024 GB |\n","|       from small pool |       9 MB |      17 MB |    4489 GB |    4489 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6463 MB |    8727 MB |  211520 GB |  211514 GB |\n","|       from large pool |    6453 MB |    8716 MB |  207030 GB |  207024 GB |\n","|       from small pool |       9 MB |      17 MB |    4489 GB |    4489 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13314 MB |  547230 MB |  534006 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     102 MB |   48336 MB |   48324 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6760 MB |    6760 MB |  422155 GB |  422148 GB |\n","|       from large pool |    6758 MB |    6758 MB |  417008 GB |  417001 GB |\n","|       from small pool |       2 MB |       3 MB |    5146 GB |    5146 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33238 K  |   33237 K  |\n","|       from large pool |     142    |     146    |   14153 K  |   14153 K  |\n","|       from small pool |     184    |     268    |   19085 K  |   19084 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33238 K  |   33237 K  |\n","|       from large pool |     142    |     146    |   14153 K  |   14153 K  |\n","|       from small pool |     184    |     268    |   19085 K  |   19084 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      60    |   24358    |   24343    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      51    |   24168    |   24162    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      17    |      26    |   16624 K  |   16624 K  |\n","|       from large pool |       8    |       9    |    8157 K  |    8157 K  |\n","|       from small pool |       9    |      19    |    8466 K  |    8466 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:05:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  20% 613/3126 [01:13<05:08,  8.14it/s, loss=1.754, ppl=3.37, wps=24876, ups=8.59, wpb=2894.5, bsz=128, num_updates=40700, lr=0.0005, gnorm=1.201, loss_scale=8, train_wall=11, gb_free=10.1, wall=5211]2022-05-26 13:05:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.98 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:05:50 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 493          |        cudaMalloc retries: 557       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7145 MB |    9711 MB |  212889 GB |  212882 GB |\n","|       from large pool |    7135 MB |    9700 MB |  208369 GB |  208362 GB |\n","|       from small pool |      10 MB |      17 MB |    4519 GB |    4519 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7145 MB |    9711 MB |  212889 GB |  212882 GB |\n","|       from large pool |    7135 MB |    9700 MB |  208369 GB |  208362 GB |\n","|       from small pool |      10 MB |      17 MB |    4519 GB |    4519 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  547332 MB |  534108 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   48438 MB |   48426 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6078 MB |    6078 MB |  424971 GB |  424965 GB |\n","|       from large pool |    6076 MB |    6076 MB |  419789 GB |  419783 GB |\n","|       from small pool |       1 MB |      14 MB |    5181 GB |    5181 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33461 K  |   33461 K  |\n","|       from large pool |     142    |     146    |   14247 K  |   14247 K  |\n","|       from small pool |     184    |     268    |   19213 K  |   19213 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33461 K  |   33461 K  |\n","|       from large pool |     142    |     146    |   14247 K  |   14247 K  |\n","|       from small pool |     184    |     268    |   19213 K  |   19213 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24409    |   24394    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24219    |   24213    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      16    |      35    |   16736 K  |   16736 K  |\n","|       from large pool |       8    |       9    |    8212 K  |    8212 K  |\n","|       from small pool |       8    |      28    |    8523 K  |    8523 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:05:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  20% 627/3126 [01:14<04:42,  8.85it/s, loss=1.754, ppl=3.37, wps=24876, ups=8.59, wpb=2894.5, bsz=128, num_updates=40700, lr=0.0005, gnorm=1.201, loss_scale=8, train_wall=11, gb_free=10.1, wall=5211]2022-05-26 13:05:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  20% 628/3126 [01:14<04:52,  8.55it/s, loss=1.754, ppl=3.37, wps=24876, ups=8.59, wpb=2894.5, bsz=128, num_updates=40700, lr=0.0005, gnorm=1.201, loss_scale=8, train_wall=11, gb_free=10.1, wall=5211]2022-05-26 13:05:51 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 494          |        cudaMalloc retries: 558       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6371 MB |    8511 MB |  212953 GB |  212947 GB |\n","|       from large pool |    6361 MB |    8500 MB |  208432 GB |  208426 GB |\n","|       from small pool |      10 MB |      17 MB |    4521 GB |    4521 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6371 MB |    8511 MB |  212953 GB |  212947 GB |\n","|       from large pool |    6361 MB |    8500 MB |  208432 GB |  208426 GB |\n","|       from small pool |      10 MB |      17 MB |    4521 GB |    4521 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  547430 MB |  534206 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   48536 MB |   48524 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6852 MB |    6852 MB |  425097 GB |  425091 GB |\n","|       from large pool |    6850 MB |    6850 MB |  419914 GB |  419907 GB |\n","|       from small pool |       1 MB |      22 MB |    5183 GB |    5183 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33472 K  |   33471 K  |\n","|       from large pool |     142    |     147    |   14252 K  |   14252 K  |\n","|       from small pool |     184    |     268    |   19219 K  |   19219 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33472 K  |   33471 K  |\n","|       from large pool |     142    |     147    |   14252 K  |   14252 K  |\n","|       from small pool |     184    |     268    |   19219 K  |   19219 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   24458    |   24443    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   24268    |   24262    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      49    |   16741 K  |   16741 K  |\n","|       from large pool |      12    |      13    |    8215 K  |    8215 K  |\n","|       from small pool |      10    |      41    |    8526 K  |    8526 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:05:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  28% 864/3126 [01:42<04:43,  7.98it/s, loss=1.72, ppl=3.3, wps=25267.4, ups=8.46, wpb=2987.2, bsz=128, num_updates=41000, lr=0.0005, gnorm=1.196, loss_scale=8, train_wall=11, gb_free=11.7, wall=5248]2022-05-26 13:06:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.04 GiB (GPU 0; 14.76 GiB total capacity; 7.17 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:06:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 495          |        cudaMalloc retries: 559       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7340 MB |    9932 MB |  214105 GB |  214098 GB |\n","|       from large pool |    7330 MB |    9922 MB |  209557 GB |  209550 GB |\n","|       from small pool |      10 MB |      17 MB |    4547 GB |    4547 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7340 MB |    9932 MB |  214105 GB |  214098 GB |\n","|       from large pool |    7330 MB |    9922 MB |  209557 GB |  209550 GB |\n","|       from small pool |      10 MB |      17 MB |    4547 GB |    4547 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  547532 MB |  534308 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   48638 MB |   48626 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5883 MB |    5883 MB |  427492 GB |  427486 GB |\n","|       from large pool |    5881 MB |    5881 MB |  422278 GB |  422272 GB |\n","|       from small pool |       1 MB |      46 MB |    5213 GB |    5213 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33660 K  |   33659 K  |\n","|       from large pool |     142    |     147    |   14329 K  |   14329 K  |\n","|       from small pool |     184    |     268    |   19330 K  |   19330 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33660 K  |   33659 K  |\n","|       from large pool |     142    |     147    |   14329 K  |   14329 K  |\n","|       from small pool |     184    |     268    |   19330 K  |   19330 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24509    |   24494    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24319    |   24313    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      64    |   16835 K  |   16835 K  |\n","|       from large pool |      17    |      18    |    8259 K  |    8259 K  |\n","|       from small pool |       6    |      56    |    8576 K  |    8576 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:06:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  32% 1000/3126 [01:58<03:23, 10.44it/s, loss=1.802, ppl=3.49, wps=24662.4, ups=8.4, wpb=2934.9, bsz=128, num_updates=41100, lr=0.0005, gnorm=1.234, loss_scale=8, train_wall=11, gb_free=9.6, wall=5260]2022-05-26 13:06:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 7.01 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:06:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 496          |        cudaMalloc retries: 560       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7178 MB |    9744 MB |  214749 GB |  214742 GB |\n","|       from large pool |    7168 MB |    9733 MB |  210187 GB |  210180 GB |\n","|       from small pool |      10 MB |      17 MB |    4562 GB |    4562 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7178 MB |    9744 MB |  214749 GB |  214742 GB |\n","|       from large pool |    7168 MB |    9733 MB |  210187 GB |  210180 GB |\n","|       from small pool |      10 MB |      17 MB |    4562 GB |    4562 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  547634 MB |  534410 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   48740 MB |   48728 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6045 MB |    6045 MB |  428831 GB |  428825 GB |\n","|       from large pool |    6043 MB |    6043 MB |  423601 GB |  423595 GB |\n","|       from small pool |       1 MB |      38 MB |    5230 GB |    5230 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33767 K  |   33766 K  |\n","|       from large pool |     142    |     146    |   14373 K  |   14372 K  |\n","|       from small pool |     184    |     268    |   19394 K  |   19393 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33767 K  |   33766 K  |\n","|       from large pool |     142    |     146    |   14373 K  |   14372 K  |\n","|       from small pool |     184    |     268    |   19394 K  |   19393 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24560    |   24545    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24370    |   24364    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      17    |      64    |   16889 K  |   16889 K  |\n","|       from large pool |       9    |      10    |    8284 K  |    8284 K  |\n","|       from small pool |       8    |      56    |    8604 K  |    8604 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:06:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  36% 1137/3126 [02:14<03:53,  8.50it/s, loss=1.776, ppl=3.42, wps=24372.1, ups=8.46, wpb=2881.4, bsz=128, num_updates=41200, lr=0.0005, gnorm=1.224, loss_scale=8, train_wall=11, gb_free=12.1, wall=5271]2022-05-26 13:06:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.75 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:06:51 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 497          |        cudaMalloc retries: 561       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7932 MB |   10800 MB |  215389 GB |  215382 GB |\n","|       from large pool |    7921 MB |   10790 MB |  210811 GB |  210804 GB |\n","|       from small pool |      10 MB |      17 MB |    4577 GB |    4577 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7932 MB |   10800 MB |  215389 GB |  215382 GB |\n","|       from large pool |    7921 MB |   10790 MB |  210811 GB |  210804 GB |\n","|       from small pool |      10 MB |      17 MB |    4577 GB |    4577 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  547736 MB |  534510 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   48842 MB |   48828 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5293 MB |    5293 MB |  430121 GB |  430116 GB |\n","|       from large pool |    5290 MB |    5290 MB |  424873 GB |  424868 GB |\n","|       from small pool |       3 MB |      28 MB |    5248 GB |    5248 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33874 K  |   33873 K  |\n","|       from large pool |     142    |     147    |   14418 K  |   14418 K  |\n","|       from small pool |     184    |     268    |   19455 K  |   19455 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33874 K  |   33873 K  |\n","|       from large pool |     142    |     147    |   14418 K  |   14418 K  |\n","|       from small pool |     184    |     268    |   19455 K  |   19455 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   24611    |   24595    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   24421    |   24414    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      22    |      61    |   16943 K  |   16943 K  |\n","|       from large pool |      11    |      12    |    8310 K  |    8310 K  |\n","|       from small pool |      11    |      53    |    8632 K  |    8632 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:06:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  38% 1187/3126 [02:20<03:22,  9.57it/s, loss=1.672, ppl=3.19, wps=24239.9, ups=8.57, wpb=2828.9, bsz=128, num_updates=41300, lr=0.0005, gnorm=1.218, loss_scale=8, train_wall=11, gb_free=10.6, wall=5283]2022-05-26 13:06:57 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.96 GiB already allocated; 609.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:06:57 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 498          |        cudaMalloc retries: 562       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7124 MB |    9690 MB |  215617 GB |  215610 GB |\n","|       from large pool |    7104 MB |    9669 MB |  211033 GB |  211026 GB |\n","|       from small pool |      20 MB |      20 MB |    4583 GB |    4583 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7124 MB |    9690 MB |  215617 GB |  215610 GB |\n","|       from large pool |    7104 MB |    9669 MB |  211033 GB |  211026 GB |\n","|       from small pool |      20 MB |      20 MB |    4583 GB |    4583 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13234 MB |   13326 MB |  547836 MB |  534602 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      22 MB |     114 MB |   48942 MB |   48920 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6109 MB |    6109 MB |  430587 GB |  430581 GB |\n","|       from large pool |    6107 MB |    6107 MB |  425332 GB |  425326 GB |\n","|       from small pool |       1 MB |      19 MB |    5254 GB |    5254 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33912 K  |   33912 K  |\n","|       from large pool |     132    |     136    |   14433 K  |   14433 K  |\n","|       from small pool |     194    |     268    |   19479 K  |   19478 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33912 K  |   33912 K  |\n","|       from large pool |     132    |     136    |   14433 K  |   14433 K  |\n","|       from small pool |     194    |     268    |   19479 K  |   19478 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      20    |      66    |   24661    |   24641    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      11    |      57    |   24471    |   24460    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      44    |   16962 K  |   16962 K  |\n","|       from large pool |      10    |      12    |    8319 K  |    8319 K  |\n","|       from small pool |      13    |      36    |    8643 K  |    8643 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:06:57 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  40% 1256/3126 [02:28<03:48,  8.19it/s, loss=1.672, ppl=3.19, wps=24239.9, ups=8.57, wpb=2828.9, bsz=128, num_updates=41300, lr=0.0005, gnorm=1.218, loss_scale=8, train_wall=11, gb_free=10.6, wall=5283]2022-05-26 13:07:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.55 GiB (GPU 0; 14.76 GiB total capacity; 6.44 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:07:05 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 499          |        cudaMalloc retries: 563       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6592 MB |    8932 MB |  215950 GB |  215944 GB |\n","|       from large pool |    6573 MB |    8912 MB |  211359 GB |  211353 GB |\n","|       from small pool |      18 MB |      19 MB |    4590 GB |    4590 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6592 MB |    8932 MB |  215950 GB |  215944 GB |\n","|       from large pool |    6573 MB |    8912 MB |  211359 GB |  211353 GB |\n","|       from small pool |      18 MB |      19 MB |    4590 GB |    4590 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13326 MB |  547928 MB |  534696 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |     114 MB |   49034 MB |   49014 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6639 MB |    6639 MB |  431262 GB |  431256 GB |\n","|       from large pool |    6638 MB |    6638 MB |  425999 GB |  425992 GB |\n","|       from small pool |       1 MB |      17 MB |    5263 GB |    5263 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   33966 K  |   33966 K  |\n","|       from large pool |     132    |     136    |   14456 K  |   14456 K  |\n","|       from small pool |     194    |     268    |   19510 K  |   19510 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   33966 K  |   33966 K  |\n","|       from large pool |     132    |     136    |   14456 K  |   14456 K  |\n","|       from small pool |     194    |     268    |   19510 K  |   19510 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      66    |   24707    |   24688    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      57    |   24517    |   24507    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      43    |   16989 K  |   16989 K  |\n","|       from large pool |      10    |      11    |    8332 K  |    8332 K  |\n","|       from small pool |      10    |      35    |    8657 K  |    8657 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:07:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  45% 1401/3126 [02:46<03:56,  7.28it/s, loss=1.844, ppl=3.59, wps=25318.1, ups=8.1, wpb=3126.7, bsz=128, num_updates=41500, lr=0.0005, gnorm=1.184, loss_scale=8, train_wall=12, gb_free=11.7, wall=5307]2022-05-26 13:07:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.71 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:07:23 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 500          |        cudaMalloc retries: 564       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6868 MB |    9259 MB |  216690 GB |  216683 GB |\n","|       from large pool |    6858 MB |    9249 MB |  212087 GB |  212080 GB |\n","|       from small pool |      10 MB |      17 MB |    4603 GB |    4603 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6868 MB |    9259 MB |  216690 GB |  216683 GB |\n","|       from large pool |    6858 MB |    9249 MB |  212087 GB |  212080 GB |\n","|       from small pool |      10 MB |      17 MB |    4603 GB |    4603 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548022 MB |  534798 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49128 MB |   49116 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6355 MB |    6355 MB |  432772 GB |  432765 GB |\n","|       from large pool |    6353 MB |    6353 MB |  427494 GB |  427488 GB |\n","|       from small pool |       1 MB |       3 MB |    5277 GB |    5277 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34081 K  |   34081 K  |\n","|       from large pool |     142    |     147    |   14509 K  |   14509 K  |\n","|       from small pool |     184    |     268    |   19572 K  |   19571 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34081 K  |   34081 K  |\n","|       from large pool |     142    |     147    |   14509 K  |   14509 K  |\n","|       from small pool |     184    |     268    |   19572 K  |   19571 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24754    |   24739    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24564    |   24558    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      32    |      34    |   17046 K  |   17046 K  |\n","|       from large pool |      24    |      25    |    8363 K  |    8363 K  |\n","|       from small pool |       8    |      18    |    8683 K  |    8683 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:07:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  45% 1407/3126 [02:47<03:41,  7.78it/s, loss=1.844, ppl=3.59, wps=25318.1, ups=8.1, wpb=3126.7, bsz=128, num_updates=41500, lr=0.0005, gnorm=1.184, loss_scale=8, train_wall=12, gb_free=11.7, wall=5307]2022-05-26 13:07:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.28 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:07:24 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 501          |        cudaMalloc retries: 565       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7453 MB |   10120 MB |  216726 GB |  216719 GB |\n","|       from large pool |    7443 MB |   10110 MB |  212122 GB |  212115 GB |\n","|       from small pool |      10 MB |      17 MB |    4604 GB |    4604 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7453 MB |   10120 MB |  216726 GB |  216719 GB |\n","|       from large pool |    7443 MB |   10110 MB |  212122 GB |  212115 GB |\n","|       from small pool |      10 MB |      17 MB |    4604 GB |    4604 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548124 MB |  534900 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49230 MB |   49218 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5770 MB |    5770 MB |  432844 GB |  432838 GB |\n","|       from large pool |    5768 MB |    5768 MB |  427565 GB |  427559 GB |\n","|       from small pool |       1 MB |      36 MB |    5278 GB |    5278 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34086 K  |   34086 K  |\n","|       from large pool |     142    |     147    |   14511 K  |   14510 K  |\n","|       from small pool |     184    |     268    |   19575 K  |   19575 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34086 K  |   34086 K  |\n","|       from large pool |     142    |     147    |   14511 K  |   14510 K  |\n","|       from small pool |     184    |     268    |   19575 K  |   19575 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24805    |   24790    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24615    |   24609    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      59    |   17048 K  |   17048 K  |\n","|       from large pool |      13    |      14    |    8364 K  |    8364 K  |\n","|       from small pool |      10    |      51    |    8684 K  |    8684 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 014:  45% 1409/3126 [02:47<03:32,  8.07it/s, loss=1.844, ppl=3.59, wps=25318.1, ups=8.1, wpb=3126.7, bsz=128, num_updates=41500, lr=0.0005, gnorm=1.184, loss_scale=8, train_wall=12, gb_free=11.7, wall=5307]2022-05-26 13:07:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  52% 1614/3126 [03:11<02:46,  9.09it/s, loss=1.795, ppl=3.47, wps=24420.9, ups=8.62, wpb=2831.6, bsz=128, num_updates=41700, lr=0.0005, gnorm=1.222, loss_scale=8, train_wall=11, gb_free=10.2, wall=5331]2022-05-26 13:07:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.87 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:07:48 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 502          |        cudaMalloc retries: 566       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7031 MB |    9422 MB |  217711 GB |  217704 GB |\n","|       from large pool |    7020 MB |    9412 MB |  213084 GB |  213078 GB |\n","|       from small pool |      10 MB |      17 MB |    4626 GB |    4626 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7031 MB |    9422 MB |  217711 GB |  217704 GB |\n","|       from large pool |    7020 MB |    9412 MB |  213084 GB |  213078 GB |\n","|       from small pool |      10 MB |      17 MB |    4626 GB |    4626 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548226 MB |  535002 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49332 MB |   49320 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6192 MB |    6192 MB |  434880 GB |  434874 GB |\n","|       from large pool |    6191 MB |    6191 MB |  429575 GB |  429569 GB |\n","|       from small pool |       1 MB |      15 MB |    5304 GB |    5304 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34248 K  |   34248 K  |\n","|       from large pool |     142    |     147    |   14578 K  |   14578 K  |\n","|       from small pool |     184    |     268    |   19670 K  |   19670 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34248 K  |   34248 K  |\n","|       from large pool |     142    |     147    |   14578 K  |   14578 K  |\n","|       from small pool |     184    |     268    |   19670 K  |   19670 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24856    |   24841    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24666    |   24660    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      37    |   17130 K  |   17130 K  |\n","|       from large pool |      19    |      20    |    8402 K  |    8402 K  |\n","|       from small pool |       9    |      29    |    8727 K  |    8727 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:07:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  59% 1859/3126 [03:41<02:37,  8.06it/s, loss=1.883, ppl=3.69, wps=25274.3, ups=8.3, wpb=3044.4, bsz=128, num_updates=41900, lr=0.0005, gnorm=1.218, loss_scale=8, train_wall=12, gb_free=10.1, wall=5355]2022-05-26 13:08:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.27 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:08:18 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 503          |        cudaMalloc retries: 567       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7445 MB |   10112 MB |  218914 GB |  218906 GB |\n","|       from large pool |    7435 MB |   10102 MB |  214262 GB |  214255 GB |\n","|       from small pool |      10 MB |      17 MB |    4651 GB |    4651 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7445 MB |   10112 MB |  218914 GB |  218906 GB |\n","|       from large pool |    7435 MB |   10102 MB |  214262 GB |  214255 GB |\n","|       from small pool |      10 MB |      17 MB |    4651 GB |    4651 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548328 MB |  535104 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49434 MB |   49422 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5778 MB |    5778 MB |  437338 GB |  437333 GB |\n","|       from large pool |    5776 MB |    5776 MB |  432005 GB |  431999 GB |\n","|       from small pool |       1 MB |       3 MB |    5333 GB |    5333 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34442 K  |   34441 K  |\n","|       from large pool |     142    |     147    |   14661 K  |   14661 K  |\n","|       from small pool |     184    |     268    |   19780 K  |   19780 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34442 K  |   34441 K  |\n","|       from large pool |     142    |     147    |   14661 K  |   14661 K  |\n","|       from small pool |     184    |     268    |   19780 K  |   19780 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24907    |   24892    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24717    |   24711    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      28    |   17226 K  |   17226 K  |\n","|       from large pool |      16    |      17    |    8450 K  |    8450 K  |\n","|       from small pool |      10    |      19    |    8776 K  |    8776 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  60% 1880/3126 [03:43<02:41,  7.73it/s, loss=1.886, ppl=3.7, wps=23746.9, ups=8.2, wpb=2897.5, bsz=128, num_updates=42000, lr=0.0005, gnorm=1.259, loss_scale=8, train_wall=12, gb_free=10.4, wall=5368]2022-05-26 13:08:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.78 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:08:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 504          |        cudaMalloc retries: 568       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8989 MB |   12335 MB |  219034 GB |  219026 GB |\n","|       from large pool |    8978 MB |   12325 MB |  214380 GB |  214371 GB |\n","|       from small pool |      10 MB |      17 MB |    4654 GB |    4654 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8989 MB |   12335 MB |  219034 GB |  219026 GB |\n","|       from large pool |    8978 MB |   12325 MB |  214380 GB |  214371 GB |\n","|       from small pool |      10 MB |      17 MB |    4654 GB |    4654 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548430 MB |  535206 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49536 MB |   49524 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4234 MB |    4513 MB |  437555 GB |  437551 GB |\n","|       from large pool |    4233 MB |    4510 MB |  432218 GB |  432214 GB |\n","|       from small pool |       1 MB |      35 MB |    5336 GB |    5336 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34459 K  |   34458 K  |\n","|       from large pool |     142    |     147    |   14669 K  |   14668 K  |\n","|       from small pool |     184    |     268    |   19790 K  |   19789 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34459 K  |   34458 K  |\n","|       from large pool |     142    |     147    |   14669 K  |   14668 K  |\n","|       from small pool |     184    |     268    |   19790 K  |   19789 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   24958    |   24943    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24768    |   24762    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      53    |   17235 K  |   17235 K  |\n","|       from large pool |      14    |      15    |    8454 K  |    8454 K  |\n","|       from small pool |       9    |      45    |    8780 K  |    8780 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  60% 1890/3126 [03:45<02:18,  8.89it/s, loss=1.886, ppl=3.7, wps=23746.9, ups=8.2, wpb=2897.5, bsz=128, num_updates=42000, lr=0.0005, gnorm=1.259, loss_scale=8, train_wall=12, gb_free=10.4, wall=5368]2022-05-26 13:08:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.43 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:08:22 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 505          |        cudaMalloc retries: 569       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7603 MB |   10271 MB |  219073 GB |  219066 GB |\n","|       from large pool |    7593 MB |   10261 MB |  214418 GB |  214410 GB |\n","|       from small pool |      10 MB |      17 MB |    4655 GB |    4655 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7603 MB |   10271 MB |  219073 GB |  219066 GB |\n","|       from large pool |    7593 MB |   10261 MB |  214418 GB |  214410 GB |\n","|       from small pool |      10 MB |      17 MB |    4655 GB |    4655 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548532 MB |  535308 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49638 MB |   49626 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5620 MB |    5620 MB |  437631 GB |  437626 GB |\n","|       from large pool |    5618 MB |    5618 MB |  432293 GB |  432287 GB |\n","|       from small pool |       1 MB |      13 MB |    5338 GB |    5338 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34465 K  |   34465 K  |\n","|       from large pool |     142    |     147    |   14670 K  |   14670 K  |\n","|       from small pool |     184    |     268    |   19794 K  |   19794 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34465 K  |   34465 K  |\n","|       from large pool |     142    |     147    |   14670 K  |   14670 K  |\n","|       from small pool |     184    |     268    |   19794 K  |   19794 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25009    |   24994    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24819    |   24813    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      34    |   17238 K  |   17238 K  |\n","|       from large pool |      13    |      14    |    8455 K  |    8455 K  |\n","|       from small pool |       8    |      26    |    8782 K  |    8782 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  61% 1892/3126 [03:45<02:22,  8.63it/s, loss=1.886, ppl=3.7, wps=23746.9, ups=8.2, wpb=2897.5, bsz=128, num_updates=42000, lr=0.0005, gnorm=1.259, loss_scale=8, train_wall=12, gb_free=10.4, wall=5368]2022-05-26 13:08:22 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.37 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  61% 1894/3126 [03:45<02:25,  8.45it/s, loss=1.886, ppl=3.7, wps=23746.9, ups=8.2, wpb=2897.5, bsz=128, num_updates=42000, lr=0.0005, gnorm=1.259, loss_scale=8, train_wall=12, gb_free=10.4, wall=5368]2022-05-26 13:08:22 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 506          |        cudaMalloc retries: 570       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7544 MB |   10238 MB |  219086 GB |  219079 GB |\n","|       from large pool |    7534 MB |   10227 MB |  214430 GB |  214423 GB |\n","|       from small pool |      10 MB |      17 MB |    4656 GB |    4656 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7544 MB |   10238 MB |  219086 GB |  219079 GB |\n","|       from large pool |    7534 MB |   10227 MB |  214430 GB |  214423 GB |\n","|       from small pool |      10 MB |      17 MB |    4656 GB |    4656 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13304 MB |  548612 MB |  535388 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |      92 MB |   49718 MB |   49706 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5679 MB |    5679 MB |  437648 GB |  437643 GB |\n","|       from large pool |    5677 MB |    5677 MB |  432309 GB |  432304 GB |\n","|       from small pool |       1 MB |       9 MB |    5338 GB |    5338 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34467 K  |   34467 K  |\n","|       from large pool |     142    |     147    |   14671 K  |   14671 K  |\n","|       from small pool |     184    |     268    |   19796 K  |   19795 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34467 K  |   34467 K  |\n","|       from large pool |     142    |     147    |   14671 K  |   14671 K  |\n","|       from small pool |     184    |     268    |   19796 K  |   19795 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      55    |   25049    |   25034    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      46    |   24859    |   24853    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      38    |   17239 K  |   17239 K  |\n","|       from large pool |      14    |      15    |    8455 K  |    8455 K  |\n","|       from small pool |      10    |      31    |    8783 K  |    8783 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:22 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  66% 2060/3126 [04:05<02:05,  8.50it/s, loss=1.806, ppl=3.5, wps=23249, ups=8.23, wpb=2823.3, bsz=128, num_updates=42100, lr=0.0005, gnorm=1.266, loss_scale=8, train_wall=11, gb_free=8.8, wall=5380]2022-05-26 13:08:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.22 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:08:42 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 507          |        cudaMalloc retries: 571       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7393 MB |   10060 MB |  219906 GB |  219899 GB |\n","|       from large pool |    7383 MB |   10049 MB |  215232 GB |  215225 GB |\n","|       from small pool |      10 MB |      17 MB |    4673 GB |    4673 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7393 MB |   10060 MB |  219906 GB |  219899 GB |\n","|       from large pool |    7383 MB |   10049 MB |  215232 GB |  215225 GB |\n","|       from small pool |      10 MB |      17 MB |    4673 GB |    4673 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548714 MB |  535490 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   49820 MB |   49808 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5830 MB |    5830 MB |  439330 GB |  439324 GB |\n","|       from large pool |    5828 MB |    5828 MB |  433971 GB |  433965 GB |\n","|       from small pool |       1 MB |      15 MB |    5358 GB |    5358 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34599 K  |   34598 K  |\n","|       from large pool |     142    |     146    |   14728 K  |   14728 K  |\n","|       from small pool |     184    |     268    |   19870 K  |   19870 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34599 K  |   34598 K  |\n","|       from large pool |     142    |     146    |   14728 K  |   14728 K  |\n","|       from small pool |     184    |     268    |   19870 K  |   19870 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25100    |   25085    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   24910    |   24904    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      21    |      40    |   17305 K  |   17305 K  |\n","|       from large pool |      12    |      13    |    8489 K  |    8489 K  |\n","|       from small pool |       9    |      32    |    8816 K  |    8816 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  68% 2137/3126 [04:14<02:07,  7.77it/s, loss=1.928, ppl=3.81, wps=24585.4, ups=7.99, wpb=3078, bsz=128, num_updates=42200, lr=0.0005, gnorm=1.267, loss_scale=8, train_wall=12, gb_free=9.9, wall=5392]2022-05-26 13:08:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.91 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:08:51 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 508          |        cudaMalloc retries: 572       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9122 MB |   12568 MB |  220293 GB |  220284 GB |\n","|       from large pool |    9103 MB |   12549 MB |  215611 GB |  215602 GB |\n","|       from small pool |      18 MB |      19 MB |    4682 GB |    4681 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9122 MB |   12568 MB |  220293 GB |  220284 GB |\n","|       from large pool |    9103 MB |   12549 MB |  215611 GB |  215602 GB |\n","|       from small pool |      18 MB |      19 MB |    4682 GB |    4681 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13326 MB |  548816 MB |  535584 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |     114 MB |   49922 MB |   49902 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4109 MB |    4542 MB |  440113 GB |  440109 GB |\n","|       from large pool |    4108 MB |    4540 MB |  434745 GB |  434741 GB |\n","|       from small pool |       1 MB |       5 MB |    5368 GB |    5368 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34659 K  |   34659 K  |\n","|       from large pool |     132    |     136    |   14754 K  |   14754 K  |\n","|       from small pool |     194    |     268    |   19905 K  |   19905 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34659 K  |   34659 K  |\n","|       from large pool |     132    |     136    |   14754 K  |   14754 K  |\n","|       from small pool |     194    |     268    |   19905 K  |   19905 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      66    |   25151    |   25132    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      57    |   24961    |   24951    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      28    |   17336 K  |   17336 K  |\n","|       from large pool |      13    |      14    |    8504 K  |    8504 K  |\n","|       from small pool |      11    |      19    |    8832 K  |    8832 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  68% 2139/3126 [04:15<02:19,  7.09it/s, loss=1.928, ppl=3.81, wps=24585.4, ups=7.99, wpb=3078, bsz=128, num_updates=42200, lr=0.0005, gnorm=1.267, loss_scale=8, train_wall=12, gb_free=9.9, wall=5392]2022-05-26 13:08:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.50 GiB (GPU 0; 14.76 GiB total capacity; 6.56 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:08:52 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 509          |        cudaMalloc retries: 573       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6712 MB |    9028 MB |  220308 GB |  220302 GB |\n","|       from large pool |    6702 MB |    9017 MB |  215626 GB |  215620 GB |\n","|       from small pool |      10 MB |      17 MB |    4682 GB |    4682 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6712 MB |    9028 MB |  220308 GB |  220302 GB |\n","|       from large pool |    6702 MB |    9017 MB |  215626 GB |  215620 GB |\n","|       from small pool |      10 MB |      17 MB |    4682 GB |    4682 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13232 MB |  548816 MB |  535592 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |      20 MB |   49922 MB |   49910 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6511 MB |    6511 MB |  440137 GB |  440131 GB |\n","|       from large pool |    6509 MB |    6509 MB |  434769 GB |  434762 GB |\n","|       from small pool |       1 MB |       5 MB |    5368 GB |    5368 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34661 K  |   34660 K  |\n","|       from large pool |     142    |     147    |   14754 K  |   14754 K  |\n","|       from small pool |     184    |     268    |   19906 K  |   19905 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34661 K  |   34660 K  |\n","|       from large pool |     142    |     147    |   14754 K  |   14754 K  |\n","|       from small pool |     184    |     268    |   19906 K  |   19905 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      19    |   25151    |   25136    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      10    |   24961    |   24955    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      30    |   17336 K  |   17336 K  |\n","|       from large pool |      20    |      21    |    8504 K  |    8504 K  |\n","|       from small pool |       8    |      20    |    8832 K  |    8832 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:08:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  72% 2242/3126 [04:27<01:52,  7.88it/s, loss=1.879, ppl=3.68, wps=24386.1, ups=8.08, wpb=3016.4, bsz=128, num_updates=42300, lr=0.0005, gnorm=1.233, loss_scale=8, train_wall=12, gb_free=9.6, wall=5405]2022-05-26 13:09:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.13 GiB (GPU 0; 14.76 GiB total capacity; 8.58 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:04 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 510          |        cudaMalloc retries: 574       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8785 MB |   11431 MB |  220841 GB |  220832 GB |\n","|       from large pool |    8774 MB |   11419 MB |  216148 GB |  216140 GB |\n","|       from small pool |      11 MB |      17 MB |    4692 GB |    4692 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8785 MB |   11431 MB |  220841 GB |  220832 GB |\n","|       from large pool |    8774 MB |   11419 MB |  216148 GB |  216140 GB |\n","|       from small pool |      11 MB |      17 MB |    4692 GB |    4692 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  548918 MB |  535694 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   50024 MB |   50012 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4438 MB |    4492 MB |  441204 GB |  441200 GB |\n","|       from large pool |    4437 MB |    4490 MB |  435824 GB |  435820 GB |\n","|       from small pool |       0 MB |       3 MB |    5380 GB |    5380 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34742 K  |   34741 K  |\n","|       from large pool |     142    |     147    |   14790 K  |   14790 K  |\n","|       from small pool |     184    |     268    |   19951 K  |   19951 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34742 K  |   34741 K  |\n","|       from large pool |     142    |     147    |   14790 K  |   14790 K  |\n","|       from small pool |     184    |     268    |   19951 K  |   19951 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25202    |   25187    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25012    |   25006    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      28    |   17377 K  |   17377 K  |\n","|       from large pool |      18    |      19    |    8524 K  |    8524 K  |\n","|       from small pool |       9    |      19    |    8852 K  |    8852 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  74% 2326/3126 [04:37<01:40,  7.93it/s, loss=1.985, ppl=3.96, wps=24134, ups=7.96, wpb=3030.5, bsz=128, num_updates=42400, lr=0.0005, gnorm=1.257, loss_scale=8, train_wall=12, gb_free=11, wall=5417]2022-05-26 13:09:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 7.02 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:14 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 511          |        cudaMalloc retries: 575       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7193 MB |    9710 MB |  221243 GB |  221236 GB |\n","|       from large pool |    7183 MB |    9699 MB |  216541 GB |  216534 GB |\n","|       from small pool |      10 MB |      17 MB |    4702 GB |    4702 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7193 MB |    9710 MB |  221243 GB |  221236 GB |\n","|       from large pool |    7183 MB |    9699 MB |  216541 GB |  216534 GB |\n","|       from small pool |      10 MB |      17 MB |    4702 GB |    4702 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  549016 MB |  535792 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   50122 MB |   50110 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6030 MB |    6030 MB |  442009 GB |  442003 GB |\n","|       from large pool |    6028 MB |    6028 MB |  436617 GB |  436611 GB |\n","|       from small pool |       1 MB |      36 MB |    5392 GB |    5392 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34807 K  |   34806 K  |\n","|       from large pool |     142    |     147    |   14816 K  |   14816 K  |\n","|       from small pool |     184    |     268    |   19990 K  |   19990 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34807 K  |   34806 K  |\n","|       from large pool |     142    |     147    |   14816 K  |   14816 K  |\n","|       from small pool |     184    |     268    |   19990 K  |   19990 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   25251    |   25236    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   25061    |   25055    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      50    |   17409 K  |   17409 K  |\n","|       from large pool |      19    |      20    |    8539 K  |    8539 K  |\n","|       from small pool |       8    |      42    |    8869 K  |    8869 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  76% 2372/3126 [04:43<01:36,  7.84it/s, loss=1.985, ppl=3.96, wps=24134, ups=7.96, wpb=3030.5, bsz=128, num_updates=42400, lr=0.0005, gnorm=1.257, loss_scale=8, train_wall=12, gb_free=11, wall=5417]2022-05-26 13:09:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.28 GiB already allocated; 609.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:20 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 512          |        cudaMalloc retries: 576       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6432 MB |    8696 MB |  221485 GB |  221479 GB |\n","|       from large pool |    6412 MB |    8676 MB |  216778 GB |  216772 GB |\n","|       from small pool |      19 MB |      20 MB |    4706 GB |    4706 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6432 MB |    8696 MB |  221485 GB |  221479 GB |\n","|       from large pool |    6412 MB |    8676 MB |  216778 GB |  216772 GB |\n","|       from small pool |      19 MB |      20 MB |    4706 GB |    4706 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13234 MB |   13326 MB |  549118 MB |  535884 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      22 MB |     114 MB |   50224 MB |   50202 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6801 MB |    6801 MB |  442490 GB |  442484 GB |\n","|       from large pool |    6799 MB |    6799 MB |  437093 GB |  437086 GB |\n","|       from small pool |       2 MB |      20 MB |    5397 GB |    5397 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34843 K  |   34843 K  |\n","|       from large pool |     132    |     136    |   14833 K  |   14832 K  |\n","|       from small pool |     194    |     268    |   20010 K  |   20010 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34843 K  |   34843 K  |\n","|       from large pool |     132    |     136    |   14833 K  |   14832 K  |\n","|       from small pool |     194    |     268    |   20010 K  |   20010 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      20    |      66    |   25302    |   25282    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      11    |      57    |   25112    |   25101    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      42    |   17427 K  |   17427 K  |\n","|       from large pool |      10    |      11    |    8549 K  |    8549 K  |\n","|       from small pool |      15    |      35    |    8878 K  |    8878 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  77% 2395/3126 [04:46<01:43,  7.08it/s, loss=1.927, ppl=3.8, wps=24662.1, ups=8.12, wpb=3036.2, bsz=128, num_updates=42500, lr=0.0005, gnorm=1.241, loss_scale=8, train_wall=12, gb_free=11, wall=5430]2022-05-26 13:09:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.86 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:23 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 513          |        cudaMalloc retries: 577       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9069 MB |   12416 MB |  221633 GB |  221624 GB |\n","|       from large pool |    9059 MB |   12406 MB |  216925 GB |  216916 GB |\n","|       from small pool |      10 MB |      17 MB |    4708 GB |    4708 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9069 MB |   12416 MB |  221633 GB |  221624 GB |\n","|       from large pool |    9059 MB |   12406 MB |  216925 GB |  216916 GB |\n","|       from small pool |      10 MB |      17 MB |    4708 GB |    4708 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13320 MB |  549204 MB |  535978 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     108 MB |   50310 MB |   50296 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4156 MB |    4516 MB |  442761 GB |  442757 GB |\n","|       from large pool |    4152 MB |    4512 MB |  437362 GB |  437358 GB |\n","|       from small pool |       3 MB |      18 MB |    5399 GB |    5399 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34861 K  |   34861 K  |\n","|       from large pool |     142    |     147    |   14841 K  |   14841 K  |\n","|       from small pool |     184    |     268    |   20020 K  |   20020 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34861 K  |   34861 K  |\n","|       from large pool |     142    |     147    |   14841 K  |   14841 K  |\n","|       from small pool |     184    |     268    |   20020 K  |   20020 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      63    |   25345    |   25329    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      54    |   25155    |   25148    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      39    |   17436 K  |   17436 K  |\n","|       from large pool |      14    |      15    |    8553 K  |    8553 K  |\n","|       from small pool |      10    |      32    |    8882 K  |    8882 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  79% 2480/3126 [04:56<01:22,  7.84it/s, loss=1.908, ppl=3.75, wps=23849.3, ups=8.27, wpb=2883.3, bsz=127.9, num_updates=42600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=10.5, wall=5442]2022-05-26 13:09:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.25 GiB (GPU 0; 14.76 GiB total capacity; 6.19 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:33 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 514          |        cudaMalloc retries: 578       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6340 MB |    8529 MB |  222019 GB |  222013 GB |\n","|       from large pool |    6330 MB |    8518 MB |  217300 GB |  217293 GB |\n","|       from small pool |       9 MB |      17 MB |    4719 GB |    4719 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6340 MB |    8529 MB |  222019 GB |  222013 GB |\n","|       from large pool |    6330 MB |    8518 MB |  217300 GB |  217293 GB |\n","|       from small pool |       9 MB |      17 MB |    4719 GB |    4719 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  549304 MB |  536080 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   50410 MB |   50398 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6883 MB |    6883 MB |  443565 GB |  443559 GB |\n","|       from large pool |    6881 MB |    6881 MB |  438154 GB |  438147 GB |\n","|       from small pool |       2 MB |      29 MB |    5411 GB |    5411 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34929 K  |   34928 K  |\n","|       from large pool |     142    |     146    |   14867 K  |   14867 K  |\n","|       from small pool |     184    |     267    |   20061 K  |   20061 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34929 K  |   34928 K  |\n","|       from large pool |     142    |     146    |   14867 K  |   14867 K  |\n","|       from small pool |     184    |     267    |   20061 K  |   20061 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25395    |   25380    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25205    |   25199    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      60    |   17470 K  |   17470 K  |\n","|       from large pool |      18    |      19    |    8568 K  |    8568 K  |\n","|       from small pool |       8    |      52    |    8902 K  |    8902 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  79% 2484/3126 [04:56<01:11,  8.95it/s, loss=1.908, ppl=3.75, wps=23849.3, ups=8.27, wpb=2883.3, bsz=127.9, num_updates=42600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=10.5, wall=5442]2022-05-26 13:09:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.18 GiB already allocated; 607.75 MiB free; 12.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  80% 2486/3126 [04:57<01:12,  8.84it/s, loss=1.908, ppl=3.75, wps=23849.3, ups=8.27, wpb=2883.3, bsz=127.9, num_updates=42600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=10.5, wall=5442]2022-05-26 13:09:34 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 515          |        cudaMalloc retries: 579       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7350 MB |   10042 MB |  222037 GB |  222030 GB |\n","|       from large pool |    7328 MB |   10019 MB |  217318 GB |  217310 GB |\n","|       from small pool |      21 MB |      22 MB |    4719 GB |    4719 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7350 MB |   10042 MB |  222037 GB |  222030 GB |\n","|       from large pool |    7328 MB |   10019 MB |  217318 GB |  217310 GB |\n","|       from small pool |      21 MB |      22 MB |    4719 GB |    4719 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13236 MB |   13288 MB |  549368 MB |  536132 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      24 MB |      76 MB |   50474 MB |   50450 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5885 MB |    5885 MB |  443599 GB |  443593 GB |\n","|       from large pool |    5883 MB |    5883 MB |  438187 GB |  438181 GB |\n","|       from small pool |       2 MB |      40 MB |    5411 GB |    5411 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34931 K  |   34931 K  |\n","|       from large pool |     126    |     130    |   14868 K  |   14868 K  |\n","|       from small pool |     200    |     268    |   20063 K  |   20063 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34931 K  |   34931 K  |\n","|       from large pool |     126    |     130    |   14868 K  |   14868 K  |\n","|       from small pool |     200    |     268    |   20063 K  |   20063 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      47    |   25427    |   25406    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      12    |      38    |   25237    |   25225    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      64    |   17471 K  |   17471 K  |\n","|       from large pool |      11    |      12    |    8569 K  |    8569 K  |\n","|       from small pool |      15    |      56    |    8902 K  |    8902 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  82% 2557/3126 [05:05<00:56, 10.02it/s, loss=1.908, ppl=3.75, wps=23849.3, ups=8.27, wpb=2883.3, bsz=127.9, num_updates=42600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=10.5, wall=5442]2022-05-26 13:09:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:42 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 516          |        cudaMalloc retries: 580       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6372 MB |    8537 MB |  222394 GB |  222388 GB |\n","|       from large pool |    6362 MB |    8527 MB |  217668 GB |  217662 GB |\n","|       from small pool |      10 MB |      17 MB |    4726 GB |    4726 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6372 MB |    8537 MB |  222394 GB |  222388 GB |\n","|       from large pool |    6362 MB |    8527 MB |  217668 GB |  217662 GB |\n","|       from small pool |      10 MB |      17 MB |    4726 GB |    4726 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  549458 MB |  536234 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   50564 MB |   50552 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6851 MB |    6851 MB |  444315 GB |  444308 GB |\n","|       from large pool |    6849 MB |    6849 MB |  438895 GB |  438888 GB |\n","|       from small pool |       1 MB |      33 MB |    5419 GB |    5419 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34988 K  |   34988 K  |\n","|       from large pool |     142    |     147    |   14893 K  |   14893 K  |\n","|       from small pool |     184    |     268    |   20095 K  |   20094 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34988 K  |   34988 K  |\n","|       from large pool |     142    |     147    |   14893 K  |   14893 K  |\n","|       from small pool |     184    |     268    |   20095 K  |   20094 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25472    |   25457    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25282    |   25276    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      51    |   17500 K  |   17500 K  |\n","|       from large pool |      19    |      20    |    8583 K  |    8583 K  |\n","|       from small pool |       9    |      43    |    8916 K  |    8916 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  82% 2563/3126 [05:06<01:11,  7.88it/s, loss=1.908, ppl=3.75, wps=23849.3, ups=8.27, wpb=2883.3, bsz=127.9, num_updates=42600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=10.5, wall=5442]2022-05-26 13:09:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.32 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 014:  82% 2564/3126 [05:06<01:11,  7.82it/s, loss=1.908, ppl=3.75, wps=23849.3, ups=8.27, wpb=2883.3, bsz=127.9, num_updates=42600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=10.5, wall=5442]2022-05-26 13:09:43 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 517          |        cudaMalloc retries: 581       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6470 MB |    8634 MB |  222419 GB |  222413 GB |\n","|       from large pool |    6460 MB |    8624 MB |  217692 GB |  217686 GB |\n","|       from small pool |      10 MB |      17 MB |    4726 GB |    4726 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6470 MB |    8634 MB |  222419 GB |  222413 GB |\n","|       from large pool |    6460 MB |    8624 MB |  217692 GB |  217686 GB |\n","|       from small pool |      10 MB |      17 MB |    4726 GB |    4726 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13320 MB |  549554 MB |  536330 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     108 MB |   50660 MB |   50648 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6753 MB |    6753 MB |  444360 GB |  444354 GB |\n","|       from large pool |    6751 MB |    6751 MB |  438940 GB |  438933 GB |\n","|       from small pool |       1 MB |      19 MB |    5420 GB |    5420 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   34992 K  |   34991 K  |\n","|       from large pool |     142    |     147    |   14895 K  |   14895 K  |\n","|       from small pool |     184    |     268    |   20096 K  |   20096 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   34992 K  |   34991 K  |\n","|       from large pool |     142    |     147    |   14895 K  |   14895 K  |\n","|       from small pool |     184    |     268    |   20096 K  |   20096 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      63    |   25520    |   25505    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      54    |   25330    |   25324    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      42    |   17502 K  |   17502 K  |\n","|       from large pool |      19    |      20    |    8584 K  |    8584 K  |\n","|       from small pool |       7    |      34    |    8917 K  |    8917 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  85% 2647/3126 [05:17<00:57,  8.31it/s, loss=1.835, ppl=3.57, wps=23572.1, ups=7.95, wpb=2966.8, bsz=128, num_updates=42700, lr=0.0005, gnorm=1.241, loss_scale=8, train_wall=12, gb_free=8.3, wall=5454]2022-05-26 13:09:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.69 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:09:54 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 518          |        cudaMalloc retries: 582       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7877 MB |   10546 MB |  222875 GB |  222867 GB |\n","|       from large pool |    7866 MB |   10535 MB |  218140 GB |  218132 GB |\n","|       from small pool |      10 MB |      17 MB |    4734 GB |    4734 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7877 MB |   10546 MB |  222875 GB |  222867 GB |\n","|       from large pool |    7866 MB |   10535 MB |  218140 GB |  218132 GB |\n","|       from small pool |      10 MB |      17 MB |    4734 GB |    4734 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  549656 MB |  536432 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   50762 MB |   50750 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5346 MB |    5346 MB |  445269 GB |  445264 GB |\n","|       from large pool |    5345 MB |    5345 MB |  439840 GB |  439835 GB |\n","|       from small pool |       1 MB |      13 MB |    5429 GB |    5429 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35058 K  |   35058 K  |\n","|       from large pool |     142    |     147    |   14925 K  |   14925 K  |\n","|       from small pool |     184    |     268    |   20133 K  |   20133 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35058 K  |   35058 K  |\n","|       from large pool |     142    |     147    |   14925 K  |   14925 K  |\n","|       from small pool |     184    |     268    |   20133 K  |   20133 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25571    |   25556    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25381    |   25375    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      39    |   17535 K  |   17535 K  |\n","|       from large pool |      20    |      21    |    8601 K  |    8601 K  |\n","|       from small pool |      10    |      31    |    8933 K  |    8933 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:09:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  86% 2698/3126 [05:23<00:50,  8.51it/s, loss=2.01, ppl=4.03, wps=24890.6, ups=8.03, wpb=3100.6, bsz=128, num_updates=42800, lr=0.0005, gnorm=1.255, loss_scale=8, train_wall=12, gb_free=12.4, wall=5467]2022-05-26 13:10:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.98 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:10:00 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 519          |        cudaMalloc retries: 583       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9199 MB |   12646 MB |  223121 GB |  223112 GB |\n","|       from large pool |    9189 MB |   12636 MB |  218381 GB |  218372 GB |\n","|       from small pool |      10 MB |      17 MB |    4740 GB |    4740 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9199 MB |   12646 MB |  223121 GB |  223112 GB |\n","|       from large pool |    9189 MB |   12636 MB |  218381 GB |  218372 GB |\n","|       from small pool |      10 MB |      17 MB |    4740 GB |    4740 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  549758 MB |  536532 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   50864 MB |   50850 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4026 MB |    4516 MB |  445761 GB |  445757 GB |\n","|       from large pool |    4022 MB |    4512 MB |  440325 GB |  440321 GB |\n","|       from small pool |       3 MB |      21 MB |    5436 GB |    5436 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35098 K  |   35098 K  |\n","|       from large pool |     142    |     147    |   14941 K  |   14941 K  |\n","|       from small pool |     184    |     268    |   20156 K  |   20156 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35098 K  |   35098 K  |\n","|       from large pool |     142    |     147    |   14941 K  |   14941 K  |\n","|       from small pool |     184    |     268    |   20156 K  |   20156 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   25622    |   25606    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   25432    |   25425    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      39    |   17555 K  |   17555 K  |\n","|       from large pool |      16    |      17    |    8611 K  |    8611 K  |\n","|       from small pool |      11    |      32    |    8944 K  |    8944 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:10:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  95% 2975/3126 [05:57<00:20,  7.39it/s, loss=1.928, ppl=3.81, wps=24703.4, ups=8.33, wpb=2966.4, bsz=128, num_updates=43000, lr=0.0005, gnorm=1.254, loss_scale=8, train_wall=12, gb_free=9.8, wall=5491]2022-05-26 13:10:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.97 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:10:34 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 520          |        cudaMalloc retries: 584       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7133 MB |    9699 MB |  224537 GB |  224530 GB |\n","|       from large pool |    7123 MB |    9688 MB |  219768 GB |  219761 GB |\n","|       from small pool |      10 MB |      17 MB |    4769 GB |    4769 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7133 MB |    9699 MB |  224537 GB |  224530 GB |\n","|       from large pool |    7123 MB |    9688 MB |  219768 GB |  219761 GB |\n","|       from small pool |      10 MB |      17 MB |    4769 GB |    4769 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  549858 MB |  536634 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   50964 MB |   50952 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6090 MB |    6090 MB |  448618 GB |  448613 GB |\n","|       from large pool |    6088 MB |    6088 MB |  443150 GB |  443144 GB |\n","|       from small pool |       1 MB |      24 MB |    5468 GB |    5468 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35319 K  |   35318 K  |\n","|       from large pool |     142    |     146    |   15039 K  |   15039 K  |\n","|       from small pool |     184    |     268    |   20279 K  |   20279 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35319 K  |   35318 K  |\n","|       from large pool |     142    |     146    |   15039 K  |   15039 K  |\n","|       from small pool |     184    |     268    |   20279 K  |   20279 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25672    |   25657    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25482    |   25476    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      13    |      40    |   17666 K  |   17666 K  |\n","|       from large pool |       6    |       7    |    8667 K  |    8667 K  |\n","|       from small pool |       7    |      33    |    8998 K  |    8998 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:10:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  98% 3053/3126 [06:06<00:08,  8.19it/s, loss=2.141, ppl=4.41, wps=25026.6, ups=7.81, wpb=3206.4, bsz=128, num_updates=43100, lr=0.0005, gnorm=1.275, loss_scale=8, train_wall=12, gb_free=8.6, wall=5504]2022-05-26 13:10:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.46 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:10:43 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 521          |        cudaMalloc retries: 585       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6610 MB |    8875 MB |  224915 GB |  224909 GB |\n","|       from large pool |    6600 MB |    8865 MB |  220137 GB |  220130 GB |\n","|       from small pool |      10 MB |      17 MB |    4778 GB |    4778 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6610 MB |    8875 MB |  224915 GB |  224909 GB |\n","|       from large pool |    6600 MB |    8865 MB |  220137 GB |  220130 GB |\n","|       from small pool |      10 MB |      17 MB |    4778 GB |    4778 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  549960 MB |  536736 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   51066 MB |   51054 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6613 MB |    6613 MB |  449407 GB |  449400 GB |\n","|       from large pool |    6611 MB |    6611 MB |  443927 GB |  443921 GB |\n","|       from small pool |       1 MB |      36 MB |    5479 GB |    5479 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35379 K  |   35379 K  |\n","|       from large pool |     142    |     147    |   15064 K  |   15063 K  |\n","|       from small pool |     184    |     268    |   20315 K  |   20315 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35379 K  |   35379 K  |\n","|       from large pool |     142    |     147    |   15064 K  |   15063 K  |\n","|       from small pool |     184    |     268    |   20315 K  |   20315 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25723    |   25708    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25533    |   25527    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      54    |   17696 K  |   17696 K  |\n","|       from large pool |      17    |      18    |    8681 K  |    8681 K  |\n","|       from small pool |       8    |      46    |    9014 K  |    9014 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:10:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  99% 3094/3126 [06:11<00:04,  7.57it/s, loss=1.943, ppl=3.84, wps=24969.8, ups=8.27, wpb=3017.9, bsz=128, num_updates=43200, lr=0.0005, gnorm=1.236, loss_scale=8, train_wall=12, gb_free=9.2, wall=5516]2022-05-26 13:10:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.83 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:10:48 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 522          |        cudaMalloc retries: 586       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8020 MB |   10889 MB |  225114 GB |  225106 GB |\n","|       from large pool |    8010 MB |   10879 MB |  220332 GB |  220324 GB |\n","|       from small pool |      10 MB |      17 MB |    4782 GB |    4782 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8020 MB |   10889 MB |  225114 GB |  225106 GB |\n","|       from large pool |    8010 MB |   10879 MB |  220332 GB |  220324 GB |\n","|       from small pool |      10 MB |      17 MB |    4782 GB |    4782 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  550062 MB |  536836 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   51168 MB |   51154 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5205 MB |    5205 MB |  449818 GB |  449813 GB |\n","|       from large pool |    5201 MB |    5201 MB |  444334 GB |  444329 GB |\n","|       from small pool |       3 MB |      18 MB |    5483 GB |    5483 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35410 K  |   35410 K  |\n","|       from large pool |     142    |     147    |   15078 K  |   15077 K  |\n","|       from small pool |     184    |     268    |   20332 K  |   20332 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35410 K  |   35410 K  |\n","|       from large pool |     142    |     147    |   15078 K  |   15077 K  |\n","|       from small pool |     184    |     268    |   20332 K  |   20332 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   25774    |   25758    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   25584    |   25577    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      45    |   17712 K  |   17712 K  |\n","|       from large pool |      17    |      18    |    8690 K  |    8690 K  |\n","|       from small pool |      10    |      38    |    9022 K  |    9022 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:10:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014:  99% 3110/3126 [06:13<00:01, 10.94it/s, loss=1.943, ppl=3.84, wps=24969.8, ups=8.27, wpb=3017.9, bsz=128, num_updates=43200, lr=0.0005, gnorm=1.236, loss_scale=8, train_wall=12, gb_free=9.2, wall=5516]2022-05-26 13:10:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.84 GiB (GPU 0; 14.76 GiB total capacity; 6.82 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:10:49 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 523          |        cudaMalloc retries: 587       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6985 MB |    9475 MB |  225173 GB |  225166 GB |\n","|       from large pool |    6974 MB |    9464 MB |  220388 GB |  220381 GB |\n","|       from small pool |      10 MB |      17 MB |    4784 GB |    4784 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6985 MB |    9475 MB |  225173 GB |  225166 GB |\n","|       from large pool |    6974 MB |    9464 MB |  220388 GB |  220381 GB |\n","|       from small pool |      10 MB |      17 MB |    4784 GB |    4784 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13324 MB |  550160 MB |  536936 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     112 MB |   51266 MB |   51254 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6238 MB |    6238 MB |  449944 GB |  449938 GB |\n","|       from large pool |    6237 MB |    6237 MB |  444458 GB |  444451 GB |\n","|       from small pool |       1 MB |      37 MB |    5486 GB |    5486 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35423 K  |   35423 K  |\n","|       from large pool |     142    |     146    |   15081 K  |   15081 K  |\n","|       from small pool |     184    |     268    |   20341 K  |   20341 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35423 K  |   35423 K  |\n","|       from large pool |     142    |     146    |   15081 K  |   15081 K  |\n","|       from small pool |     184    |     268    |   20341 K  |   20341 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      65    |   25823    |   25808    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      56    |   25633    |   25627    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      51    |   17718 K  |   17718 K  |\n","|       from large pool |      11    |      12    |    8692 K  |    8692 K  |\n","|       from small pool |       9    |      44    |    9026 K  |    9026 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:10:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 014: 100% 3125/3126 [06:14<00:00,  8.56it/s, loss=1.943, ppl=3.84, wps=24969.8, ups=8.27, wpb=3017.9, bsz=128, num_updates=43200, lr=0.0005, gnorm=1.236, loss_scale=8, train_wall=12, gb_free=9.2, wall=5516]2022-05-26 13:10:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 014 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   0% 1/337 [00:00<00:45,  7.43it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   1% 5/337 [00:00<00:17, 19.12it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   3% 9/337 [00:00<00:13, 25.01it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   4% 12/337 [00:00<00:13, 23.94it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   5% 16/337 [00:00<00:11, 26.90it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   6% 20/337 [00:00<00:10, 30.60it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   7% 24/337 [00:00<00:10, 29.62it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:   8% 28/337 [00:01<00:10, 30.07it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  10% 33/337 [00:01<00:09, 31.55it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  11% 37/337 [00:01<00:09, 30.77it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  12% 41/337 [00:01<00:09, 30.60it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  13% 45/337 [00:01<00:10, 28.80it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  15% 49/337 [00:01<00:09, 30.67it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  16% 53/337 [00:01<00:09, 29.88it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  17% 57/337 [00:01<00:08, 32.04it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  18% 61/337 [00:02<00:09, 28.98it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  19% 65/337 [00:02<00:09, 30.06it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  20% 69/337 [00:02<00:08, 31.26it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  22% 73/337 [00:02<00:08, 30.35it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  23% 77/337 [00:02<00:08, 30.86it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  24% 81/337 [00:02<00:09, 28.15it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  25% 85/337 [00:02<00:08, 30.07it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  26% 89/337 [00:03<00:07, 31.08it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  28% 93/337 [00:03<00:08, 28.00it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  29% 97/337 [00:03<00:07, 30.01it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  30% 101/337 [00:03<00:07, 30.18it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  31% 105/337 [00:03<00:08, 28.62it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  32% 109/337 [00:03<00:07, 30.30it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  34% 113/337 [00:03<00:07, 29.27it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  34% 116/337 [00:03<00:07, 29.35it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  35% 119/337 [00:04<00:08, 27.13it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  36% 122/337 [00:04<00:07, 27.42it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  37% 126/337 [00:04<00:07, 28.56it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  38% 129/337 [00:04<00:07, 28.92it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  39% 132/337 [00:04<00:07, 25.63it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  40% 136/337 [00:04<00:07, 27.59it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  41% 139/337 [00:04<00:07, 27.93it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  42% 142/337 [00:04<00:07, 27.33it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  43% 145/337 [00:05<00:07, 25.05it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  44% 148/337 [00:05<00:07, 25.75it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  45% 152/337 [00:05<00:06, 27.25it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  46% 155/337 [00:05<00:06, 26.71it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  47% 158/337 [00:05<00:07, 25.22it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  48% 161/337 [00:05<00:06, 26.17it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  49% 164/337 [00:05<00:06, 25.96it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  50% 167/337 [00:05<00:06, 25.79it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  50% 170/337 [00:06<00:07, 22.83it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  52% 174/337 [00:06<00:06, 25.18it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  53% 177/337 [00:06<00:06, 25.54it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  53% 180/337 [00:06<00:06, 24.52it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  54% 183/337 [00:06<00:06, 23.81it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  55% 186/337 [00:06<00:06, 24.89it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  56% 189/337 [00:06<00:05, 24.97it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  57% 192/337 [00:06<00:05, 25.01it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  58% 195/337 [00:07<00:05, 23.85it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  59% 198/337 [00:07<00:05, 24.94it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  60% 201/337 [00:07<00:05, 24.85it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  61% 204/337 [00:07<00:05, 24.96it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  61% 207/337 [00:07<00:05, 23.47it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  62% 210/337 [00:07<00:05, 24.38it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  63% 213/337 [00:07<00:04, 24.97it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  64% 216/337 [00:07<00:04, 24.43it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  65% 219/337 [00:08<00:05, 23.13it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  66% 222/337 [00:08<00:04, 23.37it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  67% 225/337 [00:08<00:04, 23.46it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  68% 228/337 [00:08<00:05, 21.20it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  69% 231/337 [00:08<00:04, 22.08it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  69% 234/337 [00:08<00:04, 22.96it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  70% 237/337 [00:08<00:04, 20.96it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  71% 240/337 [00:09<00:04, 22.38it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  72% 243/337 [00:09<00:04, 22.91it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  73% 246/337 [00:09<00:04, 21.40it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  74% 249/337 [00:09<00:03, 22.41it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  75% 252/337 [00:09<00:03, 22.96it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  76% 255/337 [00:09<00:03, 20.72it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  77% 258/337 [00:09<00:03, 22.13it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  77% 261/337 [00:09<00:03, 21.72it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  78% 264/337 [00:10<00:03, 20.60it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  79% 267/337 [00:10<00:03, 21.16it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  80% 270/337 [00:10<00:03, 20.25it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  81% 273/337 [00:10<00:03, 20.16it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  82% 276/337 [00:10<00:02, 20.68it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  83% 279/337 [00:10<00:02, 19.41it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  84% 282/337 [00:11<00:02, 20.03it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  85% 285/337 [00:11<00:02, 17.34it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  85% 288/337 [00:11<00:02, 18.14it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  86% 290/337 [00:11<00:02, 18.13it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  87% 292/337 [00:11<00:02, 18.19it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  87% 294/337 [00:11<00:02, 18.48it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  88% 296/337 [00:11<00:02, 18.13it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  88% 298/337 [00:11<00:02, 18.12it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  89% 300/337 [00:12<00:02, 17.22it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  90% 302/337 [00:12<00:01, 17.61it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  90% 304/337 [00:12<00:01, 17.05it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  91% 307/337 [00:12<00:01, 17.67it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  92% 309/337 [00:12<00:01, 15.90it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  92% 311/337 [00:12<00:01, 16.53it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  93% 313/337 [00:12<00:01, 15.82it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  93% 315/337 [00:13<00:01, 14.93it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  94% 317/337 [00:13<00:01, 15.35it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  95% 319/337 [00:13<00:01, 15.41it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  95% 321/337 [00:13<00:01, 14.41it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  96% 323/337 [00:13<00:00, 14.93it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  96% 325/337 [00:13<00:00, 14.52it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  97% 327/337 [00:13<00:00, 14.09it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  98% 329/337 [00:14<00:00, 13.80it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  98% 331/337 [00:14<00:00, 13.30it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  99% 333/337 [00:14<00:00, 12.09it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset:  99% 335/337 [00:14<00:00, 11.62it/s]\u001b[A\n","epoch 014 | valid on 'valid' subset: 100% 337/337 [00:14<00:00, 12.92it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 13:11:06 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.014 | ppl 129.22 | wps 68405.4 | wpb 2965.1 | bsz 127.6 | num_updates 43236 | best_loss 5.602\n","2022-05-26 13:11:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 43236 updates\n","2022-05-26 13:11:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_en_de_with_backtranslations/checkpoint_last.pt\n","2022-05-26 13:11:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_en_de_with_backtranslations/checkpoint_last.pt\n","2022-05-26 13:11:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_en_de_with_backtranslations/checkpoint_last.pt (epoch 14 @ 43236 updates, score 7.014) (writing took 3.526562413000647 seconds)\n","2022-05-26 13:11:09 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n","2022-05-26 13:11:09 | INFO | train | epoch 014 | loss 1.818 | ppl 3.53 | wps 23246.4 | ups 7.86 | wpb 2958.2 | bsz 128 | num_updates 43236 | lr 0.0005 | gnorm 1.22 | loss_scale 8 | train_wall 358 | gb_free 9.7 | wall 5538\n","2022-05-26 13:11:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3126\n","epoch 015:   0% 0/3126 [00:00<?, ?it/s]2022-05-26 13:11:09 | INFO | fairseq.trainer | begin training epoch 15\n","2022-05-26 13:11:09 | INFO | fairseq_cli.train | Start iterating over samples\n","epoch 015:   0% 15/3126 [00:02<06:47,  7.64it/s]2022-05-26 13:11:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.84 GiB (GPU 0; 14.76 GiB total capacity; 6.82 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:11:12 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 524          |        cudaMalloc retries: 588       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6985 MB |    9475 MB |  226160 GB |  226153 GB |\n","|       from large pool |    6974 MB |    9464 MB |  221363 GB |  221356 GB |\n","|       from small pool |      10 MB |      17 MB |    4797 GB |    4797 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6985 MB |    9475 MB |  226160 GB |  226153 GB |\n","|       from large pool |    6974 MB |    9464 MB |  221363 GB |  221356 GB |\n","|       from small pool |      10 MB |      17 MB |    4797 GB |    4797 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  550262 MB |  537038 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   51368 MB |   51356 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6238 MB |    6238 MB |  452081 GB |  452075 GB |\n","|       from large pool |    6237 MB |    6237 MB |  446581 GB |  446575 GB |\n","|       from small pool |       1 MB |      14 MB |    5500 GB |    5500 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35528 K  |   35528 K  |\n","|       from large pool |     142    |     146    |   15130 K  |   15129 K  |\n","|       from small pool |     184    |     268    |   20398 K  |   20398 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35528 K  |   35528 K  |\n","|       from large pool |     142    |     146    |   15130 K  |   15129 K  |\n","|       from small pool |     184    |     268    |   20398 K  |   20398 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25874    |   25859    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25684    |   25678    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      19    |      37    |   17769 K  |   17769 K  |\n","|       from large pool |      11    |      12    |    8720 K  |    8720 K  |\n","|       from small pool |       8    |      30    |    9048 K  |    9048 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:11:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:   9% 274/3126 [00:32<05:53,  8.06it/s, loss=1.631, ppl=3.1, wps=25050.8, ups=8.56, wpb=2928.2, bsz=128, num_updates=43500, lr=0.0005, gnorm=1.185, loss_scale=8, train_wall=11, gb_free=12.1, wall=5570]2022-05-26 13:11:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.97 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:11:42 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 525          |        cudaMalloc retries: 589       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7133 MB |    9699 MB |  227407 GB |  227400 GB |\n","|       from large pool |    7123 MB |    9688 MB |  222582 GB |  222575 GB |\n","|       from small pool |      10 MB |      17 MB |    4825 GB |    4825 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7133 MB |    9699 MB |  227407 GB |  227400 GB |\n","|       from large pool |    7123 MB |    9688 MB |  222582 GB |  222575 GB |\n","|       from small pool |      10 MB |      17 MB |    4825 GB |    4825 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  550364 MB |  537140 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   51470 MB |   51458 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6090 MB |    6090 MB |  454661 GB |  454655 GB |\n","|       from large pool |    6088 MB |    6088 MB |  449129 GB |  449123 GB |\n","|       from small pool |       1 MB |      23 MB |    5531 GB |    5531 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35733 K  |   35733 K  |\n","|       from large pool |     142    |     146    |   15216 K  |   15216 K  |\n","|       from small pool |     184    |     268    |   20516 K  |   20516 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35733 K  |   35733 K  |\n","|       from large pool |     142    |     146    |   15216 K  |   15216 K  |\n","|       from small pool |     184    |     268    |   20516 K  |   20516 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25925    |   25910    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25735    |   25729    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      15    |      51    |   17872 K  |   17872 K  |\n","|       from large pool |       6    |       8    |    8770 K  |    8770 K  |\n","|       from small pool |       9    |      43    |    9101 K  |    9101 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:11:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  18% 576/3126 [01:08<05:27,  7.78it/s, loss=1.662, ppl=3.16, wps=24157.7, ups=8.84, wpb=2732.4, bsz=128, num_updates=43800, lr=0.0005, gnorm=1.217, loss_scale=8, train_wall=11, gb_free=11.8, wall=5606]2022-05-26 13:12:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.22 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:12:18 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 526          |        cudaMalloc retries: 590       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7393 MB |   10060 MB |  228866 GB |  228859 GB |\n","|       from large pool |    7383 MB |   10049 MB |  224009 GB |  224002 GB |\n","|       from small pool |      10 MB |      17 MB |    4857 GB |    4857 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7393 MB |   10060 MB |  228866 GB |  228859 GB |\n","|       from large pool |    7383 MB |   10049 MB |  224009 GB |  224002 GB |\n","|       from small pool |      10 MB |      17 MB |    4857 GB |    4857 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  550466 MB |  537242 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   51572 MB |   51560 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5830 MB |    5830 MB |  457660 GB |  457654 GB |\n","|       from large pool |    5828 MB |    5828 MB |  452091 GB |  452085 GB |\n","|       from small pool |       1 MB |      19 MB |    5568 GB |    5568 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   35971 K  |   35971 K  |\n","|       from large pool |     142    |     146    |   15317 K  |   15317 K  |\n","|       from small pool |     184    |     268    |   20654 K  |   20653 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   35971 K  |   35971 K  |\n","|       from large pool |     142    |     146    |   15317 K  |   15317 K  |\n","|       from small pool |     184    |     268    |   20654 K  |   20653 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   25976    |   25961    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25786    |   25780    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      45    |   17992 K  |   17992 K  |\n","|       from large pool |      12    |      13    |    8828 K  |    8828 K  |\n","|       from small pool |       8    |      37    |    9163 K  |    9163 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:12:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  23% 716/3126 [01:25<05:36,  7.16it/s, loss=1.642, ppl=3.12, wps=24322.8, ups=8.32, wpb=2923.9, bsz=128, num_updates=43900, lr=0.0005, gnorm=1.193, loss_scale=8, train_wall=12, gb_free=11.7, wall=5618]2022-05-26 13:12:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.91 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:12:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 527          |        cudaMalloc retries: 591       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9122 MB |   12568 MB |  229567 GB |  229558 GB |\n","|       from large pool |    9103 MB |   12549 MB |  224694 GB |  224685 GB |\n","|       from small pool |      18 MB |      19 MB |    4872 GB |    4872 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9122 MB |   12568 MB |  229567 GB |  229558 GB |\n","|       from large pool |    9103 MB |   12549 MB |  224694 GB |  224685 GB |\n","|       from small pool |      18 MB |      19 MB |    4872 GB |    4872 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13326 MB |  550568 MB |  537336 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |     114 MB |   51674 MB |   51654 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4109 MB |    4542 MB |  459094 GB |  459090 GB |\n","|       from large pool |    4108 MB |    4540 MB |  453508 GB |  453504 GB |\n","|       from small pool |       1 MB |      23 MB |    5586 GB |    5586 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36082 K  |   36082 K  |\n","|       from large pool |     132    |     136    |   15363 K  |   15363 K  |\n","|       from small pool |     194    |     268    |   20718 K  |   20718 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36082 K  |   36082 K  |\n","|       from large pool |     132    |     136    |   15363 K  |   15363 K  |\n","|       from small pool |     194    |     268    |   20718 K  |   20718 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      66    |   26027    |   26008    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      57    |   25837    |   25827    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      49    |   18048 K  |   18048 K  |\n","|       from large pool |      13    |      14    |    8855 K  |    8855 K  |\n","|       from small pool |      12    |      41    |    9192 K  |    9192 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:12:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  24% 741/3126 [01:28<04:37,  8.59it/s, loss=1.642, ppl=3.12, wps=24322.8, ups=8.32, wpb=2923.9, bsz=128, num_updates=43900, lr=0.0005, gnorm=1.193, loss_scale=8, train_wall=12, gb_free=11.7, wall=5618]2022-05-26 13:12:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.13 GiB (GPU 0; 14.76 GiB total capacity; 7.27 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:12:38 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 528          |        cudaMalloc retries: 592       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7441 MB |   10084 MB |  229690 GB |  229683 GB |\n","|       from large pool |    7431 MB |   10074 MB |  224814 GB |  224807 GB |\n","|       from small pool |      10 MB |      17 MB |    4875 GB |    4875 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7441 MB |   10084 MB |  229690 GB |  229683 GB |\n","|       from large pool |    7431 MB |   10074 MB |  224814 GB |  224807 GB |\n","|       from small pool |      10 MB |      17 MB |    4875 GB |    4875 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  550662 MB |  537436 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   51768 MB |   51754 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5784 MB |    5784 MB |  459340 GB |  459334 GB |\n","|       from large pool |    5780 MB |    5780 MB |  453750 GB |  453744 GB |\n","|       from small pool |       3 MB |      18 MB |    5589 GB |    5589 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36101 K  |   36101 K  |\n","|       from large pool |     142    |     147    |   15371 K  |   15371 K  |\n","|       from small pool |     184    |     268    |   20729 K  |   20729 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36101 K  |   36101 K  |\n","|       from large pool |     142    |     147    |   15371 K  |   15371 K  |\n","|       from small pool |     184    |     268    |   20729 K  |   20729 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   26074    |   26058    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   25884    |   25877    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      45    |   18058 K  |   18057 K  |\n","|       from large pool |      18    |      19    |    8860 K  |    8860 K  |\n","|       from small pool |      10    |      37    |    9197 K  |    9197 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:12:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  27% 852/3126 [01:42<04:24,  8.58it/s, loss=1.898, ppl=3.73, wps=24994.3, ups=7.59, wpb=3294.2, bsz=128, num_updates=44000, lr=0.0005, gnorm=1.178, loss_scale=8, train_wall=12, gb_free=6.2, wall=5631]2022-05-26 13:12:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.16 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 015:  27% 853/3126 [01:42<04:32,  8.36it/s, loss=1.898, ppl=3.73, wps=24994.3, ups=7.59, wpb=3294.2, bsz=128, num_updates=44000, lr=0.0005, gnorm=1.178, loss_scale=8, train_wall=12, gb_free=6.2, wall=5631]2022-05-26 13:12:52 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 529          |        cudaMalloc retries: 593       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6371 MB |    8511 MB |  230236 GB |  230230 GB |\n","|       from large pool |    6361 MB |    8500 MB |  225348 GB |  225342 GB |\n","|       from small pool |      10 MB |      17 MB |    4887 GB |    4887 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6371 MB |    8511 MB |  230236 GB |  230230 GB |\n","|       from large pool |    6361 MB |    8500 MB |  225348 GB |  225342 GB |\n","|       from small pool |      10 MB |      17 MB |    4887 GB |    4887 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  550762 MB |  537538 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   51868 MB |   51856 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6852 MB |    6852 MB |  460449 GB |  460442 GB |\n","|       from large pool |    6850 MB |    6850 MB |  454845 GB |  454838 GB |\n","|       from small pool |       1 MB |      17 MB |    5603 GB |    5603 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36188 K  |   36188 K  |\n","|       from large pool |     142    |     147    |   15408 K  |   15408 K  |\n","|       from small pool |     184    |     268    |   20780 K  |   20780 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36188 K  |   36188 K  |\n","|       from large pool |     142    |     147    |   15408 K  |   15408 K  |\n","|       from small pool |     184    |     268    |   20780 K  |   20780 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   26124    |   26109    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   25934    |   25928    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      41    |   18101 K  |   18101 K  |\n","|       from large pool |      12    |      13    |    8880 K  |    8880 K  |\n","|       from small pool |       8    |      33    |    9220 K  |    9220 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:12:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  29% 899/3126 [01:47<05:00,  7.42it/s, loss=1.567, ppl=2.96, wps=24056.8, ups=8.57, wpb=2805.5, bsz=128, num_updates=44100, lr=0.0005, gnorm=1.195, loss_scale=8, train_wall=11, gb_free=11, wall=5643]2022-05-26 13:12:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.84 GiB (GPU 0; 14.76 GiB total capacity; 6.85 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:12:58 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 530          |        cudaMalloc retries: 594       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7011 MB |    9502 MB |  230482 GB |  230476 GB |\n","|       from large pool |    7001 MB |    9491 MB |  225590 GB |  225583 GB |\n","|       from small pool |      10 MB |      17 MB |    4892 GB |    4892 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7011 MB |    9502 MB |  230482 GB |  230476 GB |\n","|       from large pool |    7001 MB |    9491 MB |  225590 GB |  225583 GB |\n","|       from small pool |      10 MB |      17 MB |    4892 GB |    4892 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  550860 MB |  537636 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   51966 MB |   51954 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6212 MB |    6212 MB |  460946 GB |  460940 GB |\n","|       from large pool |    6210 MB |    6210 MB |  455336 GB |  455330 GB |\n","|       from small pool |       1 MB |      31 MB |    5609 GB |    5609 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36226 K  |   36226 K  |\n","|       from large pool |     142    |     146    |   15424 K  |   15424 K  |\n","|       from small pool |     184    |     268    |   20801 K  |   20801 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36226 K  |   36226 K  |\n","|       from large pool |     142    |     146    |   15424 K  |   15424 K  |\n","|       from small pool |     184    |     268    |   20801 K  |   20801 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   26173    |   26158    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   25983    |   25977    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      19    |      52    |   18120 K  |   18120 K  |\n","|       from large pool |       9    |      10    |    8890 K  |    8890 K  |\n","|       from small pool |      10    |      45    |    9230 K  |    9230 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:12:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  31% 962/3126 [01:55<04:53,  7.37it/s, loss=1.567, ppl=2.96, wps=24056.8, ups=8.57, wpb=2805.5, bsz=128, num_updates=44100, lr=0.0005, gnorm=1.195, loss_scale=8, train_wall=11, gb_free=11, wall=5643]2022-05-26 13:13:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.22 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:13:05 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 531          |        cudaMalloc retries: 595       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6372 MB |    8537 MB |  230806 GB |  230800 GB |\n","|       from large pool |    6362 MB |    8527 MB |  225907 GB |  225901 GB |\n","|       from small pool |      10 MB |      17 MB |    4899 GB |    4899 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6372 MB |    8537 MB |  230806 GB |  230800 GB |\n","|       from large pool |    6362 MB |    8527 MB |  225907 GB |  225901 GB |\n","|       from small pool |      10 MB |      17 MB |    4899 GB |    4899 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  550962 MB |  537736 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   52068 MB |   52054 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6853 MB |    6853 MB |  461618 GB |  461611 GB |\n","|       from large pool |    6849 MB |    6849 MB |  456000 GB |  455994 GB |\n","|       from small pool |       3 MB |      41 MB |    5617 GB |    5617 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36276 K  |   36275 K  |\n","|       from large pool |     142    |     147    |   15446 K  |   15445 K  |\n","|       from small pool |     184    |     268    |   20830 K  |   20829 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36276 K  |   36275 K  |\n","|       from large pool |     142    |     147    |   15446 K  |   15445 K  |\n","|       from small pool |     184    |     268    |   20830 K  |   20829 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   26224    |   26208    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   26034    |   26027    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      29    |      53    |   18145 K  |   18145 K  |\n","|       from large pool |      19    |      20    |    8902 K  |    8902 K  |\n","|       from small pool |      10    |      45    |    9243 K  |    9243 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:13:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  42% 1312/3126 [02:37<03:30,  8.62it/s, loss=1.854, ppl=3.61, wps=25316.2, ups=8.05, wpb=3144.9, bsz=128, num_updates=44500, lr=0.0005, gnorm=1.231, loss_scale=8, train_wall=12, gb_free=11.4, wall=5691]2022-05-26 13:13:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.50 GiB (GPU 0; 14.76 GiB total capacity; 6.56 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:13:47 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 532          |        cudaMalloc retries: 596       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6712 MB |    9028 MB |  232540 GB |  232534 GB |\n","|       from large pool |    6702 MB |    9017 MB |  227601 GB |  227595 GB |\n","|       from small pool |      10 MB |      17 MB |    4939 GB |    4939 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6712 MB |    9028 MB |  232540 GB |  232534 GB |\n","|       from large pool |    6702 MB |    9017 MB |  227601 GB |  227595 GB |\n","|       from small pool |      10 MB |      17 MB |    4939 GB |    4939 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  551062 MB |  537838 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   52168 MB |   52156 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6511 MB |    6511 MB |  465112 GB |  465106 GB |\n","|       from large pool |    6509 MB |    6509 MB |  459450 GB |  459443 GB |\n","|       from small pool |       1 MB |      16 MB |    5662 GB |    5662 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36551 K  |   36551 K  |\n","|       from large pool |     142    |     147    |   15560 K  |   15560 K  |\n","|       from small pool |     184    |     268    |   20991 K  |   20991 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36551 K  |   36551 K  |\n","|       from large pool |     142    |     147    |   15560 K  |   15560 K  |\n","|       from small pool |     184    |     268    |   20991 K  |   20991 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   26274    |   26259    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   26084    |   26078    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      37    |   18283 K  |   18283 K  |\n","|       from large pool |      20    |      21    |    8967 K  |    8967 K  |\n","|       from small pool |       6    |      29    |    9316 K  |    9316 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:13:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  44% 1373/3126 [02:45<04:20,  6.74it/s, loss=1.813, ppl=3.51, wps=24607.8, ups=8.05, wpb=3056.9, bsz=128, num_updates=44600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=8, wall=5703]    2022-05-26 13:13:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.41 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:13:55 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 533          |        cudaMalloc retries: 597       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6559 MB |    8824 MB |  232845 GB |  232839 GB |\n","|       from large pool |    6549 MB |    8814 MB |  227900 GB |  227894 GB |\n","|       from small pool |      10 MB |      17 MB |    4944 GB |    4944 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6559 MB |    8824 MB |  232845 GB |  232839 GB |\n","|       from large pool |    6549 MB |    8814 MB |  227900 GB |  227894 GB |\n","|       from small pool |      10 MB |      17 MB |    4944 GB |    4944 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  551160 MB |  537936 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   52266 MB |   52254 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6664 MB |    6664 MB |  465733 GB |  465727 GB |\n","|       from large pool |    6662 MB |    6662 MB |  460063 GB |  460057 GB |\n","|       from small pool |       1 MB |       3 MB |    5669 GB |    5669 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     324    |     327    |   36599 K  |   36599 K  |\n","|       from large pool |     142    |     147    |   15581 K  |   15580 K  |\n","|       from small pool |     182    |     268    |   21018 K  |   21018 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     324    |     327    |   36599 K  |   36599 K  |\n","|       from large pool |     142    |     147    |   15581 K  |   15580 K  |\n","|       from small pool |     182    |     268    |   21018 K  |   21018 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   26323    |   26308    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   26133    |   26127    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      25    |   18307 K  |   18307 K  |\n","|       from large pool |      16    |      17    |    8979 K  |    8979 K  |\n","|       from small pool |       8    |      16    |    9327 K  |    9327 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:13:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  45% 1400/3126 [02:48<03:07,  9.21it/s, loss=1.813, ppl=3.51, wps=24607.8, ups=8.05, wpb=3056.9, bsz=128, num_updates=44600, lr=0.0005, gnorm=1.25, loss_scale=8, train_wall=12, gb_free=8, wall=5703]2022-05-26 13:13:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.31 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:13:58 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 534          |        cudaMalloc retries: 598       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6463 MB |    8727 MB |  232975 GB |  232969 GB |\n","|       from large pool |    6453 MB |    8716 MB |  228027 GB |  228021 GB |\n","|       from small pool |       9 MB |      17 MB |    4948 GB |    4948 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6463 MB |    8727 MB |  232975 GB |  232969 GB |\n","|       from large pool |    6453 MB |    8716 MB |  228027 GB |  228021 GB |\n","|       from small pool |       9 MB |      17 MB |    4948 GB |    4948 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  551258 MB |  538034 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   52364 MB |   52352 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6760 MB |    6760 MB |  466010 GB |  466004 GB |\n","|       from large pool |    6758 MB |    6758 MB |  460337 GB |  460330 GB |\n","|       from small pool |       2 MB |      39 MB |    5673 GB |    5673 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36621 K  |   36620 K  |\n","|       from large pool |     142    |     146    |   15589 K  |   15589 K  |\n","|       from small pool |     184    |     268    |   21031 K  |   21030 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36621 K  |   36620 K  |\n","|       from large pool |     142    |     146    |   15589 K  |   15589 K  |\n","|       from small pool |     184    |     268    |   21031 K  |   21030 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   26372    |   26357    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   26182    |   26176    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      16    |      57    |   18318 K  |   18318 K  |\n","|       from large pool |       8    |       9    |    8984 K  |    8984 K  |\n","|       from small pool |       8    |      49    |    9333 K  |    9333 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:13:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  52% 1613/3126 [03:13<02:46,  9.08it/s, loss=1.768, ppl=3.41, wps=25198.1, ups=8.33, wpb=3023.6, bsz=128, num_updates=44800, lr=0.0005, gnorm=1.24, loss_scale=8, train_wall=12, gb_free=12, wall=5728]2022-05-26 13:14:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.04 GiB (GPU 0; 14.76 GiB total capacity; 7.17 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:23 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 535          |        cudaMalloc retries: 599       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7340 MB |    9932 MB |  234016 GB |  234009 GB |\n","|       from large pool |    7330 MB |    9922 MB |  229047 GB |  229040 GB |\n","|       from small pool |      10 MB |      17 MB |    4969 GB |    4969 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7340 MB |    9932 MB |  234016 GB |  234009 GB |\n","|       from large pool |    7330 MB |    9922 MB |  229047 GB |  229040 GB |\n","|       from small pool |      10 MB |      17 MB |    4969 GB |    4969 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  551360 MB |  538134 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   52466 MB |   52452 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5885 MB |    5885 MB |  468133 GB |  468127 GB |\n","|       from large pool |    5881 MB |    5881 MB |  462435 GB |  462429 GB |\n","|       from small pool |       3 MB |      15 MB |    5697 GB |    5697 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36788 K  |   36788 K  |\n","|       from large pool |     142    |     147    |   15663 K  |   15663 K  |\n","|       from small pool |     184    |     268    |   21125 K  |   21124 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36788 K  |   36788 K  |\n","|       from large pool |     142    |     147    |   15663 K  |   15663 K  |\n","|       from small pool |     184    |     268    |   21125 K  |   21124 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   26423    |   26407    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   26233    |   26226    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      37    |   18402 K  |   18402 K  |\n","|       from large pool |      17    |      18    |    9027 K  |    9027 K  |\n","|       from small pool |       8    |      29    |    9375 K  |    9375 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 015:  52% 1614/3126 [03:14<02:58,  8.45it/s, loss=1.768, ppl=3.41, wps=25198.1, ups=8.33, wpb=3023.6, bsz=128, num_updates=44800, lr=0.0005, gnorm=1.24, loss_scale=8, train_wall=12, gb_free=12, wall=5728]2022-05-26 13:14:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  54% 1698/3126 [03:24<02:32,  9.39it/s, loss=1.745, ppl=3.35, wps=24747.3, ups=8.21, wpb=3014.8, bsz=128, num_updates=44900, lr=0.0005, gnorm=1.227, loss_scale=8, train_wall=12, gb_free=10.8, wall=5740]2022-05-26 13:14:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.45 GiB (GPU 0; 14.76 GiB total capacity; 6.42 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:34 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 536          |        cudaMalloc retries: 600       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6576 MB |    8865 MB |  234431 GB |  234425 GB |\n","|       from large pool |    6566 MB |    8854 MB |  229453 GB |  229447 GB |\n","|       from small pool |      10 MB |      17 MB |    4977 GB |    4977 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6576 MB |    8865 MB |  234431 GB |  234425 GB |\n","|       from large pool |    6566 MB |    8854 MB |  229453 GB |  229447 GB |\n","|       from small pool |      10 MB |      17 MB |    4977 GB |    4977 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  551460 MB |  538236 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   52566 MB |   52554 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6647 MB |    6647 MB |  468988 GB |  468982 GB |\n","|       from large pool |    6645 MB |    6645 MB |  463280 GB |  463274 GB |\n","|       from small pool |       1 MB |      19 MB |    5707 GB |    5707 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36855 K  |   36854 K  |\n","|       from large pool |     142    |     146    |   15692 K  |   15692 K  |\n","|       from small pool |     184    |     268    |   21162 K  |   21162 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36855 K  |   36854 K  |\n","|       from large pool |     142    |     146    |   15692 K  |   15692 K  |\n","|       from small pool |     184    |     268    |   21162 K  |   21162 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   26473    |   26458    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   26283    |   26277    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      44    |   18436 K  |   18436 K  |\n","|       from large pool |      17    |      18    |    9044 K  |    9044 K  |\n","|       from small pool |       9    |      36    |    9392 K  |    9392 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  55% 1711/3126 [03:25<02:49,  8.35it/s, loss=1.745, ppl=3.35, wps=24747.3, ups=8.21, wpb=3014.8, bsz=128, num_updates=44900, lr=0.0005, gnorm=1.227, loss_scale=8, train_wall=12, gb_free=10.8, wall=5740]2022-05-26 13:14:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.96 GiB already allocated; 609.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:35 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 537          |        cudaMalloc retries: 601       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7124 MB |    9690 MB |  234492 GB |  234485 GB |\n","|       from large pool |    7104 MB |    9669 MB |  229513 GB |  229506 GB |\n","|       from small pool |      20 MB |      20 MB |    4979 GB |    4978 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7124 MB |    9690 MB |  234492 GB |  234485 GB |\n","|       from large pool |    7104 MB |    9669 MB |  229513 GB |  229506 GB |\n","|       from small pool |      20 MB |      20 MB |    4979 GB |    4978 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13234 MB |   13312 MB |  551548 MB |  538314 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      22 MB |     100 MB |   52654 MB |   52632 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6109 MB |    6109 MB |  469107 GB |  469101 GB |\n","|       from large pool |    6107 MB |    6107 MB |  463398 GB |  463392 GB |\n","|       from small pool |       1 MB |       5 MB |    5709 GB |    5709 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36865 K  |   36864 K  |\n","|       from large pool |     132    |     136    |   15696 K  |   15696 K  |\n","|       from small pool |     194    |     268    |   21168 K  |   21167 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36865 K  |   36864 K  |\n","|       from large pool |     132    |     136    |   15696 K  |   15696 K  |\n","|       from small pool |     194    |     268    |   21168 K  |   21167 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      20    |      59    |   26517    |   26497    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      11    |      50    |   26327    |   26316    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      20    |      26    |   18441 K  |   18441 K  |\n","|       from large pool |      10    |      12    |    9046 K  |    9046 K  |\n","|       from small pool |      10    |      17    |    9394 K  |    9394 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  55% 1715/3126 [03:26<02:54,  8.08it/s, loss=1.745, ppl=3.35, wps=24747.3, ups=8.21, wpb=3014.8, bsz=128, num_updates=44900, lr=0.0005, gnorm=1.227, loss_scale=8, train_wall=12, gb_free=10.8, wall=5740]2022-05-26 13:14:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.43 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:36 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 538          |        cudaMalloc retries: 602       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7603 MB |   10271 MB |  234514 GB |  234506 GB |\n","|       from large pool |    7593 MB |   10261 MB |  229534 GB |  229527 GB |\n","|       from small pool |      10 MB |      17 MB |    4979 GB |    4979 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7603 MB |   10271 MB |  234514 GB |  234506 GB |\n","|       from large pool |    7593 MB |   10261 MB |  229534 GB |  229527 GB |\n","|       from small pool |      10 MB |      17 MB |    4979 GB |    4979 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13308 MB |  551622 MB |  538398 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |      96 MB |   52728 MB |   52716 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5620 MB |    5620 MB |  469143 GB |  469137 GB |\n","|       from large pool |    5618 MB |    5618 MB |  463433 GB |  463428 GB |\n","|       from small pool |       1 MB |      17 MB |    5709 GB |    5709 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36867 K  |   36867 K  |\n","|       from large pool |     142    |     147    |   15698 K  |   15698 K  |\n","|       from small pool |     184    |     268    |   21169 K  |   21169 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36867 K  |   36867 K  |\n","|       from large pool |     142    |     147    |   15698 K  |   15698 K  |\n","|       from small pool |     184    |     268    |   21169 K  |   21169 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      57    |   26554    |   26539    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      48    |   26364    |   26358    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      41    |   18442 K  |   18442 K  |\n","|       from large pool |      13    |      14    |    9047 K  |    9047 K  |\n","|       from small pool |      10    |      34    |    9395 K  |    9395 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  56% 1752/3126 [03:30<02:48,  8.16it/s, loss=1.745, ppl=3.35, wps=24747.3, ups=8.21, wpb=3014.8, bsz=128, num_updates=44900, lr=0.0005, gnorm=1.227, loss_scale=8, train_wall=12, gb_free=10.8, wall=5740]2022-05-26 13:14:40 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.69 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","epoch 015:  56% 1753/3126 [03:30<03:02,  7.51it/s, loss=1.745, ppl=3.35, wps=24747.3, ups=8.21, wpb=3014.8, bsz=128, num_updates=44900, lr=0.0005, gnorm=1.227, loss_scale=8, train_wall=12, gb_free=10.8, wall=5740]2022-05-26 13:14:40 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 539          |        cudaMalloc retries: 603       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7877 MB |   10546 MB |  234710 GB |  234703 GB |\n","|       from large pool |    7866 MB |   10535 MB |  229728 GB |  229720 GB |\n","|       from small pool |      10 MB |      17 MB |    4982 GB |    4982 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7877 MB |   10546 MB |  234710 GB |  234703 GB |\n","|       from large pool |    7866 MB |   10535 MB |  229728 GB |  229720 GB |\n","|       from small pool |      10 MB |      17 MB |    4982 GB |    4982 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  551724 MB |  538498 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   52830 MB |   52816 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5348 MB |    5348 MB |  469530 GB |  469525 GB |\n","|       from large pool |    5345 MB |    5345 MB |  463817 GB |  463812 GB |\n","|       from small pool |       3 MB |      18 MB |    5713 GB |    5713 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36896 K  |   36896 K  |\n","|       from large pool |     142    |     147    |   15711 K  |   15711 K  |\n","|       from small pool |     184    |     268    |   21185 K  |   21185 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36896 K  |   36896 K  |\n","|       from large pool |     142    |     147    |   15711 K  |   15711 K  |\n","|       from small pool |     184    |     268    |   21185 K  |   21185 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   26605    |   26589    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   26415    |   26408    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      30    |      40    |   18456 K  |   18456 K  |\n","|       from large pool |      20    |      21    |    9054 K  |    9054 K  |\n","|       from small pool |      10    |      33    |    9402 K  |    9402 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:40 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  57% 1769/3126 [03:32<03:02,  7.45it/s, loss=1.745, ppl=3.35, wps=24747.3, ups=8.21, wpb=3014.8, bsz=128, num_updates=44900, lr=0.0005, gnorm=1.227, loss_scale=8, train_wall=12, gb_free=10.8, wall=5740]2022-05-26 13:14:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.37 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:43 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 540          |        cudaMalloc retries: 604       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7544 MB |   10238 MB |  234805 GB |  234797 GB |\n","|       from large pool |    7534 MB |   10227 MB |  229820 GB |  229813 GB |\n","|       from small pool |      10 MB |      17 MB |    4984 GB |    4984 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7544 MB |   10238 MB |  234805 GB |  234797 GB |\n","|       from large pool |    7534 MB |   10227 MB |  229820 GB |  229813 GB |\n","|       from small pool |      10 MB |      17 MB |    4984 GB |    4984 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  551820 MB |  538596 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   52926 MB |   52914 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5679 MB |    5679 MB |  469712 GB |  469706 GB |\n","|       from large pool |    5677 MB |    5677 MB |  463996 GB |  463991 GB |\n","|       from small pool |       1 MB |      36 MB |    5715 GB |    5715 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36910 K  |   36910 K  |\n","|       from large pool |     142    |     147    |   15716 K  |   15716 K  |\n","|       from small pool |     184    |     268    |   21193 K  |   21193 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36910 K  |   36910 K  |\n","|       from large pool |     142    |     147    |   15716 K  |   15716 K  |\n","|       from small pool |     184    |     268    |   21193 K  |   21193 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   26653    |   26638    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   26463    |   26457    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      60    |   18463 K  |   18463 K  |\n","|       from large pool |      14    |      15    |    9057 K  |    9057 K  |\n","|       from small pool |       9    |      52    |    9405 K  |    9405 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  58% 1805/3126 [03:37<02:51,  7.68it/s, loss=1.73, ppl=3.32, wps=23009, ups=7.87, wpb=2922.8, bsz=128, num_updates=45000, lr=0.0005, gnorm=1.212, loss_scale=8, train_wall=12, gb_free=11.9, wall=5752]2022-05-26 13:14:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 7.02 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:47 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 541          |        cudaMalloc retries: 605       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7193 MB |    9710 MB |  234979 GB |  234972 GB |\n","|       from large pool |    7183 MB |    9699 MB |  229992 GB |  229985 GB |\n","|       from small pool |      10 MB |      17 MB |    4987 GB |    4987 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7193 MB |    9710 MB |  234979 GB |  234972 GB |\n","|       from large pool |    7183 MB |    9699 MB |  229992 GB |  229985 GB |\n","|       from small pool |      10 MB |      17 MB |    4987 GB |    4987 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  551918 MB |  538694 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   53024 MB |   53012 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6030 MB |    6030 MB |  470064 GB |  470058 GB |\n","|       from large pool |    6028 MB |    6028 MB |  464344 GB |  464339 GB |\n","|       from small pool |       1 MB |      11 MB |    5719 GB |    5719 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36937 K  |   36937 K  |\n","|       from large pool |     142    |     147    |   15729 K  |   15729 K  |\n","|       from small pool |     184    |     268    |   21208 K  |   21208 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36937 K  |   36937 K  |\n","|       from large pool |     142    |     147    |   15729 K  |   15729 K  |\n","|       from small pool |     184    |     268    |   21208 K  |   21208 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   26702    |   26687    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   26512    |   26506    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      38    |   18477 K  |   18477 K  |\n","|       from large pool |      19    |      20    |    9065 K  |    9065 K  |\n","|       from small pool |       7    |      30    |    9412 K  |    9412 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 015:  58% 1806/3126 [03:37<02:59,  7.35it/s, loss=1.73, ppl=3.32, wps=23009, ups=7.87, wpb=2922.8, bsz=128, num_updates=45000, lr=0.0005, gnorm=1.212, loss_scale=8, train_wall=12, gb_free=11.9, wall=5752]2022-05-26 13:14:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  59% 1839/3126 [03:41<02:35,  8.26it/s, loss=1.73, ppl=3.32, wps=23009, ups=7.87, wpb=2922.8, bsz=128, num_updates=45000, lr=0.0005, gnorm=1.212, loss_scale=8, train_wall=12, gb_free=11.9, wall=5752]2022-05-26 13:14:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.46 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:51 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 542          |        cudaMalloc retries: 606       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6610 MB |    8875 MB |  235155 GB |  235148 GB |\n","|       from large pool |    6600 MB |    8865 MB |  230163 GB |  230156 GB |\n","|       from small pool |      10 MB |      17 MB |    4991 GB |    4991 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6610 MB |    8875 MB |  235155 GB |  235148 GB |\n","|       from large pool |    6600 MB |    8865 MB |  230163 GB |  230156 GB |\n","|       from small pool |      10 MB |      17 MB |    4991 GB |    4991 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13324 MB |  552018 MB |  538794 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     112 MB |   53124 MB |   53112 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6613 MB |    6613 MB |  470421 GB |  470415 GB |\n","|       from large pool |    6611 MB |    6611 MB |  464698 GB |  464691 GB |\n","|       from small pool |       1 MB |       3 MB |    5723 GB |    5723 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36964 K  |   36963 K  |\n","|       from large pool |     142    |     147    |   15740 K  |   15740 K  |\n","|       from small pool |     184    |     268    |   21224 K  |   21223 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36964 K  |   36963 K  |\n","|       from large pool |     142    |     147    |   15740 K  |   15740 K  |\n","|       from small pool |     184    |     268    |   21224 K  |   21223 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      65    |   26752    |   26737    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      56    |   26562    |   26556    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      26    |   18490 K  |   18490 K  |\n","|       from large pool |      17    |      18    |    9071 K  |    9071 K  |\n","|       from small pool |       8    |      16    |    9419 K  |    9419 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  60% 1876/3126 [03:45<02:17,  9.06it/s, loss=1.73, ppl=3.32, wps=23009, ups=7.87, wpb=2922.8, bsz=128, num_updates=45000, lr=0.0005, gnorm=1.212, loss_scale=8, train_wall=12, gb_free=11.9, wall=5752]2022-05-26 13:14:55 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.18 GiB already allocated; 607.75 MiB free; 12.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:14:55 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 543          |        cudaMalloc retries: 607       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7350 MB |   10042 MB |  235339 GB |  235331 GB |\n","|       from large pool |    7328 MB |   10019 MB |  230343 GB |  230336 GB |\n","|       from small pool |      21 MB |      22 MB |    4995 GB |    4995 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7350 MB |   10042 MB |  235339 GB |  235331 GB |\n","|       from large pool |    7328 MB |   10019 MB |  230343 GB |  230336 GB |\n","|       from small pool |      21 MB |      22 MB |    4995 GB |    4995 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13236 MB |   13326 MB |  552120 MB |  538884 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      24 MB |     114 MB |   53226 MB |   53202 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5885 MB |    5885 MB |  470797 GB |  470791 GB |\n","|       from large pool |    5883 MB |    5883 MB |  465069 GB |  465063 GB |\n","|       from small pool |       2 MB |      12 MB |    5728 GB |    5728 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   36993 K  |   36992 K  |\n","|       from large pool |     126    |     130    |   15753 K  |   15752 K  |\n","|       from small pool |     200    |     268    |   21240 K  |   21239 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   36993 K  |   36992 K  |\n","|       from large pool |     126    |     130    |   15753 K  |   15752 K  |\n","|       from small pool |     200    |     268    |   21240 K  |   21239 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      21    |      66    |   26803    |   26782    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      12    |      57    |   26613    |   26601    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      30    |   18505 K  |   18505 K  |\n","|       from large pool |      11    |      12    |    9078 K  |    9078 K  |\n","|       from small pool |      13    |      23    |    9426 K  |    9426 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:14:55 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  65% 2043/3126 [04:05<01:48, 10.00it/s, loss=1.859, ppl=3.63, wps=24461.2, ups=8.36, wpb=2926, bsz=126.8, num_updates=45200, lr=0.0005, gnorm=1.281, loss_scale=8, train_wall=12, gb_free=11.9, wall=5777]2022-05-26 13:15:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.70 GiB (GPU 0; 14.76 GiB total capacity; 8.98 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:15 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 544          |        cudaMalloc retries: 608       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9199 MB |   12646 MB |  236162 GB |  236153 GB |\n","|       from large pool |    9189 MB |   12636 MB |  231149 GB |  231140 GB |\n","|       from small pool |      10 MB |      17 MB |    5013 GB |    5013 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9199 MB |   12646 MB |  236162 GB |  236153 GB |\n","|       from large pool |    9189 MB |   12636 MB |  231149 GB |  231140 GB |\n","|       from small pool |      10 MB |      17 MB |    5013 GB |    5013 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  552210 MB |  538986 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   53316 MB |   53304 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4024 MB |    4514 MB |  472466 GB |  472462 GB |\n","|       from large pool |    4022 MB |    4512 MB |  466717 GB |  466713 GB |\n","|       from small pool |       1 MB |      26 MB |    5749 GB |    5749 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37124 K  |   37124 K  |\n","|       from large pool |     142    |     147    |   15807 K  |   15807 K  |\n","|       from small pool |     184    |     268    |   21316 K  |   21316 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37124 K  |   37124 K  |\n","|       from large pool |     142    |     147    |   15807 K  |   15807 K  |\n","|       from small pool |     184    |     268    |   21316 K  |   21316 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   26848    |   26833    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   26658    |   26652    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      46    |   18571 K  |   18571 K  |\n","|       from large pool |      16    |      17    |    9110 K  |    9110 K  |\n","|       from small pool |       7    |      39    |    9461 K  |    9461 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  67% 2092/3126 [04:11<02:03,  8.36it/s, loss=1.754, ppl=3.37, wps=23812.6, ups=8.44, wpb=2822.7, bsz=128, num_updates=45300, lr=0.0005, gnorm=1.244, loss_scale=8, train_wall=11, gb_free=12.1, wall=5789]2022-05-26 13:15:21 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.75 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:21 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 545          |        cudaMalloc retries: 609       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7932 MB |   10800 MB |  236380 GB |  236373 GB |\n","|       from large pool |    7921 MB |   10790 MB |  231361 GB |  231353 GB |\n","|       from small pool |      10 MB |      17 MB |    5019 GB |    5019 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7932 MB |   10800 MB |  236380 GB |  236373 GB |\n","|       from large pool |    7921 MB |   10790 MB |  231361 GB |  231353 GB |\n","|       from small pool |      10 MB |      17 MB |    5019 GB |    5019 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13320 MB |  552306 MB |  539082 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     108 MB |   53412 MB |   53400 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5291 MB |    5291 MB |  472907 GB |  472902 GB |\n","|       from large pool |    5290 MB |    5290 MB |  467151 GB |  467146 GB |\n","|       from small pool |       1 MB |      17 MB |    5755 GB |    5755 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37162 K  |   37162 K  |\n","|       from large pool |     142    |     147    |   15823 K  |   15822 K  |\n","|       from small pool |     184    |     268    |   21339 K  |   21339 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37162 K  |   37162 K  |\n","|       from large pool |     142    |     147    |   15823 K  |   15822 K  |\n","|       from small pool |     184    |     268    |   21339 K  |   21339 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      63    |   26896    |   26881    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      54    |   26706    |   26700    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      19    |      34    |   18590 K  |   18590 K  |\n","|       from large pool |      11    |      12    |    9118 K  |    9118 K  |\n","|       from small pool |       8    |      26    |    9471 K  |    9471 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:21 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  67% 2105/3126 [04:13<02:04,  8.19it/s, loss=1.754, ppl=3.37, wps=23812.6, ups=8.44, wpb=2822.7, bsz=128, num_updates=45300, lr=0.0005, gnorm=1.244, loss_scale=8, train_wall=11, gb_free=12.1, wall=5789]2022-05-26 13:15:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.55 GiB (GPU 0; 14.76 GiB total capacity; 6.44 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:23 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 546          |        cudaMalloc retries: 610       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6592 MB |    8932 MB |  236447 GB |  236440 GB |\n","|       from large pool |    6573 MB |    8912 MB |  231426 GB |  231420 GB |\n","|       from small pool |      18 MB |      19 MB |    5020 GB |    5020 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6592 MB |    8932 MB |  236447 GB |  236440 GB |\n","|       from large pool |    6573 MB |    8912 MB |  231426 GB |  231420 GB |\n","|       from small pool |      18 MB |      19 MB |    5020 GB |    5020 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13304 MB |  552386 MB |  539154 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |      92 MB |   53492 MB |   53472 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6639 MB |    6639 MB |  473033 GB |  473027 GB |\n","|       from large pool |    6638 MB |    6638 MB |  467276 GB |  467270 GB |\n","|       from small pool |       1 MB |      14 MB |    5756 GB |    5756 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37172 K  |   37172 K  |\n","|       from large pool |     132    |     136    |   15827 K  |   15827 K  |\n","|       from small pool |     194    |     268    |   21344 K  |   21344 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37172 K  |   37172 K  |\n","|       from large pool |     132    |     136    |   15827 K  |   15827 K  |\n","|       from small pool |     194    |     268    |   21344 K  |   21344 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      55    |   26936    |   26917    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      46    |   26746    |   26736    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      41    |   18595 K  |   18595 K  |\n","|       from large pool |      10    |      11    |    9121 K  |    9121 K  |\n","|       from small pool |      13    |      34    |    9473 K  |    9473 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  69% 2153/3126 [04:18<02:03,  7.87it/s, loss=1.754, ppl=3.37, wps=23812.6, ups=8.44, wpb=2822.7, bsz=128, num_updates=45300, lr=0.0005, gnorm=1.244, loss_scale=8, train_wall=11, gb_free=12.1, wall=5789]2022-05-26 13:15:28 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.71 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:28 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 547          |        cudaMalloc retries: 611       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6868 MB |    9259 MB |  236667 GB |  236660 GB |\n","|       from large pool |    6858 MB |    9249 MB |  231640 GB |  231634 GB |\n","|       from small pool |      10 MB |      17 MB |    5026 GB |    5026 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6868 MB |    9259 MB |  236667 GB |  236660 GB |\n","|       from large pool |    6858 MB |    9249 MB |  231640 GB |  231634 GB |\n","|       from small pool |      10 MB |      17 MB |    5026 GB |    5026 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  552480 MB |  539256 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   53586 MB |   53574 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6355 MB |    6355 MB |  473485 GB |  473479 GB |\n","|       from large pool |    6353 MB |    6353 MB |  467722 GB |  467716 GB |\n","|       from small pool |       1 MB |       5 MB |    5763 GB |    5763 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37209 K  |   37209 K  |\n","|       from large pool |     142    |     147    |   15841 K  |   15841 K  |\n","|       from small pool |     184    |     268    |   21367 K  |   21367 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37209 K  |   37209 K  |\n","|       from large pool |     142    |     147    |   15841 K  |   15841 K  |\n","|       from small pool |     184    |     268    |   21367 K  |   21367 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   26983    |   26968    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   26793    |   26787    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      32    |      34    |   18614 K  |   18614 K  |\n","|       from large pool |      24    |      25    |    9129 K  |    9129 K  |\n","|       from small pool |       8    |      17    |    9484 K  |    9484 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 015:  69% 2154/3126 [04:18<02:08,  7.57it/s, loss=1.754, ppl=3.37, wps=23812.6, ups=8.44, wpb=2822.7, bsz=128, num_updates=45300, lr=0.0005, gnorm=1.244, loss_scale=8, train_wall=11, gb_free=12.1, wall=5789]2022-05-26 13:15:28 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  71% 2235/3126 [04:28<01:45,  8.44it/s, loss=1.793, ppl=3.47, wps=23770.1, ups=8.17, wpb=2908.7, bsz=128, num_updates=45400, lr=0.0005, gnorm=1.246, loss_scale=8, train_wall=11, gb_free=12.1, wall=5801]2022-05-26 13:15:38 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.13 GiB (GPU 0; 14.76 GiB total capacity; 8.58 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:38 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 548          |        cudaMalloc retries: 612       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8785 MB |   11431 MB |  237078 GB |  237070 GB |\n","|       from large pool |    8774 MB |   11419 MB |  232043 GB |  232035 GB |\n","|       from small pool |      11 MB |      17 MB |    5034 GB |    5034 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8785 MB |   11431 MB |  237078 GB |  237070 GB |\n","|       from large pool |    8774 MB |   11419 MB |  232043 GB |  232035 GB |\n","|       from small pool |      11 MB |      17 MB |    5034 GB |    5034 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  552582 MB |  539358 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   53688 MB |   53676 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4438 MB |    4492 MB |  474340 GB |  474335 GB |\n","|       from large pool |    4437 MB |    4490 MB |  468566 GB |  468562 GB |\n","|       from small pool |       0 MB |      17 MB |    5773 GB |    5773 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37274 K  |   37274 K  |\n","|       from large pool |     142    |     147    |   15870 K  |   15870 K  |\n","|       from small pool |     184    |     268    |   21403 K  |   21403 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37274 K  |   37274 K  |\n","|       from large pool |     142    |     147    |   15870 K  |   15870 K  |\n","|       from small pool |     184    |     268    |   21403 K  |   21403 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   27034    |   27019    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   26844    |   26838    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      45    |   18646 K  |   18646 K  |\n","|       from large pool |      18    |      19    |    9146 K  |    9146 K  |\n","|       from small pool |       9    |      37    |    9500 K  |    9500 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:38 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  73% 2288/3126 [04:35<01:58,  7.05it/s, loss=1.793, ppl=3.47, wps=23770.1, ups=8.17, wpb=2908.7, bsz=128, num_updates=45400, lr=0.0005, gnorm=1.246, loss_scale=8, train_wall=11, gb_free=12.1, wall=5801]2022-05-26 13:15:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.21 GiB (GPU 0; 14.76 GiB total capacity; 6.32 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:45 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 549          |        cudaMalloc retries: 613       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6470 MB |    8634 MB |  237363 GB |  237357 GB |\n","|       from large pool |    6460 MB |    8624 MB |  232323 GB |  232317 GB |\n","|       from small pool |      10 MB |      17 MB |    5039 GB |    5039 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6470 MB |    8634 MB |  237363 GB |  237357 GB |\n","|       from large pool |    6460 MB |    8624 MB |  232323 GB |  232317 GB |\n","|       from small pool |      10 MB |      17 MB |    5039 GB |    5039 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13320 MB |  552678 MB |  539454 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     108 MB |   53784 MB |   53772 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6753 MB |    6753 MB |  474891 GB |  474885 GB |\n","|       from large pool |    6751 MB |    6751 MB |  469112 GB |  469106 GB |\n","|       from small pool |       1 MB |       3 MB |    5779 GB |    5779 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37315 K  |   37315 K  |\n","|       from large pool |     142    |     147    |   15889 K  |   15889 K  |\n","|       from small pool |     184    |     268    |   21426 K  |   21426 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37315 K  |   37315 K  |\n","|       from large pool |     142    |     147    |   15889 K  |   15889 K  |\n","|       from small pool |     184    |     268    |   21426 K  |   21426 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      63    |   27082    |   27067    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      54    |   26892    |   26886    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      28    |   18667 K  |   18667 K  |\n","|       from large pool |      19    |      20    |    9157 K  |    9157 K  |\n","|       from small pool |       8    |      20    |    9510 K  |    9510 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  74% 2323/3126 [04:39<01:47,  7.49it/s, loss=1.811, ppl=3.51, wps=24572.4, ups=7.97, wpb=3083.2, bsz=128, num_updates=45500, lr=0.0005, gnorm=1.235, loss_scale=8, train_wall=12, gb_free=6.3, wall=5814]2022-05-26 13:15:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.28 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:49 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 550          |        cudaMalloc retries: 614       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7453 MB |   10120 MB |  237545 GB |  237538 GB |\n","|       from large pool |    7443 MB |   10110 MB |  232502 GB |  232495 GB |\n","|       from small pool |      10 MB |      17 MB |    5042 GB |    5042 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7453 MB |   10120 MB |  237545 GB |  237538 GB |\n","|       from large pool |    7443 MB |   10110 MB |  232502 GB |  232495 GB |\n","|       from small pool |      10 MB |      17 MB |    5042 GB |    5042 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  552780 MB |  539556 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   53886 MB |   53874 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5770 MB |    5770 MB |  475262 GB |  475257 GB |\n","|       from large pool |    5768 MB |    5768 MB |  469480 GB |  469474 GB |\n","|       from small pool |       1 MB |       3 MB |    5782 GB |    5782 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37343 K  |   37343 K  |\n","|       from large pool |     142    |     147    |   15902 K  |   15901 K  |\n","|       from small pool |     184    |     268    |   21441 K  |   21441 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37343 K  |   37343 K  |\n","|       from large pool |     142    |     147    |   15902 K  |   15901 K  |\n","|       from small pool |     184    |     268    |   21441 K  |   21441 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   27133    |   27118    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   26943    |   26937    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      26    |   18681 K  |   18681 K  |\n","|       from large pool |      13    |      14    |    9164 K  |    9164 K  |\n","|       from small pool |      10    |      18    |    9516 K  |    9516 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  77% 2407/3126 [04:49<01:34,  7.62it/s, loss=1.863, ppl=3.64, wps=25019.5, ups=8.16, wpb=3064.6, bsz=128, num_updates=45600, lr=0.0005, gnorm=1.233, loss_scale=8, train_wall=12, gb_free=10.7, wall=5826]2022-05-26 13:15:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.78 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:15:59 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 551          |        cudaMalloc retries: 615       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8989 MB |   12335 MB |  237965 GB |  237956 GB |\n","|       from large pool |    8978 MB |   12325 MB |  232913 GB |  232904 GB |\n","|       from small pool |      10 MB |      17 MB |    5051 GB |    5051 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8989 MB |   12335 MB |  237965 GB |  237956 GB |\n","|       from large pool |    8978 MB |   12325 MB |  232913 GB |  232904 GB |\n","|       from small pool |      10 MB |      17 MB |    5051 GB |    5051 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  552882 MB |  539656 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   53988 MB |   53974 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4236 MB |    4515 MB |  476111 GB |  476107 GB |\n","|       from large pool |    4233 MB |    4510 MB |  470318 GB |  470314 GB |\n","|       from small pool |       3 MB |      15 MB |    5793 GB |    5793 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37409 K  |   37409 K  |\n","|       from large pool |     142    |     147    |   15930 K  |   15930 K  |\n","|       from small pool |     184    |     268    |   21479 K  |   21479 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37409 K  |   37409 K  |\n","|       from large pool |     142    |     147    |   15930 K  |   15930 K  |\n","|       from small pool |     184    |     268    |   21479 K  |   21479 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   27184    |   27168    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   26994    |   26987    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      24    |      35    |   18714 K  |   18714 K  |\n","|       from large pool |      14    |      15    |    9180 K  |    9180 K  |\n","|       from small pool |      10    |      28    |    9534 K  |    9534 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:15:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  77% 2421/3126 [04:51<01:22,  8.52it/s, loss=1.863, ppl=3.64, wps=25019.5, ups=8.16, wpb=3064.6, bsz=128, num_updates=45600, lr=0.0005, gnorm=1.233, loss_scale=8, train_wall=12, gb_free=10.7, wall=5826]2022-05-26 13:16:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 6.98 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:16:01 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 552          |        cudaMalloc retries: 616       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7145 MB |    9711 MB |  238030 GB |  238023 GB |\n","|       from large pool |    7135 MB |    9700 MB |  232977 GB |  232970 GB |\n","|       from small pool |      10 MB |      17 MB |    5053 GB |    5053 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7145 MB |    9711 MB |  238030 GB |  238023 GB |\n","|       from large pool |    7135 MB |    9700 MB |  232977 GB |  232970 GB |\n","|       from small pool |      10 MB |      17 MB |    5053 GB |    5053 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  552978 MB |  539754 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   54084 MB |   54072 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6078 MB |    6078 MB |  476246 GB |  476240 GB |\n","|       from large pool |    6076 MB |    6076 MB |  470451 GB |  470445 GB |\n","|       from small pool |       1 MB |       9 MB |    5794 GB |    5794 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37420 K  |   37419 K  |\n","|       from large pool |     142    |     146    |   15934 K  |   15934 K  |\n","|       from small pool |     184    |     268    |   21485 K  |   21485 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37420 K  |   37419 K  |\n","|       from large pool |     142    |     146    |   15934 K  |   15934 K  |\n","|       from small pool |     184    |     268    |   21485 K  |   21485 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   27232    |   27217    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   27042    |   27036    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      14    |      33    |   18719 K  |   18719 K  |\n","|       from large pool |       8    |       9    |    9183 K  |    9183 K  |\n","|       from small pool |       6    |      24    |    9536 K  |    9536 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:16:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  79% 2464/3126 [04:56<01:20,  8.19it/s, loss=1.863, ppl=3.64, wps=25019.5, ups=8.16, wpb=3064.6, bsz=128, num_updates=45600, lr=0.0005, gnorm=1.233, loss_scale=8, train_wall=12, gb_free=10.7, wall=5826]2022-05-26 13:16:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.50 GiB (GPU 0; 14.76 GiB total capacity; 8.86 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:16:06 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 553          |        cudaMalloc retries: 617       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    9069 MB |   12416 MB |  238236 GB |  238227 GB |\n","|       from large pool |    9059 MB |   12406 MB |  233179 GB |  233170 GB |\n","|       from small pool |      10 MB |      17 MB |    5057 GB |    5057 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    9069 MB |   12416 MB |  238236 GB |  238227 GB |\n","|       from large pool |    9059 MB |   12406 MB |  233179 GB |  233170 GB |\n","|       from small pool |      10 MB |      17 MB |    5057 GB |    5057 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  553076 MB |  539852 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   54182 MB |   54170 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    4154 MB |    4514 MB |  476664 GB |  476659 GB |\n","|       from large pool |    4152 MB |    4512 MB |  470864 GB |  470860 GB |\n","|       from small pool |       1 MB |       6 MB |    5799 GB |    5799 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37453 K  |   37453 K  |\n","|       from large pool |     142    |     147    |   15949 K  |   15949 K  |\n","|       from small pool |     184    |     268    |   21504 K  |   21504 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37453 K  |   37453 K  |\n","|       from large pool |     142    |     147    |   15949 K  |   15949 K  |\n","|       from small pool |     184    |     268    |   21504 K  |   21504 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   27281    |   27266    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   27091    |   27085    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      27    |   18736 K  |   18736 K  |\n","|       from large pool |      14    |      15    |    9191 K  |    9191 K  |\n","|       from small pool |       9    |      18    |    9544 K  |    9544 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","epoch 015:  79% 2465/3126 [04:56<01:32,  7.14it/s, loss=1.863, ppl=3.64, wps=25019.5, ups=8.16, wpb=3064.6, bsz=128, num_updates=45600, lr=0.0005, gnorm=1.233, loss_scale=8, train_wall=12, gb_free=10.7, wall=5826]2022-05-26 13:16:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  85% 2660/3126 [05:19<00:55,  8.36it/s, loss=1.831, ppl=3.56, wps=25442.8, ups=8.46, wpb=3007.2, bsz=128, num_updates=45800, lr=0.0005, gnorm=1.26, loss_scale=8, train_wall=11, gb_free=8.6, wall=5850]2022-05-26 13:16:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 7.27 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:16:29 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 554          |        cudaMalloc retries: 618       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7445 MB |   10112 MB |  239174 GB |  239167 GB |\n","|       from large pool |    7435 MB |   10102 MB |  234096 GB |  234088 GB |\n","|       from small pool |      10 MB |      17 MB |    5078 GB |    5078 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7445 MB |   10112 MB |  239174 GB |  239167 GB |\n","|       from large pool |    7435 MB |   10102 MB |  234096 GB |  234088 GB |\n","|       from small pool |      10 MB |      17 MB |    5078 GB |    5078 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  553178 MB |  539954 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   54284 MB |   54272 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5778 MB |    5778 MB |  478590 GB |  478584 GB |\n","|       from large pool |    5776 MB |    5776 MB |  472766 GB |  472761 GB |\n","|       from small pool |       1 MB |      17 MB |    5823 GB |    5823 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37608 K  |   37607 K  |\n","|       from large pool |     142    |     147    |   16015 K  |   16015 K  |\n","|       from small pool |     184    |     268    |   21593 K  |   21592 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37608 K  |   37607 K  |\n","|       from large pool |     142    |     147    |   16015 K  |   16015 K  |\n","|       from small pool |     184    |     268    |   21593 K  |   21592 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   27332    |   27317    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   27142    |   27136    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      25    |      42    |   18814 K  |   18814 K  |\n","|       from large pool |      16    |      17    |    9229 K  |    9229 K  |\n","|       from small pool |       9    |      34    |    9584 K  |    9584 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:16:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  86% 2673/3126 [05:21<00:58,  7.77it/s, loss=1.831, ppl=3.56, wps=25442.8, ups=8.46, wpb=3007.2, bsz=128, num_updates=45800, lr=0.0005, gnorm=1.26, loss_scale=8, train_wall=11, gb_free=8.6, wall=5850]2022-05-26 13:16:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.99 GiB (GPU 0; 14.76 GiB total capacity; 7.01 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:16:31 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 555          |        cudaMalloc retries: 619       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7178 MB |    9744 MB |  239258 GB |  239251 GB |\n","|       from large pool |    7168 MB |    9733 MB |  234178 GB |  234171 GB |\n","|       from small pool |      10 MB |      17 MB |    5079 GB |    5079 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7178 MB |    9744 MB |  239258 GB |  239251 GB |\n","|       from large pool |    7168 MB |    9733 MB |  234178 GB |  234171 GB |\n","|       from small pool |      10 MB |      17 MB |    5079 GB |    5079 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  553276 MB |  540052 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   54382 MB |   54370 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6045 MB |    6045 MB |  478747 GB |  478741 GB |\n","|       from large pool |    6043 MB |    6043 MB |  472922 GB |  472916 GB |\n","|       from small pool |       1 MB |      24 MB |    5824 GB |    5824 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37618 K  |   37618 K  |\n","|       from large pool |     142    |     146    |   16020 K  |   16020 K  |\n","|       from small pool |     184    |     268    |   21598 K  |   21598 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37618 K  |   37618 K  |\n","|       from large pool |     142    |     146    |   16020 K  |   16020 K  |\n","|       from small pool |     184    |     268    |   21598 K  |   21598 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   27381    |   27366    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   27191    |   27185    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      17    |      42    |   18820 K  |   18820 K  |\n","|       from large pool |       9    |      10    |    9232 K  |    9232 K  |\n","|       from small pool |       8    |      35    |    9587 K  |    9587 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:16:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  89% 2770/3126 [05:32<00:40,  8.78it/s, loss=1.837, ppl=3.57, wps=23939.5, ups=8.39, wpb=2852.5, bsz=128, num_updates=45900, lr=0.0005, gnorm=1.264, loss_scale=8, train_wall=11, gb_free=10, wall=5862]2022-05-26 13:16:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.40 GiB (GPU 0; 14.76 GiB total capacity; 6.28 GiB already allocated; 611.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:16:42 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 556          |        cudaMalloc retries: 620       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6432 MB |    8696 MB |  239694 GB |  239687 GB |\n","|       from large pool |    6412 MB |    8676 MB |  234603 GB |  234597 GB |\n","|       from small pool |      19 MB |      20 MB |    5090 GB |    5090 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6432 MB |    8696 MB |  239694 GB |  239687 GB |\n","|       from large pool |    6412 MB |    8676 MB |  234603 GB |  234597 GB |\n","|       from small pool |      19 MB |      20 MB |    5090 GB |    5090 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13232 MB |   13326 MB |  553378 MB |  540146 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      20 MB |     114 MB |   54484 MB |   54464 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6799 MB |    6799 MB |  479617 GB |  479610 GB |\n","|       from large pool |    6799 MB |    6799 MB |  473779 GB |  473773 GB |\n","|       from small pool |       0 MB |       3 MB |    5837 GB |    5837 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37693 K  |   37693 K  |\n","|       from large pool |     132    |     136    |   16050 K  |   16049 K  |\n","|       from small pool |     194    |     268    |   21643 K  |   21643 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37693 K  |   37693 K  |\n","|       from large pool |     132    |     136    |   16050 K  |   16049 K  |\n","|       from small pool |     194    |     268    |   21643 K  |   21643 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      19    |      66    |   27432    |   27413    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |      10    |      57    |   27242    |   27232    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      23    |      25    |   18857 K  |   18857 K  |\n","|       from large pool |      10    |      11    |    9249 K  |    9249 K  |\n","|       from small pool |      13    |      17    |    9607 K  |    9607 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:16:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  93% 2917/3126 [05:50<00:22,  9.22it/s, loss=1.804, ppl=3.49, wps=25079.5, ups=8.3, wpb=3021.4, bsz=128, num_updates=46100, lr=0.0005, gnorm=1.26, loss_scale=8, train_wall=12, gb_free=11.4, wall=5886]2022-05-26 13:17:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.57 GiB (GPU 0; 14.76 GiB total capacity; 7.83 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:17:00 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 557          |        cudaMalloc retries: 621       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    8020 MB |   10889 MB |  240424 GB |  240416 GB |\n","|       from large pool |    8010 MB |   10879 MB |  235318 GB |  235310 GB |\n","|       from small pool |      10 MB |      17 MB |    5106 GB |    5106 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    8020 MB |   10889 MB |  240424 GB |  240416 GB |\n","|       from large pool |    8010 MB |   10879 MB |  235318 GB |  235310 GB |\n","|       from small pool |      10 MB |      17 MB |    5106 GB |    5106 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  553472 MB |  540246 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   54578 MB |   54564 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5205 MB |    5205 MB |  481120 GB |  481115 GB |\n","|       from large pool |    5201 MB |    5201 MB |  475264 GB |  475259 GB |\n","|       from small pool |       3 MB |      16 MB |    5855 GB |    5855 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37809 K  |   37809 K  |\n","|       from large pool |     142    |     147    |   16100 K  |   16100 K  |\n","|       from small pool |     184    |     268    |   21709 K  |   21709 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37809 K  |   37809 K  |\n","|       from large pool |     142    |     147    |   16100 K  |   16100 K  |\n","|       from small pool |     184    |     268    |   21709 K  |   21709 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   27479    |   27463    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   27289    |   27282    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      28    |      46    |   18916 K  |   18916 K  |\n","|       from large pool |      17    |      18    |    9278 K  |    9278 K  |\n","|       from small pool |      11    |      38    |    9637 K  |    9637 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:17:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  93% 2918/3126 [05:50<00:24,  8.41it/s, loss=1.804, ppl=3.49, wps=25079.5, ups=8.3, wpb=3021.4, bsz=128, num_updates=46100, lr=0.0005, gnorm=1.26, loss_scale=8, train_wall=12, gb_free=11.4, wall=5886]2022-05-26 13:17:00 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 7.60 GiB already allocated; 617.75 MiB free; 12.92 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:17:00 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 558          |        cudaMalloc retries: 622       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7787 MB |   10889 MB |  240434 GB |  240426 GB |\n","|       from large pool |    7776 MB |   10879 MB |  235327 GB |  235320 GB |\n","|       from small pool |      10 MB |      17 MB |    5106 GB |    5106 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7787 MB |   10889 MB |  240434 GB |  240426 GB |\n","|       from large pool |    7776 MB |   10879 MB |  235327 GB |  235320 GB |\n","|       from small pool |      10 MB |      17 MB |    5106 GB |    5106 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13226 MB |   13326 MB |  553472 MB |  540246 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      14 MB |     114 MB |   54578 MB |   54564 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    5438 MB |    5438 MB |  481131 GB |  481125 GB |\n","|       from large pool |    5435 MB |    5435 MB |  475275 GB |  475269 GB |\n","|       from small pool |       3 MB |      16 MB |    5855 GB |    5855 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37810 K  |   37809 K  |\n","|       from large pool |     142    |     147    |   16100 K  |   16100 K  |\n","|       from small pool |     184    |     268    |   21709 K  |   21709 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37810 K  |   37809 K  |\n","|       from large pool |     142    |     147    |   16100 K  |   16100 K  |\n","|       from small pool |     184    |     268    |   21709 K  |   21709 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      16    |      66    |   27479    |   27463    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       7    |      57    |   27289    |   27282    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      34    |      46    |   18916 K  |   18916 K  |\n","|       from large pool |      23    |      24    |    9278 K  |    9278 K  |\n","|       from small pool |      11    |      38    |    9637 K  |    9637 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:17:00 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  94% 2951/3126 [05:54<00:18,  9.70it/s, loss=1.804, ppl=3.49, wps=25079.5, ups=8.3, wpb=3021.4, bsz=128, num_updates=46100, lr=0.0005, gnorm=1.26, loss_scale=8, train_wall=12, gb_free=11.4, wall=5886]2022-05-26 13:17:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.64 GiB (GPU 0; 14.76 GiB total capacity; 6.87 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:17:04 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 559          |        cudaMalloc retries: 623       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    7031 MB |    9422 MB |  240586 GB |  240579 GB |\n","|       from large pool |    7020 MB |    9412 MB |  235476 GB |  235470 GB |\n","|       from small pool |      10 MB |      17 MB |    5109 GB |    5109 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    7031 MB |    9422 MB |  240586 GB |  240579 GB |\n","|       from large pool |    7020 MB |    9412 MB |  235476 GB |  235470 GB |\n","|       from small pool |      10 MB |      17 MB |    5109 GB |    5109 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13322 MB |  553568 MB |  540344 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     110 MB |   54674 MB |   54662 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6192 MB |    6192 MB |  481438 GB |  481432 GB |\n","|       from large pool |    6191 MB |    6191 MB |  475579 GB |  475573 GB |\n","|       from small pool |       1 MB |      17 MB |    5859 GB |    5859 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37835 K  |   37835 K  |\n","|       from large pool |     142    |     147    |   16110 K  |   16110 K  |\n","|       from small pool |     184    |     268    |   21725 K  |   21725 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37835 K  |   37835 K  |\n","|       from large pool |     142    |     147    |   16110 K  |   16110 K  |\n","|       from small pool |     184    |     268    |   21725 K  |   21725 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      64    |   27527    |   27512    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      55    |   27337    |   27331    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      27    |      43    |   18929 K  |   18929 K  |\n","|       from large pool |      19    |      20    |    9284 K  |    9284 K  |\n","|       from small pool |       8    |      35    |    9644 K  |    9644 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:17:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015:  99% 3081/3126 [06:09<00:05,  8.94it/s, loss=1.851, ppl=3.61, wps=23402.5, ups=8.1, wpb=2888.1, bsz=128, num_updates=46200, lr=0.0005, gnorm=1.282, loss_scale=8, train_wall=12, gb_free=10, wall=5898]2022-05-26 13:17:19 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.25 GiB (GPU 0; 14.76 GiB total capacity; 6.19 GiB already allocated; 619.75 MiB free; 12.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","2022-05-26 13:17:19 | WARNING | fairseq.trainer | |===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 560          |        cudaMalloc retries: 624       |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |    6340 MB |    8529 MB |  241182 GB |  241175 GB |\n","|       from large pool |    6330 MB |    8518 MB |  236057 GB |  236051 GB |\n","|       from small pool |       9 MB |      17 MB |    5124 GB |    5124 GB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |    6340 MB |    8529 MB |  241182 GB |  241175 GB |\n","|       from large pool |    6330 MB |    8518 MB |  236057 GB |  236051 GB |\n","|       from small pool |       9 MB |      17 MB |    5124 GB |    5124 GB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   13224 MB |   13326 MB |  553670 MB |  540446 MB |\n","|       from large pool |   13212 MB |   13212 MB |  498894 MB |  485682 MB |\n","|       from small pool |      12 MB |     114 MB |   54776 MB |   54764 MB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |    6883 MB |    6883 MB |  482674 GB |  482667 GB |\n","|       from large pool |    6881 MB |    6881 MB |  476798 GB |  476791 GB |\n","|       from small pool |       2 MB |      41 MB |    5876 GB |    5876 GB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     326    |     329    |   37936 K  |   37936 K  |\n","|       from large pool |     142    |     146    |   16151 K  |   16151 K  |\n","|       from small pool |     184    |     268    |   21784 K  |   21784 K  |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     326    |     329    |   37936 K  |   37936 K  |\n","|       from large pool |     142    |     146    |   16151 K  |   16151 K  |\n","|       from small pool |     184    |     268    |   21784 K  |   21784 K  |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      15    |      66    |   27578    |   27563    |\n","|       from large pool |       9    |       9    |     190    |     181    |\n","|       from small pool |       6    |      57    |   27388    |   27382    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      26    |      55    |   18980 K  |   18980 K  |\n","|       from large pool |      18    |      19    |    9308 K  |    9308 K  |\n","|       from small pool |       8    |      46    |    9671 K  |    9671 K  |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n","2022-05-26 13:17:19 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n","epoch 015: 100% 3125/3126 [06:14<00:00,  9.60it/s, loss=1.799, ppl=3.48, wps=23870.6, ups=8.64, wpb=2763.5, bsz=128, num_updates=46300, lr=0.0005, gnorm=1.289, loss_scale=8, train_wall=11, gb_free=11.5, wall=5910]2022-05-26 13:17:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n","\n","epoch 015 | valid on 'valid' subset:   0% 0/337 [00:00<?, ?it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   0% 1/337 [00:00<00:44,  7.54it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   1% 5/337 [00:00<00:17, 19.15it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   3% 10/337 [00:00<00:14, 22.86it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   4% 15/337 [00:00<00:11, 27.41it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   6% 20/337 [00:00<00:09, 31.76it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   7% 24/337 [00:00<00:09, 32.43it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   8% 28/337 [00:00<00:09, 32.74it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:   9% 32/337 [00:01<00:09, 33.02it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  11% 36/337 [00:01<00:09, 31.78it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  12% 41/337 [00:01<00:08, 33.14it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  13% 45/337 [00:01<00:09, 30.18it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  15% 49/337 [00:01<00:09, 31.13it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  16% 53/337 [00:01<00:09, 29.67it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  17% 57/337 [00:01<00:08, 31.71it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  18% 61/337 [00:02<00:09, 28.34it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  19% 65/337 [00:02<00:08, 30.74it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  20% 69/337 [00:02<00:08, 31.72it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  22% 73/337 [00:02<00:08, 30.35it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  23% 77/337 [00:02<00:08, 31.94it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  24% 81/337 [00:02<00:08, 29.34it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  25% 85/337 [00:02<00:08, 29.86it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  26% 89/337 [00:02<00:08, 30.98it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  28% 93/337 [00:03<00:08, 27.97it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  29% 97/337 [00:03<00:07, 30.14it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  30% 101/337 [00:03<00:07, 31.02it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  31% 105/337 [00:03<00:08, 28.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  32% 109/337 [00:03<00:07, 30.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  34% 113/337 [00:03<00:07, 31.13it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  35% 117/337 [00:03<00:08, 27.34it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  36% 121/337 [00:04<00:07, 28.14it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  37% 125/337 [00:04<00:07, 29.07it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  38% 129/337 [00:04<00:07, 29.29it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  39% 132/337 [00:04<00:07, 26.42it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  40% 136/337 [00:04<00:07, 27.96it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  41% 139/337 [00:04<00:07, 27.71it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  42% 142/337 [00:04<00:07, 27.36it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  43% 145/337 [00:04<00:07, 26.27it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  44% 149/337 [00:05<00:06, 27.72it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  45% 152/337 [00:05<00:06, 28.23it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  46% 155/337 [00:05<00:06, 26.35it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  47% 158/337 [00:05<00:07, 25.31it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  48% 161/337 [00:05<00:06, 26.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  49% 165/337 [00:05<00:06, 26.61it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  50% 168/337 [00:05<00:06, 25.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  51% 171/337 [00:06<00:07, 23.12it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  52% 175/337 [00:06<00:06, 25.34it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  53% 178/337 [00:06<00:06, 25.89it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  54% 181/337 [00:06<00:06, 24.07it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  55% 184/337 [00:06<00:05, 25.51it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  55% 187/337 [00:06<00:05, 26.25it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  56% 190/337 [00:06<00:05, 26.15it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  57% 193/337 [00:06<00:06, 23.91it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  58% 196/337 [00:06<00:05, 25.02it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  59% 199/337 [00:07<00:05, 25.33it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  60% 202/337 [00:07<00:05, 26.03it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  61% 205/337 [00:07<00:05, 23.36it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  62% 208/337 [00:07<00:05, 24.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  63% 211/337 [00:07<00:05, 24.56it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  64% 214/337 [00:07<00:04, 24.88it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  64% 217/337 [00:07<00:05, 22.83it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  65% 220/337 [00:07<00:05, 22.81it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  66% 223/337 [00:08<00:04, 23.76it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  67% 226/337 [00:08<00:04, 22.67it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  68% 229/337 [00:08<00:05, 20.28it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  69% 232/337 [00:08<00:04, 21.67it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  70% 235/337 [00:08<00:04, 22.56it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  71% 238/337 [00:08<00:04, 20.55it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  72% 241/337 [00:08<00:04, 22.10it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  72% 244/337 [00:09<00:04, 22.92it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  73% 247/337 [00:09<00:04, 21.55it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  74% 250/337 [00:09<00:03, 21.93it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  75% 253/337 [00:09<00:03, 22.46it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  76% 256/337 [00:09<00:03, 20.52it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  77% 259/337 [00:09<00:03, 21.09it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  78% 262/337 [00:09<00:03, 21.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  79% 265/337 [00:10<00:03, 20.50it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  80% 268/337 [00:10<00:03, 21.42it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  80% 271/337 [00:10<00:03, 19.82it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  81% 274/337 [00:10<00:03, 20.48it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  82% 277/337 [00:10<00:03, 19.17it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  83% 280/337 [00:10<00:02, 19.97it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  84% 283/337 [00:11<00:02, 20.06it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  85% 286/337 [00:11<00:02, 18.00it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  86% 289/337 [00:11<00:02, 18.25it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  87% 292/337 [00:11<00:02, 19.21it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  87% 294/337 [00:11<00:02, 19.31it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  88% 296/337 [00:11<00:02, 18.51it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  89% 299/337 [00:11<00:01, 19.30it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  89% 301/337 [00:12<00:02, 17.96it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  90% 303/337 [00:12<00:01, 18.24it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  91% 305/337 [00:12<00:01, 17.46it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  91% 307/337 [00:12<00:01, 17.85it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  92% 309/337 [00:12<00:01, 16.01it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  92% 311/337 [00:12<00:01, 16.63it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  93% 313/337 [00:12<00:01, 16.06it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  93% 315/337 [00:12<00:01, 14.71it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  94% 317/337 [00:13<00:01, 15.54it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  95% 319/337 [00:13<00:01, 15.65it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  95% 321/337 [00:13<00:01, 14.96it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  96% 323/337 [00:13<00:00, 15.22it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  96% 325/337 [00:13<00:00, 14.70it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  97% 327/337 [00:13<00:00, 13.74it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  98% 329/337 [00:13<00:00, 13.88it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  98% 331/337 [00:14<00:00, 13.39it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  99% 333/337 [00:14<00:00, 12.16it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset:  99% 335/337 [00:14<00:00, 11.67it/s]\u001b[A\n","epoch 015 | valid on 'valid' subset: 100% 337/337 [00:14<00:00, 13.04it/s]\u001b[A\n","                                                                          \u001b[A2022-05-26 13:17:38 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.164 | ppl 143.39 | wps 69222.4 | wpb 2965.1 | bsz 127.6 | num_updates 46325 | best_loss 5.602\n","2022-05-26 13:17:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 46325 updates\n","2022-05-26 13:17:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints_en_de_with_backtranslations/checkpoint_last.pt\n","2022-05-26 13:17:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints_en_de_with_backtranslations/checkpoint_last.pt\n","2022-05-26 13:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints_en_de_with_backtranslations/checkpoint_last.pt (epoch 15 @ 46325 updates, score 7.164) (writing took 3.6237190430001647 seconds)\n","                                                                                                                                                                                                                     2022-05-26 13:17:42 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n","2022-05-26 13:17:42 | INFO | train | epoch 015 | loss 1.748 | ppl 3.36 | wps 23273.4 | ups 7.87 | wpb 2958.2 | bsz 128 | num_updates 46325 | lr 0.0005 | gnorm 1.225 | loss_scale 8 | train_wall 358 | gb_free 8.3 | wall 5931\n","2022-05-26 13:17:42 | INFO | fairseq_cli.train | done training in 5930.7 seconds\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free ▆▅▅▁▃█▇▇▆▄▄█▂▄▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm ▁▆▆▆▆▆▆▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss █▅▄▃▃▂▂▂▂▂▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss_scale ▃▃▃▃▃█▃▃▃▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl █▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall ▆▆▆█▆▆▆▃▃▃▃▃▃▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups ▁▅▄▆▆▆▇▇▇▇▇▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall ▁▂▂▃▃▄▄▅▅▆▆▇▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb ▁█████▄██▁█████\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps ▁▅▄▆▆▇▇▇▇▇▇████\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz ██████████████████████▁█████████████████\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free ▄▇▅▆▃▄▅▁▆▇█▅▂▆▇▅▆▅▄▆▅▇▃▇█▆▅▄█▂▆▆▆▅▆█▆▆█▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm ▁▄▄▆▇▇▇▇▆▇▇▇▇▅▆▆▆▇█▇▇▆▇▇▆██▆▇█▇█▇██▇█▇██\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss █▆▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/loss_scale ▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃████▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall █▁█████████▁▁██▁█▁███▁███▁██▁▁▁████▁███▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups ▇██▇▇▇▇▇▇▇▇██▇▇▇▇█▇▇▇▁▇▇▇▇▇▇▇▁▇▇▇▇▇█▇▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb █▁▆▇█▄▆▅▆▅▆▂▃▇▆▅▇▂▅▅█▃▇▆▇▅▆▆▅▄▅█▆█▆▄▆▇▆▄\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps █▇███▇▇▇▇▇█▇▇▇███▇▇▇█▁██▇▇▇█▇▁▇█▇█▇▇▇██▇\n","\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss █▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss ▅▂▁▁▁▂▃▃▄▅▅▆▇▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl ▄▂▁▁▁▂▂▂▃▄▄▅▆▇█\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps ▁▄▆▆▂▄▆▇▆█▆▅▆▄▇\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/bsz 128.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:          train/gb_free 8.3\n","\u001b[34m\u001b[1mwandb\u001b[0m:            train/gnorm 1.225\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/loss 1.748\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/loss_scale 8.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:               train/lr 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ppl 3.36\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train/train_wall 358.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/ups 7.87\n","\u001b[34m\u001b[1mwandb\u001b[0m:             train/wall 5931.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wpb 2958.2\n","\u001b[34m\u001b[1mwandb\u001b[0m:              train/wps 23273.4\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/bsz 128.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:    train_inner/gb_free 11.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:      train_inner/gnorm 1.289\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/loss 1.799\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/loss_scale 8.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:         train_inner/lr 0.0005\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ppl 3.48\n","\u001b[34m\u001b[1mwandb\u001b[0m: train_inner/train_wall 11.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/ups 8.64\n","\u001b[34m\u001b[1mwandb\u001b[0m:       train_inner/wall 5910.0\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wpb 2763.5\n","\u001b[34m\u001b[1mwandb\u001b[0m:        train_inner/wps 23870.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:        valid/best_loss 5.602\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/bsz 127.6\n","\u001b[34m\u001b[1mwandb\u001b[0m:             valid/loss 7.164\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/ppl 143.39\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wpb 2965.1\n","\u001b[34m\u001b[1mwandb\u001b[0m:              valid/wps 69222.4\n","\u001b[34m\u001b[1mwandb\u001b[0m: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcheckpoints_en_de_with_backtranslations\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/szumi/en-de-backtranslation/runs/27ica23y\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220526_113852-27ica23y/logs\u001b[0m\n"]}]},{"cell_type":"code","source":["# evaluate model trained with backtranslations\n","! fairseq-generate ./data/binarized/augmented.tokenized.en-de \\\n","    --path checkpoints_en_de_with_backtranslations/checkpoint_best.pt \\\n","    --batch-size 256 \\\n","    --beam 5 \\\n","    --seed 1 \\\n","    --wandb-project \"en-de-backtranslation\" \\\n","    --results-path saved_back \\\n","    --scoring bleu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqlksanxJr7g","executionInfo":{"status":"ok","timestamp":1653571591822,"user_tz":-120,"elapsed":520985,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"f5a6ed3f-7e88-4e9c-8c78-f23da4fb08f8"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 13:17:53 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","2022-05-26 13:17:54 | INFO | fairseq_cli.generate | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'en-de-backtranslation', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints_en_de_with_backtranslations/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': 'saved_back'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 256, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 256, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}, 'task': {'_name': 'translation', 'data': './data/binarized/augmented.tokenized.en-de', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n","2022-05-26 13:17:55 | INFO | fairseq.tasks.translation | [en] dictionary: 73392 types\n","2022-05-26 13:17:55 | INFO | fairseq.tasks.translation | [de] dictionary: 102504 types\n","2022-05-26 13:17:55 | INFO | fairseq_cli.generate | loading model(s) from checkpoints_en_de_with_backtranslations/checkpoint_best.pt\n","2022-05-26 13:17:59 | INFO | fairseq.data.data_utils | loaded 43,291 examples from: ./data/binarized/augmented.tokenized.en-de/test.en-de.en\n","2022-05-26 13:18:00 | INFO | fairseq.data.data_utils | loaded 43,291 examples from: ./data/binarized/augmented.tokenized.en-de/test.en-de.de\n","2022-05-26 13:18:00 | INFO | fairseq.tasks.translation | ./data/binarized/augmented.tokenized.en-de test en-de 43291 examples\n","2022-05-26 13:26:28 | INFO | fairseq_cli.generate | NOTE: hypothesis and token scores are output in base 2\n","2022-05-26 13:26:28 | INFO | fairseq_cli.generate | Translated 43,291 sentences (858,489 tokens) in 321.9s (134.48 sentences/s, 2666.92 tokens/s)\n","grep: ./savsaved_backed/generate-test.txt: No such file or directory\n","2022-05-26 13:26:31 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","Namespace(ignore_case=False, order=4, ref='gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='gen.out.sys')\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/fairseq-score\", line 8, in <module>\n","    sys.exit(cli_main())\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/score.py\", line 98, in cli_main\n","    score(f)\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/score.py\", line 92, in score\n","    print(scorer.result_string(args.order))\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/scoring/bleu.py\", line 162, in result_string\n","    self.score(order=order),\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/scoring/bleu.py\", line 136, in score\n","    return self.brevity() * math.exp(psum / order) * 100\n","  File \"/usr/local/lib/python3.7/dist-packages/fairseq/scoring/bleu.py\", line 150, in brevity\n","    r = self.stat.reflen / self.stat.predlen\n","ZeroDivisionError: division by zero\n"]}]},{"cell_type":"code","source":["!grep ^H ./saved_back/generate-test.txt | cut -f3- > gen.out.sys\n","!grep ^T ./saved_back/generate-test.txt | cut -f2- > gen.out.ref\n","\n","!fairseq-score --sys gen.out.sys --ref gen.out.ref"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fyGpoRtslVJn","executionInfo":{"status":"ok","timestamp":1653572217571,"user_tz":-120,"elapsed":12649,"user":{"displayName":"Wiktor Łazarski","userId":"07764886936012676817"}},"outputId":"8ad3e6d5-2869-450f-e6ac-dac50162fdb7"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-05-26 13:36:46 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n","Namespace(ignore_case=False, order=4, ref='gen.out.ref', sacrebleu=False, sentence_bleu=False, sys='gen.out.sys')\n","BLEU4 = 9.44, 38.4/15.6/7.4/3.7 (BP=0.834, ratio=0.847, syslen=815198, reflen=963015)\n"]}]}]}